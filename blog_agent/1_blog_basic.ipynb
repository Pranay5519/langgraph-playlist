{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d664e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, TypedDict, Annotated\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser , PydanticOutputParser\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import Send\n",
    "from pathlib import Path\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e463cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task(BaseModel):\n",
    "    id: int\n",
    "    title: str\n",
    "    brief: str = Field(..., description=\"What to cover\")\n",
    "class Plan(BaseModel):\n",
    "    blog_title: str\n",
    "    tasks: List[Task]\n",
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    plan: Plan\n",
    "    # reducer: results from workers get concatenated automatically\n",
    "    sections: Annotated[List[str], operator.add]\n",
    "    final: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ed7d481",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "structured_llm = ChatOllama(\n",
    "    model=\"deepseek-r1:latest\",\n",
    "    format=\"json\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# LLM for content generation (without JSON mode)\n",
    "content_llm = ChatOllama(\n",
    "    model=\"llama3.2:latest\",\n",
    "    temperature=0.7\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851e1fd3",
   "metadata": {},
   "source": [
    "### Orchestration Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "132bfb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    plan : Plan\n",
    "    sections: Annotated[List[str], operator.add]\n",
    "    final: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a23a5333",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "google_model = ChatGoogleGenerativeAI(model = \"gemini-2.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "127a688a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def orchestrator(state: State) -> dict:\n",
    "\n",
    "    plan = google_model.with_structured_output(Plan).invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=(\n",
    "                    \"Create a blog plan with 5-7 sections on the following topic.\"\n",
    "                )\n",
    "            ),\n",
    "            HumanMessage(content=f\"Topic: {state['topic']}\"),\n",
    "        ]\n",
    "    )\n",
    "    return {\"plan\": plan}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32cf9575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def orchestrator(state: State) -> dict:\n",
    "    \"\"\"Creates a blog outline with sections\"\"\"\n",
    "    \n",
    "    # Setup parser for Plan schema\n",
    "    parser = PydanticOutputParser(pydantic_object=Plan)\n",
    "    \n",
    "    # Create prompt\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Create a blog plan  5with-7 sections.\\n\\n{format_instructions}\"),\n",
    "        (\"human\", \"Topic: {topic}\")\n",
    "    ])\n",
    "    prompt = prompt.partial(format_instructions=parser.get_format_instructions()) # tell the output rules to the prompt automatically\n",
    "    \n",
    "    # Build and run chain\n",
    "    chain = prompt | structured_llm | parser\n",
    "    plan_dict = chain.invoke({\"topic\": state[\"topic\"]})\n",
    "    \n",
    "    # CRITICAL FIX: Convert dict to Pydantic Plan object\n",
    "    # The parser returns a dict, but State expects a Plan object\n",
    "    plan_object = Plan(**plan_dict)\n",
    "    \n",
    "    return {\"plan\": plan_object}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4099c132",
   "metadata": {},
   "outputs": [],
   "source": [
    "or_graph = StateGraph(State)\n",
    "or_graph.add_node(\"orchestrator\" , orchestrator)\n",
    "\n",
    "or_graph.add_edge(START, \"orchestrator\")\n",
    "or_graph.add_edge(\"orchestrator\", END)\n",
    "app_or = or_graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3b031e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = app_or.invoke({\"topic\": \"Write a blog on Self Attention\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b45295b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'Write a blog on Self Attention',\n",
       " 'plan': Plan(blog_title='Understanding Self Attention Mechanisms', tasks=[Task(id=1, title='Introduction to Self Attention', brief=\"Explain what self attention is and why it's important in modern AI.\"), Task(id=2, title='How Self Attention Works', brief='Detail the mathematical and computational aspects of the self-attention mechanism.'), Task(id=3, title='Benefits of Self Attention', brief='Discuss the advantages over previous methods, like improved accuracy and efficiency.'), Task(id=4, title='Comparison with Traditional Methods', brief='Compare self attention with attention mechanisms or other models like RNNs.'), Task(id=5, title='Applications in AI', brief='Explore real-world uses, such as in transformers, NLP, and computer vision.'), Task(id=6, title='Challenges and Limitations', brief='Address issues like computational cost, interpretability, and ethical concerns.'), Task(id=7, title='Future Directions', brief='Speculate on upcoming developments and potential impacts on AI.')]),\n",
       " 'sections': []}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9db81704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'task': Task(id=1, title='Introduction to Self Attention', brief=\"Explain what self attention is and why it's important in modern AI.\"), 'topic': 'Write a blog on Self Attention', 'plan': Plan(blog_title='Understanding Self Attention Mechanisms', tasks=[Task(id=1, title='Introduction to Self Attention', brief=\"Explain what self attention is and why it's important in modern AI.\"), Task(id=2, title='How Self Attention Works', brief='Detail the mathematical and computational aspects of the self-attention mechanism.'), Task(id=3, title='Benefits of Self Attention', brief='Discuss the advantages over previous methods, like improved accuracy and efficiency.'), Task(id=4, title='Comparison with Traditional Methods', brief='Compare self attention with attention mechanisms or other models like RNNs.'), Task(id=5, title='Applications in AI', brief='Explore real-world uses, such as in transformers, NLP, and computer vision.'), Task(id=6, title='Challenges and Limitations', brief='Address issues like computational cost, interpretability, and ethical concerns.'), Task(id=7, title='Future Directions', brief='Speculate on upcoming developments and potential impacts on AI.')])}\n",
      "{'task': Task(id=2, title='How Self Attention Works', brief='Detail the mathematical and computational aspects of the self-attention mechanism.'), 'topic': 'Write a blog on Self Attention', 'plan': Plan(blog_title='Understanding Self Attention Mechanisms', tasks=[Task(id=1, title='Introduction to Self Attention', brief=\"Explain what self attention is and why it's important in modern AI.\"), Task(id=2, title='How Self Attention Works', brief='Detail the mathematical and computational aspects of the self-attention mechanism.'), Task(id=3, title='Benefits of Self Attention', brief='Discuss the advantages over previous methods, like improved accuracy and efficiency.'), Task(id=4, title='Comparison with Traditional Methods', brief='Compare self attention with attention mechanisms or other models like RNNs.'), Task(id=5, title='Applications in AI', brief='Explore real-world uses, such as in transformers, NLP, and computer vision.'), Task(id=6, title='Challenges and Limitations', brief='Address issues like computational cost, interpretability, and ethical concerns.'), Task(id=7, title='Future Directions', brief='Speculate on upcoming developments and potential impacts on AI.')])}\n",
      "{'task': Task(id=3, title='Benefits of Self Attention', brief='Discuss the advantages over previous methods, like improved accuracy and efficiency.'), 'topic': 'Write a blog on Self Attention', 'plan': Plan(blog_title='Understanding Self Attention Mechanisms', tasks=[Task(id=1, title='Introduction to Self Attention', brief=\"Explain what self attention is and why it's important in modern AI.\"), Task(id=2, title='How Self Attention Works', brief='Detail the mathematical and computational aspects of the self-attention mechanism.'), Task(id=3, title='Benefits of Self Attention', brief='Discuss the advantages over previous methods, like improved accuracy and efficiency.'), Task(id=4, title='Comparison with Traditional Methods', brief='Compare self attention with attention mechanisms or other models like RNNs.'), Task(id=5, title='Applications in AI', brief='Explore real-world uses, such as in transformers, NLP, and computer vision.'), Task(id=6, title='Challenges and Limitations', brief='Address issues like computational cost, interpretability, and ethical concerns.'), Task(id=7, title='Future Directions', brief='Speculate on upcoming developments and potential impacts on AI.')])}\n",
      "{'task': Task(id=4, title='Comparison with Traditional Methods', brief='Compare self attention with attention mechanisms or other models like RNNs.'), 'topic': 'Write a blog on Self Attention', 'plan': Plan(blog_title='Understanding Self Attention Mechanisms', tasks=[Task(id=1, title='Introduction to Self Attention', brief=\"Explain what self attention is and why it's important in modern AI.\"), Task(id=2, title='How Self Attention Works', brief='Detail the mathematical and computational aspects of the self-attention mechanism.'), Task(id=3, title='Benefits of Self Attention', brief='Discuss the advantages over previous methods, like improved accuracy and efficiency.'), Task(id=4, title='Comparison with Traditional Methods', brief='Compare self attention with attention mechanisms or other models like RNNs.'), Task(id=5, title='Applications in AI', brief='Explore real-world uses, such as in transformers, NLP, and computer vision.'), Task(id=6, title='Challenges and Limitations', brief='Address issues like computational cost, interpretability, and ethical concerns.'), Task(id=7, title='Future Directions', brief='Speculate on upcoming developments and potential impacts on AI.')])}\n",
      "{'task': Task(id=5, title='Applications in AI', brief='Explore real-world uses, such as in transformers, NLP, and computer vision.'), 'topic': 'Write a blog on Self Attention', 'plan': Plan(blog_title='Understanding Self Attention Mechanisms', tasks=[Task(id=1, title='Introduction to Self Attention', brief=\"Explain what self attention is and why it's important in modern AI.\"), Task(id=2, title='How Self Attention Works', brief='Detail the mathematical and computational aspects of the self-attention mechanism.'), Task(id=3, title='Benefits of Self Attention', brief='Discuss the advantages over previous methods, like improved accuracy and efficiency.'), Task(id=4, title='Comparison with Traditional Methods', brief='Compare self attention with attention mechanisms or other models like RNNs.'), Task(id=5, title='Applications in AI', brief='Explore real-world uses, such as in transformers, NLP, and computer vision.'), Task(id=6, title='Challenges and Limitations', brief='Address issues like computational cost, interpretability, and ethical concerns.'), Task(id=7, title='Future Directions', brief='Speculate on upcoming developments and potential impacts on AI.')])}\n",
      "{'task': Task(id=6, title='Challenges and Limitations', brief='Address issues like computational cost, interpretability, and ethical concerns.'), 'topic': 'Write a blog on Self Attention', 'plan': Plan(blog_title='Understanding Self Attention Mechanisms', tasks=[Task(id=1, title='Introduction to Self Attention', brief=\"Explain what self attention is and why it's important in modern AI.\"), Task(id=2, title='How Self Attention Works', brief='Detail the mathematical and computational aspects of the self-attention mechanism.'), Task(id=3, title='Benefits of Self Attention', brief='Discuss the advantages over previous methods, like improved accuracy and efficiency.'), Task(id=4, title='Comparison with Traditional Methods', brief='Compare self attention with attention mechanisms or other models like RNNs.'), Task(id=5, title='Applications in AI', brief='Explore real-world uses, such as in transformers, NLP, and computer vision.'), Task(id=6, title='Challenges and Limitations', brief='Address issues like computational cost, interpretability, and ethical concerns.'), Task(id=7, title='Future Directions', brief='Speculate on upcoming developments and potential impacts on AI.')])}\n",
      "{'task': Task(id=7, title='Future Directions', brief='Speculate on upcoming developments and potential impacts on AI.'), 'topic': 'Write a blog on Self Attention', 'plan': Plan(blog_title='Understanding Self Attention Mechanisms', tasks=[Task(id=1, title='Introduction to Self Attention', brief=\"Explain what self attention is and why it's important in modern AI.\"), Task(id=2, title='How Self Attention Works', brief='Detail the mathematical and computational aspects of the self-attention mechanism.'), Task(id=3, title='Benefits of Self Attention', brief='Discuss the advantages over previous methods, like improved accuracy and efficiency.'), Task(id=4, title='Comparison with Traditional Methods', brief='Compare self attention with attention mechanisms or other models like RNNs.'), Task(id=5, title='Applications in AI', brief='Explore real-world uses, such as in transformers, NLP, and computer vision.'), Task(id=6, title='Challenges and Limitations', brief='Address issues like computational cost, interpretability, and ethical concerns.'), Task(id=7, title='Future Directions', brief='Speculate on upcoming developments and potential impacts on AI.')])}\n"
     ]
    }
   ],
   "source": [
    "for task in output[\"plan\"].tasks:\n",
    "    print({\n",
    "            \"task\": task,\n",
    "            \"topic\": output[\"topic\"],\n",
    "            \"plan\": output[\"plan\"]\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d2522a",
   "metadata": {},
   "source": [
    "### Fan OutCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "800723d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fanout(state: State):\n",
    "    \"\"\"Creates parallel Send commands for each task\"\"\"\n",
    "    \n",
    "    # CRITICAL FIX: Access plan.tasks as attribute, NOT plan[\"tasks\"]\n",
    "    # state[\"plan\"] is a Plan object (Pydantic), not a dict\n",
    "    return [\n",
    "        Send(\"worker\", {\n",
    "            \"task\": task,\n",
    "            \"topic\": state[\"topic\"],\n",
    "            \"plan\": state[\"plan\"]\n",
    "        })\n",
    "        for task in state[\"plan\"].tasks  # Use .tasks attribute\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ea43b72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(payload: dict) -> dict:\n",
    "    \"\"\"Generates markdown content for one section\"\"\"\n",
    "    \n",
    "    # Extract data from payload\n",
    "    task = payload[\"task\"]\n",
    "    topic = payload[\"topic\"]\n",
    "    plan = payload[\"plan\"]\n",
    "    \n",
    "    # CRITICAL FIX: Access plan.blog_title as attribute\n",
    "    blog_title = plan.blog_title  # Use .blog_title attribute, NOT [\"blog_title\"]\n",
    "    \n",
    "    # Generate content using content_llm (without JSON mode)\n",
    "    response = content_llm.invoke([\n",
    "        SystemMessage(content=\"Write one clean Markdown section.\"),\n",
    "        HumanMessage(content=f\"\"\"\n",
    "Blog: {blog_title}\n",
    "Topic: {topic}\n",
    "\n",
    "Section: {task.title}\n",
    "Brief: {task.brief}\n",
    "\n",
    "Write 2-3 paragraphs. Return only the section content in Markdown.\n",
    "\"\"\")\n",
    "    ])\n",
    "    \n",
    "    section_md = response.content.strip()\n",
    "    \n",
    "    # IMPORTANT: Return as list because sections uses operator.add reducer\n",
    "    # Multiple worker returns get automatically concatenated into sections list\n",
    "    return {\"sections\": [section_md]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "861abaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reducer(state: State) -> dict:\n",
    "    \"\"\"Combines sections into final blog and saves to file\"\"\"\n",
    "    \n",
    "    # CRITICAL FIX: Access attributes, not dict keys\n",
    "    title = state[\"plan\"].blog_title  # Use .blog_title attribute\n",
    "    \n",
    "    # Join all sections with double newlines\n",
    "    body = \"\\n\\n\".join(state[\"sections\"]).strip()\n",
    "    \n",
    "    # Create final markdown\n",
    "    final_md = f\"# {title}\\n\\n{body}\\n\"\n",
    "    \n",
    "    # Save to file\n",
    "    filename = title.lower().replace(\" \", \"_\") + \".md\"\n",
    "    Path(filename).write_text(final_md, encoding=\"utf-8\")\n",
    "    \n",
    "    print(f\"✅ Saved to {filename}\")\n",
    "    \n",
    "    return {\"final\": final_md}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b2d9e917",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = StateGraph(State)\n",
    "\n",
    "# Add all nodes\n",
    "g.add_node(\"orchestrator\", orchestrator)\n",
    "g.add_node(\"worker\", worker)\n",
    "g.add_node(\"reducer\", reducer)\n",
    "\n",
    "# Connect nodes\n",
    "g.add_edge(START, \"orchestrator\")\n",
    "g.add_conditional_edges(\"orchestrator\", fanout, [\"worker\"])  # Parallel fanout\n",
    "g.add_edge(\"worker\", \"reducer\")\n",
    "g.add_edge(\"reducer\", END)\n",
    "\n",
    "# Compile\n",
    "app = g.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6853fa17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved to demystifying_self-attention_in_transformers:_an_in-depth_exploration.md\n"
     ]
    }
   ],
   "source": [
    "result = app.invoke({\n",
    "        \"topic\": \"Self Attention in Transformers\",\n",
    "        \"sections\": []  # Must initialize as empty list\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6e46beca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Demystifying Self-Attention in Transformers: An In-Depth Exploration\n",
      "\n",
      "# Introduction to Transformers and Attention\n",
      "## Overview of Transformer Architecture\n",
      "\n",
      "Transformers have revolutionized the field of natural language processing (NLP) with their ability to efficiently handle long-range dependencies in sequences. Introduced by Vaswani et al. in 2017, the Transformer architecture replaces traditional recurrent neural networks (RNNs) with self-attention mechanisms to model complex interactions between input elements. This shift enables models to focus on specific parts of the input sequence that are relevant for a particular task.\n",
      "\n",
      "## Role of Attention Mechanisms\n",
      "\n",
      "Attention mechanisms play a crucial role in Transformers by allowing them to weigh the importance of different input elements when generating output. The attention mechanism enables the model to selectively focus on certain parts of the input data, rather than relying solely on sequential dependencies. This allows Transformers to capture subtle relationships between inputs that may not be apparent through traditional RNNs.\n",
      "\n",
      "## Key Benefits of Attention Mechanisms\n",
      "\n",
      "The introduction of self-attention mechanisms in Transformers has several key benefits, including:\n",
      "\n",
      "*   Improved handling of long-range dependencies\n",
      "*   Increased flexibility and efficiency in modeling complex relationships\n",
      "*   Ability to handle variable-length input sequences without the need for padding or truncation\n",
      "\n",
      "### Understanding Self-Attention\n",
      "\n",
      "Self-attention is a fundamental component of transformer architectures, allowing models to weigh the importance of different input elements relative to each other. Unlike traditional attention mechanisms that rely on external context or memory, self-attention enables the model to focus on all inputs simultaneously and weigh their relative importance without needing additional data.\n",
      "\n",
      "The key components of self-attention include:\n",
      "\n",
      "*   Query (Q): The query vector is used to determine which elements to attend to.\n",
      "*   Key (K): The key vector is used to calculate attention scores.\n",
      "*   Value (V): The value vector represents the weighted sum of input elements based on their attention scores.\n",
      "\n",
      "The core difference between self-attention and traditional attention lies in its ability to process all inputs simultaneously, rather than relying on a predefined context or memory. This allows transformers to handle long-range dependencies and relationships within sequences more effectively.\n",
      "\n",
      "### Technical Deep Dive: How Self-Attention Calculates\n",
      "#### Score Computation\n",
      "The core of self-attention is calculating the scores between all pairs of tokens in a sequence. This is typically done using a dot product of query and key tensors, followed by a softmax function to normalize the scores.\n",
      "\n",
      "`Score = (Query * Key) / √(Dimensionality of Query or Key)` \n",
      "`: Softmax(scores)`\n",
      "\n",
      "#### Weighted Sum\n",
      "The weighted sum is computed by multiplying each score with its corresponding weight from the attention weights tensor. The weights are learned during training and represent the relative importance of each token in the input sequence.\n",
      "\n",
      "`Attention Score = (Softmax(score) * Attention Weights)`\n",
      " `Output = Attention Score (summed over all tokens)`\n",
      "\n",
      "# Advantages of Self-Attention in NLP\n",
      "## Improving Model Performance\n",
      "Self-attention enhances model performance in NLP tasks by allowing it to focus on relevant parts of the input sequence simultaneously, rather than relying solely on sequential information. This is particularly useful for tasks such as text classification, sentiment analysis, and machine translation.\n",
      "\n",
      "## Handling Contextual Relationships\n",
      "Self-attention helps models capture long-range dependencies and contextual relationships between words or tokens in a sentence. By weighing the importance of different token interactions, self-attention enables models to better understand nuances in language, leading to improved accuracy and reliability in NLP applications.\n",
      "\n",
      "## Overcoming Sequential Limitations\n",
      "In traditional sequential models, each input element is processed independently, which can lead to information being lost or not utilized effectively due to limitations in processing sequences. Self-attention mitigates these issues by enabling models to consider all elements simultaneously, thereby improving overall performance in tasks that require capturing complex relationships and patterns within input data.\n",
      "\n",
      "### Applications and Use Cases\n",
      "#### Real-World Applications\n",
      "Self-attention has been successfully applied in various real-world scenarios, including:\n",
      "\n",
      "* **Natural Language Processing (NLP)**: Self-attention is used in transformer-based models to process sequential data such as text or speech. This allows the model to understand the relationships between different words and phrases in a sentence.\n",
      "* **Computer Vision**: In computer vision applications, self-attention can be used to focus on specific regions of an image or video. For example, object detection tasks often rely on self-attention to identify and highlight relevant areas of interest.\n",
      "\n",
      "#### Use Cases\n",
      "Some notable use cases for self-attention include:\n",
      "\n",
      "* **Chatbots and Virtual Assistants**: Self-attention enables chatbots to understand the context and intent behind user queries, allowing for more accurate and informative responses.\n",
      "* **Image and Video Processing**: Self-attention can be used in image and video processing applications such as image segmentation, object tracking, and video analysis.\n",
      "\n",
      "### Challenges and Limitations\n",
      "#### Issues with Scaling\n",
      "Self-attention mechanisms have been shown to be highly effective for a wide range of natural language processing tasks, but they also come with significant challenges. One major issue is scaling, where the mechanism becomes computationally expensive as the input size increases. This has led researchers to explore alternative attention mechanisms that can scale more efficiently.\n",
      "\n",
      "#### Over-Sensitivity to Input Order\n",
      "Another challenge faced by self-attention is its sensitivity to input order. The model's performance can be affected significantly if the input sequence is not in a specific order, which can occur in real-world scenarios where data may be preprocessed or stored in a particular way. Researchers have been exploring techniques to mitigate this issue and develop more robust models.\n",
      "\n",
      "#### Lack of Interpretability\n",
      "Self-attention mechanisms are inherently difficult to interpret, making it challenging for humans to understand how the model is arriving at its predictions. This lack of interpretability can make it harder to trust the model and identify potential biases or errors. Researchers have been working on developing more interpretable attention mechanisms that can provide insights into the decision-making process of the model.\n",
      "\n",
      "# Future Trends and Conclusion\n",
      "\n",
      "As researchers continue to push the boundaries of transformer architecture, we can expect significant advancements in self-attention mechanisms. One area that holds great promise is the development of more efficient attention algorithms, such as those using sparse or low-rank approximations. These innovations could lead to substantial reductions in computational complexity and memory requirements, making transformers even more viable for large-scale applications.\n",
      "\n",
      "Another trend that may shape the future of self-attention is the integration of multimodal learning capabilities. As we move towards more complex tasks that involve multiple data types (e.g., text, images, audio), self-attention mechanisms will need to be adapted to effectively handle diverse input modalities. This could involve new attention architectures that can seamlessly switch between different modalities or learn shared representations across them.\n",
      "\n",
      "Ultimately, the continued evolution of self-attention in transformers holds immense potential for transforming natural language processing and other NLP tasks. By unlocking new levels of efficiency, multimodality, and expressiveness, we may soon see transformative applications across a wide range of domains. As researchers, it is essential that we continue to push the boundaries of what is possible with this technology, driving innovation and advancements that will shape the future of AI.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(result[\"final\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
