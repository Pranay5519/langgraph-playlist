{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e49117fe",
   "metadata": {},
   "source": [
    "# Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ec991c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from langchain_core.output_parsers import JsonOutputParser , PydanticOutputParser\n",
    "import operator\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import date, timedelta\n",
    "from typing import TypedDict, List, Optional, Literal, Annotated\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import Send\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d3707f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1) Schemas\n",
    "# -----------------------------\n",
    "class Task(BaseModel):\n",
    "    id: int\n",
    "    title: str\n",
    "\n",
    "    goal: str = Field(\n",
    "        ...,\n",
    "        description=\"One sentence describing what the reader should be able to do/understand after this section.\",\n",
    "    )\n",
    "    bullets: List[str] = Field(\n",
    "        ...,\n",
    "        min_length=3,\n",
    "        max_length=6,\n",
    "        description=\"5-7 concrete, non-overlapping subpoints to cover in this section.\",\n",
    "    )\n",
    "    target_words: int = Field(..., description=\"Target word count for this section (120–550).\")\n",
    "\n",
    "    tags: List[str] = Field(default_factory=list)\n",
    "    requires_research: bool = False\n",
    "    requires_citations: bool = False\n",
    "    requires_code: bool = False\n",
    "\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    blog_title: str\n",
    "    audience: str\n",
    "    tone: str\n",
    "    blog_kind: Literal[\"explainer\", \"tutorial\", \"news_roundup\", \"comparison\", \"system_design\"] = \"explainer\"\n",
    "    constraints: List[str] = Field(default_factory=list)\n",
    "    tasks: List[Task]\n",
    "\n",
    "\n",
    "class EvidenceItem(BaseModel):\n",
    "    title: Optional[str] = None\n",
    "    url: str\n",
    "    snippet: Optional[str] = None\n",
    "    published_at: Optional[str] = None\n",
    "    source: Optional[str] = None\n",
    "\n",
    "\n",
    "class RouterDecision(BaseModel):\n",
    "    needs_research: bool\n",
    "    mode: Literal[\"closed_book\", \"hybrid\", \"open_book\"]\n",
    "    queries: List[str] = Field(default_factory=list)\n",
    "\n",
    "\n",
    "class EvidencePack(BaseModel):\n",
    "    evidence: List[EvidenceItem] = Field(default_factory=list)\n",
    "    \n",
    "\n",
    "class ImageSpec(BaseModel):\n",
    "    placeholder: str = Field(..., description=\"e.g. [[IMAGE_1]]\")\n",
    "    filename: str = Field(..., description=\"Save under images/, e.g. qkv_flow.png\")\n",
    "    alt: str\n",
    "    caption: str\n",
    "    prompt: str = Field(..., description=\"Prompt to send to the image model.\")\n",
    "    size: Literal[\"1024x1024\", \"1024x1536\", \"1536x1024\"] = \"1024x1024\"\n",
    "    quality: Literal[\"low\", \"medium\", \"high\"] = \"medium\"\n",
    "\n",
    "\n",
    "class GlobalImagePlan(BaseModel):\n",
    "    md_with_placeholders: str\n",
    "    images: List[ImageSpec] = Field(default_factory=list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ce014d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    topic: str\n",
    "\n",
    "    # routing / research\n",
    "    mode: str\n",
    "    needs_research: bool\n",
    "    queries: List[str]\n",
    "    evidence: List[EvidenceItem]\n",
    "    plan: Optional[Plan]\n",
    "\n",
    "    # workers\n",
    "    sections: Annotated[List[tuple[int, str]], operator.add]  # (task_id, section_md)\n",
    "\n",
    "    # reducer/image\n",
    "    merged_md: str\n",
    "    md_with_placeholders: str\n",
    "    image_specs: List[dict]\n",
    "   \n",
    "    final: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e75ad9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_llm = ChatOllama(\n",
    "    model=\"qwen3:latest\",\n",
    "    format=\"json\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# LLM for content generation (without JSON mode)\n",
    "content_llm = ChatOllama(\n",
    "    model=\"qwen3:latest\",\n",
    "    temperature=0.7\n",
    ")\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "google_llm = ChatGoogleGenerativeAI(model =\"gemini-2.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f167dace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 3) Router (decide upfront)\n",
    "# -----------------------------\n",
    "ROUTER_SYSTEM = \"\"\"You are a routing module for a technical blog planner.\n",
    "\n",
    "Decide whether web research is needed BEFORE planning.\n",
    "\n",
    "Modes:\n",
    "- closed_book (needs_research=false):\n",
    "  Evergreen topics where correctness does not depend on recent facts (concepts, fundamentals).\n",
    "- hybrid (needs_research=true):\n",
    "  Mostly evergreen but needs up-to-date examples/tools/models to be useful.\n",
    "- open_book (needs_research=true):\n",
    "  Mostly volatile: weekly roundups, \"this week\", \"latest\", rankings, pricing, policy/regulation.\n",
    "\n",
    "If needs_research=true:\n",
    "- Output 3–10 high-signal queries.\n",
    "- Queries should be scoped and specific (avoid generic queries like just \"AI\" or \"LLM\").\n",
    "- If user asked for \"last week/this week/latest\", reflect that constraint IN THE QUERIES.\n",
    "\"\"\"\n",
    "def router_node(state: State) -> dict:\n",
    "\n",
    "    parser = PydanticOutputParser(pydantic_object=RouterDecision)\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", ROUTER_SYSTEM + \"\\n{format_instructions}\"),\n",
    "        (\"human\", \"{topic}\")\n",
    "    ])\n",
    "\n",
    "    prompt = prompt.partial(\n",
    "        format_instructions=parser.get_format_instructions()\n",
    "    )\n",
    "\n",
    "    chain = prompt | structured_llm | parser     # ✅ plain ChatOllama\n",
    "\n",
    "    decision = chain.invoke({\"topic\": state[\"topic\"]})\n",
    "\n",
    "    return {\n",
    "        \"needs_research\": decision.needs_research,\n",
    "        \"mode\": decision.mode,\n",
    "        \"queries\": decision.queries,\n",
    "    }\n",
    "\n",
    "def route_next(state: State) -> str:\n",
    "    return \"research\" if state[\"needs_research\"] else \"orchestrator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e6d7120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 4) Research (Tavily) \n",
    "# -----------------------------\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "def _tavily_search(query: str, max_results: int = 5) -> List[dict]:\n",
    "    \n",
    "    tool = TavilySearchResults(max_results=max_results)\n",
    "    results = tool.invoke({\"query\": query})\n",
    "\n",
    "    normalized: List[dict] = []\n",
    "    for r in results or []:\n",
    "        normalized.append({\n",
    "            \"title\": r.get(\"title\"),\n",
    "            \"url\": r.get(\"url\"),\n",
    "            \"snippet\": r.get(\"content\") or r.get(\"snippet\"),\n",
    "            \"published_at\": r.get(\"published_date\") or r.get(\"published_at\"),\n",
    "            \"source\": r.get(\"source\"),\n",
    "       })\n",
    "    return normalized\n",
    "\n",
    "\n",
    "RESEARCH_SYSTEM = \"\"\"You are a research synthesizer for technical writing.\n",
    "\n",
    "Given raw web search results, produce a deduplicated list of EvidenceItem objects.\n",
    "\n",
    "Rules:\n",
    "- Only include items with a non-empty url.\n",
    "- Prefer relevant + authoritative sources (company blogs, docs, reputable outlets).\n",
    "- If a published date is explicitly present in the result payload, keep it as YYYY-MM-DD.\n",
    "  If missing or unclear, set published_at=null. Do NOT guess.\n",
    "- Keep snippets short.\n",
    "- Deduplicate by URL.\n",
    "-Every EvidenceItem must contain the keys:\n",
    "    title, url, snippet, published_at, source.\n",
    "-If a value is missing, use null.\n",
    "\"\"\"\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "import json\n",
    "\n",
    "def research_node(state: State) -> dict:\n",
    "\n",
    "    queries = state.get(\"queries\", []) or []\n",
    "    max_results = 6\n",
    "\n",
    "    raw_results: List[dict] = []\n",
    "    for q in queries:\n",
    "        raw_results.extend(_tavily_search(q, max_results=max_results))\n",
    "\n",
    "    if not raw_results:\n",
    "        return {\"evidence\": []}\n",
    "\n",
    "    parser = PydanticOutputParser(pydantic_object=EvidencePack)\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", ROUTER_SYSTEM + \"\\n{format_instructions}\"),\n",
    "        (\"human\", \"{raw_results}\")\n",
    "    ])\n",
    "\n",
    "    prompt = prompt.partial(\n",
    "        format_instructions=parser.get_format_instructions()\n",
    "    )\n",
    "\n",
    "    chain = prompt | structured_llm | parser\n",
    "\n",
    "    pack = chain.invoke({\n",
    "        \"raw_results\": json.dumps(raw_results, ensure_ascii=False)\n",
    "    })\n",
    "\n",
    "    dedup = {}\n",
    "    for e in pack.evidence:\n",
    "        if e.url:\n",
    "            dedup[e.url] = e\n",
    "\n",
    "    return {\"evidence\": list(dedup.values())}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8330f31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 5) Orchestrator (Plan)\n",
    "# -----------------------------\n",
    "ORCH_SYSTEM = \"\"\"You are a senior technical writer and developer advocate.\n",
    "Your job is to produce a highly actionable outline for a technical blog post.\n",
    "\n",
    "Hard requirements:\n",
    "- Create 5-7 sections (tasks) suitable for the topic and audience.\n",
    "- Each task must include:\n",
    "  1) goal (1 sentence)\n",
    "  2) 5-7 bullets that are concrete, specific, and non-overlapping\n",
    "  3) target word count (120–550)\n",
    "\n",
    "Quality bar:\n",
    "- Assume the reader is a developer; use correct terminology.\n",
    "- Bullets must be actionable: build/compare/measure/verify/debug.\n",
    "- Ensure the overall plan includes at least 2 of these somewhere:\n",
    "  * minimal code sketch / MWE (set requires_code=True for that section)\n",
    "  * edge cases / failure modes\n",
    "  * performance/cost considerations\n",
    "  * security/privacy considerations (if relevant)\n",
    "  * debugging/observability tips\n",
    "\n",
    "Grounding rules:\n",
    "- Mode closed_book: keep it evergreen; do not depend on evidence.\n",
    "- Mode hybrid:\n",
    "  - Use evidence for up-to-date examples (models/tools/releases) in bullets.\n",
    "  - Mark sections using fresh info as requires_research=True and requires_citations=True.\n",
    "- Mode open_book:\n",
    "  - Set blog_kind = \"news_roundup\".\n",
    "  - Every section is about summarizing events + implications.\n",
    "  - DO NOT include tutorial/how-to sections unless user explicitly asked for that.\n",
    "  - If evidence is empty or insufficient, create a plan that transparently says \"insufficient sources\"\n",
    "    and includes only what can be supported.\n",
    "\n",
    "Output must strictly match the Plan schema.\n",
    "\"\"\"\n",
    "def orchestrator_node(state: State) -> dict:\n",
    "\n",
    "    evidence = state.get(\"evidence\", [])\n",
    "    mode = state.get(\"mode\", \"closed_book\")\n",
    "\n",
    "    parser = PydanticOutputParser(pydantic_object=Plan)\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", ORCH_SYSTEM + \"\\n{format_instructions}\"),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Topic: {topic}\\n\"\n",
    "            \"Mode: {mode}\\n\\n\"\n",
    "            \"Evidence (ONLY use for fresh claims; may be empty):\\n\"\n",
    "            \"{evidence}\"\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    prompt = prompt.partial(\n",
    "        format_instructions=parser.get_format_instructions()\n",
    "    )\n",
    "\n",
    "    chain = prompt | structured_llm | parser   # ✅ plain ChatOllama\n",
    "\n",
    "    plan = chain.invoke({\n",
    "        \"topic\": state[\"topic\"],\n",
    "        \"mode\": mode,\n",
    "        \"evidence\": [e.model_dump() for e in evidence][:16],\n",
    "    })\n",
    "\n",
    "    return {\"plan\": plan}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f82e3b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 6) Fanout\n",
    "# -----------------------------\n",
    "def fanout(state: State):\n",
    "    return [\n",
    "        Send(\n",
    "            \"worker\",\n",
    "            {\n",
    "                \"task\": task.model_dump(),\n",
    "                \"topic\": state[\"topic\"],\n",
    "                \"mode\": state[\"mode\"],\n",
    "                \"plan\": state[\"plan\"].model_dump(),\n",
    "                \"evidence\": [e.model_dump() for e in state.get(\"evidence\", [])],\n",
    "            },\n",
    "        )\n",
    "        for task in state[\"plan\"].tasks\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b7587b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 7) Worker (write one section)\n",
    "# -----------------------------\n",
    "WORKER_SYSTEM = \"\"\"You are a senior technical writer and developer advocate.\n",
    "Write ONE section of a technical blog post in Markdown.\n",
    "\n",
    "Hard constraints:\n",
    "- Follow the provided Goal and cover ALL Bullets in order (do not skip or merge bullets).\n",
    "- Stay close to Target words (±15%).\n",
    "- Output ONLY the section content in Markdown (no blog title H1, no extra commentary).\n",
    "- Start with a '## <Section Title>' heading.\n",
    "\n",
    "Scope guard:\n",
    "- If blog_kind == \"news_roundup\": do NOT turn this into a tutorial/how-to guide.\n",
    "  Do NOT teach web scraping, RSS, automation, or \"how to fetch news\" unless bullets explicitly ask for it.\n",
    "  Focus on summarizing events and implications.\n",
    "\n",
    "Grounding policy:\n",
    "- If mode == open_book:\n",
    "  - Do NOT introduce any specific event/company/model/funding/policy claim unless it is supported by provided Evidence URLs.\n",
    "  - For each event claim, attach a source as a Markdown link: ([Source](URL)).\n",
    "  - Only use URLs provided in Evidence. If not supported, write: \"Not found in provided sources.\"\n",
    "- If requires_citations == true:\n",
    "  - For outside-world claims, cite Evidence URLs the same way.\n",
    "- Evergreen reasoning is OK without citations unless requires_citations is true.\n",
    "\n",
    "Code:\n",
    "- If requires_code == true, include at least one minimal, correct code snippet relevant to the bullets.\n",
    "\n",
    "Style:\n",
    "- Short paragraphs, bullets where helpful, code fences for code.\n",
    "- Avoid fluff/marketing. Be precise and implementation-oriented.\n",
    "\"\"\"\n",
    "\n",
    "def worker_node(payload: dict) -> dict:\n",
    "    \n",
    "    task = Task(**payload[\"task\"])\n",
    "    plan = Plan(**payload[\"plan\"])\n",
    "    evidence = [EvidenceItem(**e) for e in payload.get(\"evidence\", [])]\n",
    "    topic = payload[\"topic\"]\n",
    "    mode = payload.get(\"mode\", \"closed_book\")\n",
    "\n",
    "    bullets_text = \"\\n- \" + \"\\n- \".join(task.bullets)\n",
    "\n",
    "    evidence_text = \"\"\n",
    "    if evidence:\n",
    "        evidence_text = \"\\n\".join(\n",
    "            f\"- {e.title} | {e.url} | {e.published_at or 'date:unknown'}\".strip()\n",
    "            for e in evidence[:20]\n",
    "        )\n",
    "\n",
    "    section_md = content_llm.invoke(\n",
    "        [\n",
    "            SystemMessage(content=WORKER_SYSTEM),\n",
    "            HumanMessage(\n",
    "                content=(\n",
    "                    f\"Blog title: {plan.blog_title}\\n\"\n",
    "                    f\"Audience: {plan.audience}\\n\"\n",
    "                    f\"Tone: {plan.tone}\\n\"\n",
    "                    f\"Blog kind: {plan.blog_kind}\\n\"\n",
    "                    f\"Constraints: {plan.constraints}\\n\"\n",
    "                    f\"Topic: {topic}\\n\"\n",
    "                    f\"Mode: {mode}\\n\\n\"\n",
    "                    f\"Section title: {task.title}\\n\"\n",
    "                    f\"Goal: {task.goal}\\n\"\n",
    "                    f\"Target words: {task.target_words}\\n\"\n",
    "                    f\"Tags: {task.tags}\\n\"\n",
    "                    f\"requires_research: {task.requires_research}\\n\"\n",
    "                    f\"requires_citations: {task.requires_citations}\\n\"\n",
    "                    f\"requires_code: {task.requires_code}\\n\"\n",
    "                    f\"Bullets:{bullets_text}\\n\\n\"\n",
    "                    f\"Evidence (ONLY use these URLs when citing):\\n{evidence_text}\\n\"\n",
    "                )\n",
    "            ),\n",
    "        ]\n",
    "    ).content.strip()\n",
    "\n",
    "    return {\"sections\": [(task.id, section_md)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78446a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 8) ReducerWithImages (subgraph)\n",
    "#    merge_content -> decide_images -> generate_and_place_images\n",
    "# ============================================================\n",
    "def merge_content(state: State) -> dict:\n",
    "\n",
    "    plan = state[\"plan\"]\n",
    "\n",
    "    ordered_sections = [md for _, md in sorted(state[\"sections\"], key=lambda x: x[0])]\n",
    "    body = \"\\n\\n\".join(ordered_sections).strip()\n",
    "    merged_md = f\"# {plan.blog_title}\\n\\n{body}\\n\"\n",
    "    return {\"merged_md\": merged_md}\n",
    "\n",
    "\n",
    "DECIDE_IMAGES_SYSTEM = \"\"\"You are an expert technical editor.\n",
    "Your ONLY task is to decide where images/diagrams should go in the existing blog content.\n",
    "\n",
    "CRITICAL RULES:\n",
    "1. You MUST preserve ALL original content verbatim - every word, every code block, every section\n",
    "2. ONLY insert image placeholders [[IMAGE_1]], [[IMAGE_2]], [[IMAGE_3]] at appropriate locations\n",
    "3. DO NOT rewrite, summarize, or remove ANY content\n",
    "4. DO NOT remove or modify code blocks\n",
    "5. Max 3 images total\n",
    "6. Each image must materially improve understanding (diagram/flow/architecture visual)\n",
    "7. If no images needed: return the EXACT input markdown unchanged with images=[]\n",
    "\n",
    "WORKFLOW:\n",
    "- Read the full markdown\n",
    "- Identify 0-3 locations where a technical diagram would help\n",
    "- Insert [[IMAGE_X]] placeholders at those locations\n",
    "- Return the COMPLETE markdown with placeholders inserted\n",
    "\n",
    "Return strictly GlobalImagePlan with:\n",
    "- md_with_placeholders: FULL original content + placeholders\n",
    "- images: list of image specifications\n",
    "\n",
    "Example of correct insertion:\n",
    "Original: \"## Section\\nText here.\\n\\nMore text.\"\n",
    "Correct: \"## Section\\nText here.\\n\\n[[IMAGE_1]]\\n\\nMore text.\"\n",
    "WRONG: \"## Section\\nSummary of text.\\n\\n[[IMAGE_1]]\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def decide_images(state: State) -> dict:\n",
    "    parser = PydanticOutputParser(pydantic_object=GlobalImagePlan)    \n",
    "    merged_md = state[\"merged_md\"]\n",
    "    plan = state[\"plan\"]\n",
    "    assert plan is not None\n",
    "    \n",
    "    # Check input size\n",
    "    input_tokens = len(merged_md) // 4  # rough estimate\n",
    "    print(f\"Input markdown size: {len(merged_md)} chars (~{input_tokens} tokens)\")\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", DECIDE_IMAGES_SYSTEM + \"\\n{format_instructions}\"),\n",
    "        (\"human\", \"Topic: {topic}\\n\"\n",
    "                  \"Blog kind: {blog_kind}\\n\\n\"\n",
    "                  \"Insert placeholders + propose image prompts.\\n\\n\"\n",
    "                  \"{merged_md}\")\n",
    "    ])\n",
    "    prompt = prompt.partial(\n",
    "        format_instructions=parser.get_format_instructions()\n",
    "    )\n",
    "    \n",
    "    # Use with_structured_output instead for better reliability\n",
    "    from langchain_core.output_parsers import JsonOutputParser\n",
    "    \n",
    "    try:\n",
    "        # Option A: Use structured output (more reliable)\n",
    "        chain = prompt | structured_llm.with_structured_output(GlobalImagePlan)\n",
    "        image_plan = chain.invoke({\n",
    "            \"topic\": state[\"topic\"],\n",
    "            \"blog_kind\": plan.blog_kind,\n",
    "            \"merged_md\": merged_md\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Image planning failed: {e}\")\n",
    "        print(\"Falling back to no images\")\n",
    "        # Fallback: return original without images\n",
    "        return {\n",
    "            \"md_with_placeholders\": merged_md,\n",
    "            \"image_specs\": [],\n",
    "        }\n",
    "\n",
    "    # Validation\n",
    "    if len(image_plan.md_with_placeholders) < len(merged_md) * 0.85:\n",
    "        print(f\"⚠️ Content loss detected: {len(merged_md)} → {len(image_plan.md_with_placeholders)}\")\n",
    "        return {\n",
    "            \"md_with_placeholders\": merged_md,\n",
    "            \"image_specs\": [],\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"md_with_placeholders\": image_plan.md_with_placeholders,\n",
    "        \"image_specs\": [img.model_dump() for img in image_plan.images],\n",
    "    }\n",
    "\n",
    "MARKDOWN_IMAGE_PROMPT = \"\"\"\n",
    "You are an AI system designer agent.\n",
    "\n",
    "Your task is to generate a clean ASCII diagram of the given system.\n",
    "\n",
    "Rules:\n",
    "- Use ONLY text characters like:\n",
    "  |  _  -  =  +  >  <\n",
    "- No markdown tables\n",
    "- No emojis\n",
    "- No explanation\n",
    "- Only the diagram\n",
    "\n",
    "Style:\n",
    "- Rectangular boxes\n",
    "- Clear arrows between components\n",
    "- Vertically aligned flow\n",
    "- One main pipeline\n",
    "\"\"\"\n",
    "\n",
    "def generate_and_place_images(state: State) -> dict:\n",
    "    \"\"\"Generate ASCII markdown diagrams and place them in markdown\"\"\"\n",
    "\n",
    "    plan = state[\"plan\"]\n",
    "    assert plan is not None\n",
    "\n",
    "    md = state.get(\"md_with_placeholders\") or state[\"merged_md\"]\n",
    "    image_specs = state.get(\"image_specs\", []) or []\n",
    "\n",
    "    if not image_specs:\n",
    "        out_file = f\"{plan.blog_title}.md\"\n",
    "        Path(out_file).write_text(md, encoding=\"utf-8\")\n",
    "        return {\"final\": md}\n",
    "\n",
    "    for spec in image_specs:\n",
    "        placeholder = spec[\"placeholder\"]\n",
    "\n",
    "        try:\n",
    "            resp = google_llm.invoke([\n",
    "                SystemMessage(content=MARKDOWN_IMAGE_PROMPT),\n",
    "                HumanMessage(content=spec[\"prompt\"])\n",
    "            ])\n",
    "\n",
    "            ascii_diagram = f\"```text\\n{resp.content.strip()}\\n```\"\n",
    "\n",
    "            md = md.replace(placeholder, \"\\n\" + ascii_diagram + \"\\n\")\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            prompt_block = (\n",
    "                f\"> **[IMAGE GENERATION FAILED]** {spec.get('caption','')}\\n>\\n\"\n",
    "                f\"> **Alt:** {spec.get('alt','')}\\n>\\n\"\n",
    "                f\"> **Prompt:** {spec.get('prompt','')}\\n>\\n\"\n",
    "                f\"> **Error:** {e}\\n\"\n",
    "            )\n",
    "            md = md.replace(placeholder, prompt_block)\n",
    "            print(f\"md image gen failed: {e}\")\n",
    "\n",
    "    out_file = f\"{plan.blog_title}.md\"\n",
    "    Path(out_file).write_text(md, encoding=\"utf-8\")\n",
    "\n",
    "    return {\"final\": md}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a639525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x0000021A644E6190>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build reducer subgraph\n",
    "reducer_graph = StateGraph(State)\n",
    "reducer_graph.add_node(\"merge_content\", merge_content)\n",
    "reducer_graph.add_node(\"decide_images\", decide_images)\n",
    "reducer_graph.add_node(\"generate_and_place_images\", generate_and_place_images)\n",
    "reducer_graph.add_edge(START, \"merge_content\")\n",
    "reducer_graph.add_edge(\"merge_content\", \"decide_images\")\n",
    "reducer_graph.add_edge(\"decide_images\", \"generate_and_place_images\")\n",
    "reducer_graph.add_edge(\"generate_and_place_images\", END)\n",
    "reducer_subgraph = reducer_graph.compile()\n",
    "reducer_subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb9f90b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x0000021A67378390>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 9) Build main graph\n",
    "# -----------------------------\n",
    "g = StateGraph(State)\n",
    "g.add_node(\"router\", router_node)\n",
    "g.add_node(\"research\", research_node)\n",
    "g.add_node(\"orchestrator\", orchestrator_node)\n",
    "g.add_node(\"worker\", worker_node)\n",
    "g.add_node(\"merge_content\", merge_content)\n",
    "g.add_node(\"decide_images\" , decide_images)\n",
    "g.add_node(\"generate_and_place_images\" , generate_and_place_images)\n",
    "\n",
    "g.add_edge(START, \"router\")\n",
    "g.add_conditional_edges(\"router\", route_next, {\"research\": \"research\", \"orchestrator\": \"orchestrator\"})\n",
    "g.add_edge(\"research\", \"orchestrator\")\n",
    "\n",
    "g.add_conditional_edges(\"orchestrator\", fanout, [\"worker\"])\n",
    "g.add_edge(\"worker\", \"merge_content\")\n",
    "g.add_edge(\"merge_content\", END)\n",
    "# g.add_edge(\"decide_images\",\"generate_and_place_images\" )\n",
    "# g.add_edge(\"generate_and_place_images\" , END)\n",
    "\n",
    "\n",
    "app = g.compile(debug= True)\n",
    "app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "96a1066f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m[values]\u001b[0m {'topic': 'Build End-to-End Production-Ready Systems Using LangGraph', 'sections': []}\n",
      "\u001b[1m[updates]\u001b[0m {'router': {'needs_research': True, 'mode': 'hybrid', 'queries': ['How to structure LangGraph workflows for production scalability and fault tolerance', 'Best practices for integrating LangGraph with Kubernetes for production deployment', 'Recent case studies of end-to-end LangGraph systems in production environments', 'Tools and frameworks for monitoring LangGraph applications in production', 'Security considerations for deploying LangGraph in enterprise production systems', 'Performance optimization techniques for LangGraph in high-throughput scenarios', 'Comparison of LangGraph deployment patterns vs. traditional ML pipelines in production', 'How to implement CI/CD pipelines for LangGraph-based applications', \"Recent advancements in LangGraph's state management for production workloads\", 'Cost optimization strategies for running LangGraph in production cloud environments']}}\n",
      "\u001b[1m[values]\u001b[0m {'topic': 'Build End-to-End Production-Ready Systems Using LangGraph', 'mode': 'hybrid', 'needs_research': True, 'queries': ['How to structure LangGraph workflows for production scalability and fault tolerance', 'Best practices for integrating LangGraph with Kubernetes for production deployment', 'Recent case studies of end-to-end LangGraph systems in production environments', 'Tools and frameworks for monitoring LangGraph applications in production', 'Security considerations for deploying LangGraph in enterprise production systems', 'Performance optimization techniques for LangGraph in high-throughput scenarios', 'Comparison of LangGraph deployment patterns vs. traditional ML pipelines in production', 'How to implement CI/CD pipelines for LangGraph-based applications', \"Recent advancements in LangGraph's state management for production workloads\", 'Cost optimization strategies for running LangGraph in production cloud environments'], 'sections': []}\n"
     ]
    },
    {
     "ename": "OutputParserException",
     "evalue": "Invalid json output: להתמודדות עם LangGraph ותפעולו ב坏境 של הייצור, חשוב להבין את המגוון של תכונות, תפעול, והוצאות. הנה סיכום מפורט:\n\n### 1. **תכונות ותפעול**\n- **תהליך תכנון ותפעול**: LangGraph מספק תהליך קל להתקנה ב坏境 של הייצור, במיוחד ביבליוטיקה כמו LangSmith וMCP (Model Context Protocol). תהליך זה מאפשר תכנון דינמי של תהליכי עבודה וניהול תקשורת עם מודלים.\n- **תפעול ב坏境 של הייצור**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהלع של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מсол.\n</think>\n\nIt seems like your message is incomplete or contains some errors. Could you please clarify or provide more context? For example, are you asking about a specific topic, or is there a particular question you'd like to explore? I'm here to help!\nFor troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\VS_CODE\\Langraph\\myvenv\\Lib\\site-packages\\langchain_core\\output_parsers\\json.py:84\u001b[39m, in \u001b[36mJsonOutputParser.parse_result\u001b[39m\u001b[34m(self, result, partial)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparse_json_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\VS_CODE\\Langraph\\myvenv\\Lib\\site-packages\\langchain_core\\utils\\json.py:164\u001b[39m, in \u001b[36mparse_json_markdown\u001b[39m\u001b[34m(json_string, parser)\u001b[39m\n\u001b[32m    163\u001b[39m     json_str = json_string \u001b[38;5;28;01mif\u001b[39;00m match \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m match.group(\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parse_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\VS_CODE\\Langraph\\myvenv\\Lib\\site-packages\\langchain_core\\utils\\json.py:194\u001b[39m, in \u001b[36m_parse_json\u001b[39m\u001b[34m(json_str, parser)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;66;03m# Parse the JSON string into a Python dictionary\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_str\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\VS_CODE\\Langraph\\myvenv\\Lib\\site-packages\\langchain_core\\utils\\json.py:137\u001b[39m, in \u001b[36mparse_partial_json\u001b[39m\u001b[34m(s, strict)\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;66;03m# If we got here, we ran out of characters to remove\u001b[39;00m\n\u001b[32m    135\u001b[39m \u001b[38;5;66;03m# and still couldn't parse the string as JSON, so return the parse error\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[38;5;66;03m# for the original string.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\Python311\\Lib\\json\\__init__.py:359\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    358\u001b[39m     kw[\u001b[33m'\u001b[39m\u001b[33mparse_constant\u001b[39m\u001b[33m'\u001b[39m] = parse_constant\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\Python311\\Lib\\json\\decoder.py:337\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    333\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[32m    334\u001b[39m \u001b[33;03mcontaining a JSON document).\u001b[39;00m\n\u001b[32m    335\u001b[39m \n\u001b[32m    336\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m end = _w(s, end).end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\Python311\\Lib\\json\\decoder.py:355\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOutputParserException\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m response = \u001b[43mapp\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtopic\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mBuild End-to-End Production-Ready Systems Using LangGraph\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\VS_CODE\\Langraph\\myvenv\\Lib\\site-packages\\langgraph\\pregel\\main.py:3071\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3068\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3069\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3071\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3072\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3073\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3074\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3076\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3077\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3078\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3079\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3080\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3081\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3082\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3083\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3084\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3085\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3086\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\VS_CODE\\Langraph\\myvenv\\Lib\\site-packages\\langgraph\\pregel\\main.py:2646\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2644\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2645\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2646\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2647\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2649\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2650\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2651\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2652\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2653\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2655\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2656\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\VS_CODE\\Langraph\\myvenv\\Lib\\site-packages\\langgraph\\pregel\\_runner.py:167\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    165\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\VS_CODE\\Langraph\\myvenv\\Lib\\site-packages\\langgraph\\pregel\\_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\VS_CODE\\Langraph\\myvenv\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:656\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    658\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\VS_CODE\\Langraph\\myvenv\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:400\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    398\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 67\u001b[39m, in \u001b[36mresearch_node\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     61\u001b[39m prompt = prompt.partial(\n\u001b[32m     62\u001b[39m     format_instructions=parser.get_format_instructions()\n\u001b[32m     63\u001b[39m )\n\u001b[32m     65\u001b[39m chain = prompt | structured_llm | parser\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m pack = \u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mraw_results\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m dedup = {}\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m pack.evidence:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\VS_CODE\\Langraph\\myvenv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3151\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3149\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3150\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3151\u001b[39m                 input_ = context.run(step.invoke, input_, config)\n\u001b[32m   3152\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3153\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\VS_CODE\\Langraph\\myvenv\\Lib\\site-packages\\langchain_core\\output_parsers\\base.py:201\u001b[39m, in \u001b[36mBaseOutputParser.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    195\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    198\u001b[39m     **kwargs: Any,\n\u001b[32m    199\u001b[39m ) -> T:\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, BaseMessage):\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    209\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_with_config(\n\u001b[32m    210\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m.parse_result([Generation(text=inner_input)]),\n\u001b[32m    211\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    212\u001b[39m         config,\n\u001b[32m    213\u001b[39m         run_type=\u001b[33m\"\u001b[39m\u001b[33mparser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    214\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\VS_CODE\\Langraph\\myvenv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2058\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   2054\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   2055\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   2056\u001b[39m         output = cast(\n\u001b[32m   2057\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m2058\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2059\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   2060\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2061\u001b[39m \u001b[43m                \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2062\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2063\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2064\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2065\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   2066\u001b[39m         )\n\u001b[32m   2067\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2068\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\VS_CODE\\Langraph\\myvenv\\Lib\\site-packages\\langchain_core\\runnables\\config.py:435\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    434\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m435\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\VS_CODE\\Langraph\\myvenv\\Lib\\site-packages\\langchain_core\\output_parsers\\base.py:202\u001b[39m, in \u001b[36mBaseOutputParser.invoke.<locals>.<lambda>\u001b[39m\u001b[34m(inner_input)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    195\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    198\u001b[39m     **kwargs: Any,\n\u001b[32m    199\u001b[39m ) -> T:\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, BaseMessage):\n\u001b[32m    201\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_with_config(\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m             \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    205\u001b[39m             \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    206\u001b[39m             config,\n\u001b[32m    207\u001b[39m             run_type=\u001b[33m\"\u001b[39m\u001b[33mparser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    208\u001b[39m         )\n\u001b[32m    209\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_with_config(\n\u001b[32m    210\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m.parse_result([Generation(text=inner_input)]),\n\u001b[32m    211\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    212\u001b[39m         config,\n\u001b[32m    213\u001b[39m         run_type=\u001b[33m\"\u001b[39m\u001b[33mparser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    214\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\VS_CODE\\Langraph\\myvenv\\Lib\\site-packages\\langchain_core\\output_parsers\\pydantic.py:74\u001b[39m, in \u001b[36mPydanticOutputParser.parse_result\u001b[39m\u001b[34m(self, result, partial)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Parse the result of an LLM call to a Pydantic object.\u001b[39;00m\n\u001b[32m     59\u001b[39m \n\u001b[32m     60\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     71\u001b[39m \u001b[33;03m    The parsed Pydantic object.\u001b[39;00m\n\u001b[32m     72\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     json_object = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._parse_obj(json_object)\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\Desktop\\VS_CODE\\Langraph\\myvenv\\Lib\\site-packages\\langchain_core\\output_parsers\\json.py:87\u001b[39m, in \u001b[36mJsonOutputParser.parse_result\u001b[39m\u001b[34m(self, result, partial)\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     86\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid json output: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(msg, llm_output=text) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mOutputParserException\u001b[39m: Invalid json output: להתמודדות עם LangGraph ותפעולו ב坏境 של הייצור, חשוב להבין את המגוון של תכונות, תפעול, והוצאות. הנה סיכום מפורט:\n\n### 1. **תכונות ותפעול**\n- **תהליך תכנון ותפעול**: LangGraph מספק תהליך קל להתקנה ב坏境 של הייצור, במיוחד ביבליוטיקה כמו LangSmith וMCP (Model Context Protocol). תהליך זה מאפשר תכנון דינמי של תהליכי עבודה וניהול תקשורת עם מודלים.\n- **תפעול ב坏境 של הייצור**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים וניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים וניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהלع של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מודלים.\n- **ניהול תקשורת עם מודלים**: LangGraph מאפשר תפעול ב坏境 של הייצור עם תהליך קל להתקנה, כולל תהליך של שדרוגים وניהול תקשורת עם מודלים. תהליך זה כולל תהליך של שדרוגים وניהול תקשורת עם מсол.\n</think>\n\nIt seems like your message is incomplete or contains some errors. Could you please clarify or provide more context? For example, are you asking about a specific topic, or is there a particular question you'd like to explore? I'm here to help!\nFor troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE ",
      "During task with name 'research' and id '61b42abf-31f8-384b-d14c-b261e2311239'"
     ]
    }
   ],
   "source": [
    "response = app.invoke({'topic' : \"Build End-to-End Production-Ready Systems Using LangGraph\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "54fd5b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# The Rise of AI: Transforming Renewable Energy Systems (2020–2026)\n",
      "\n",
      "## AI in Renewable Energy Forecasting\n",
      "\n",
      "Accurate forecasting of solar and wind energy generation is critical for grid stability and resource optimization. AI models, particularly time-series techniques, have become essential for predicting energy demand and generation patterns. For solar irradiance forecasting, a simple LSTM-based model can be implemented using TensorFlow/Keras. Below is a minimal example:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import tensorflow as tf\n",
      "from tensorflow.keras.models import Sequential\n",
      "from tensorflow.keras.layers import LSTM, Dense\n",
      "\n",
      "# Load and preprocess solar irradiance data (e.g., time-series with timestamps)\n",
      "# Ensure data is normalized and split into training/test sets\n",
      "model = Sequential([\n",
      "    LSTM(50, input_shape=(X_train.shape[1], 1)),\n",
      "    Dense(1)\n",
      "])\n",
      "model.compile(optimizer='adam', loss='mse')\n",
      "model.fit(X_train, y_train, epochs=20, validation_split=0.2)\n",
      "```\n",
      "\n",
      "Traditional ARIMA models often struggle with non-linear patterns in wind power data. Hybrid ML models, combining statistical methods with deep learning (e.g., ARIMA + LSTM), consistently achieve lower Mean Absolute Error (MAE) than standalone approaches. Studies from 2020 (DOE’s SETO) reported baseline MAE around 12%, while 2024 ScienceDirect research demonstrated improvements to ~8% using advanced hybrid architectures ([Source](https://www.sciencedirect.com/science/article/pii/S2405896324000123)).\n",
      "\n",
      "Data quality remains a persistent challenge. Missing timestamps in historical datasets can skew predictions; address this via interpolation or temporal alignment tools. For edge cases like sudden weather shifts, validate model robustness using cross-validation and synthetic data augmentation to simulate extreme conditions.\n",
      "\n",
      "## Grid Optimization with Deep Learning\n",
      "\n",
      "AI-driven grid optimization has emerged as a critical enabler for managing the variability of renewable energy sources. A 2024 ScienceDirect study ([Source](https://www.sciencedirect.com/science/article/pii/S2405896324000123)) highlights hybrid models combining neural networks with traditional control theory, achieving a 15% improvement in load balancing compared to conventional methods. These models excel at predicting demand fluctuations and integrating intermittent solar/wind generation, though their computational overhead requires careful resource allocation.\n",
      "\n",
      "Reinforcement learning (RL) offers dynamic adaptation to grid conditions but incurs higher latency and training costs than rule-based systems. For instance, RL algorithms may struggle with real-time constraints in microgrids, whereas rule-based approaches provide deterministic responses at lower computational expense. The tradeoff hinges on system scale and the complexity of operational scenarios—RL is preferable for large, heterogeneous grids, while rule-based systems suffice for localized, predictable environments.\n",
      "\n",
      "Simulating edge cases like sudden solar panel outages is essential for robustness. Tools like PyPSA and Pandapower ([Source](https://www.sciencedirect.com/science/article/pii/S2213146X2600015X)) enable detailed power flow analysis, allowing developers to stress-test grid resilience under extreme conditions. For example, a 2026 study demonstrated how these tools can model cascading failures in decentralized networks, ensuring fail-safe protocols are validated before deployment.\n",
      "\n",
      "Real-time AI decision-making introduces latency challenges, particularly in microgrids where milliseconds matter. A 2020 DOE report ([Source](https://www.energy.gov/eere/solar/does-seto-2020-funding-program)) emphasized that AI latency must remain below 50ms for stable operation, requiring optimized model architectures and edge computing infrastructure. Techniques like model quantization and federated learning are increasingly adopted to balance speed and accuracy.\n",
      "\n",
      "Decentralized grid systems face heightened security risks, including DDoS attacks on communication nodes. A 2020 review ([Source](https://link.springer.com/book/10.1007/978-3-030-50144-6)) underscores the need for end-to-end encryption and anomaly detection algorithms to mitigate threats. While no single solution addresses all vulnerabilities, combining AI-driven monitoring with zero-trust architectures is critical for securing modern energy infrastructures.\n",
      "\n",
      "## Energy Storage System Innovations  \n",
      "\n",
      "Recent advancements in machine learning (ML) have significantly enhanced the optimization of energy storage systems, particularly in battery state-of-charge (SoC) prediction and grid integration. A 2026 ScienceDirect paper ([Source](https://www.sciencedirect.com/science/article/pii/S2213146X2600015X)) highlights ML-driven models that improve SoC accuracy by integrating temporal and environmental data, such as temperature and load patterns. These models leverage recurrent neural networks (RNNs) and transformer architectures to predict battery degradation and optimize charge-discharge cycles, reducing energy loss and extending lifespan.  \n",
      "\n",
      "To compare the economic viability of lithium-ion and flow batteries, developers can build a cost model using Python’s Pandas library. By aggregating data on capital expenditures (CapEx), operational costs, and energy efficiency, the model can quantify total cost of ownership (TCO) over time. For example, flow batteries often exhibit lower degradation rates but higher upfront costs, while lithium-ion systems offer faster response times at the expense of longevity. Pandas enables efficient data manipulation, allowing for scenario analysis under varying demand and policy conditions.  \n",
      "\n",
      "Scalability challenges in distributed storage networks, such as Kubernetes clusters, require careful debugging of communication latency and resource allocation. Issues like uneven load distribution or API bottlenecks can degrade system performance. Solutions include implementing dynamic scheduling algorithms and optimizing containerized microservices to ensure seamless horizontal scaling. Monitoring tools like Prometheus can further aid in identifying and resolving bottlenecks in real-time.  \n",
      "\n",
      "Measuring the return on investment (ROI) of AI-powered storage systems versus traditional methods involves analyzing key metrics like energy efficiency, downtime reduction, and grid stability. AI systems often demonstrate a 15–30% improvement in ROI within five years due to predictive maintenance and demand forecasting capabilities. However, traditional methods may still excel in low-complexity scenarios with minimal computational overhead.  \n",
      "\n",
      "Privacy risks in IoT-enabled smart meters, such as data leakage through unencrypted communication, demand rigorous validation of security protocols. Developers must ensure end-to-end encryption, regular firmware updates, and anonymization of meter data to mitigate vulnerabilities. Compliance with standards like IEEE 1547 for grid interoperability further strengthens system security while maintaining operational efficiency.\n",
      "\n",
      "## AI-Driven Solar and Wind Projects  \n",
      "\n",
      "The integration of AI into solar and wind energy systems has accelerated since 2020, with significant advancements in efficiency, maintenance, and grid compatibility. The U.S. Department of Energy’s (DOE) Solar Energy Technologies Office (SETO) allocated $30 million in 2020 to develop AI tools for photovoltaic (PV) module efficiency, focusing on materials science and real-time performance optimization ([DOE's SETO 2020 Funding Program](https://www.energy.gov/eere/solar/does-seto-2020-funding-program)). By 2024, hybrid models combining physics-based simulations with machine learning outperformed standalone ML approaches for concentrated solar power (CSP) plants, achieving 15% higher accuracy in predicting thermal efficiency ([Recent Advances in Machine Learning for Renewable Energy Systems](https://www.sciencedirect.com/science/article/pii/S2405896324000123)).  \n",
      "\n",
      "AI-driven predictive maintenance in wind turbines has demonstrated a 22% reduction in downtime, with ROI typically realized within 18–24 months through reduced repair costs and extended asset lifespans. Debugging sensor data anomalies in remote solar farms now leverages federated learning frameworks, enabling decentralized anomaly detection without compromising data privacy. Finally, 2026 grid integration standards mandate AI system compliance with dynamic load-balancing protocols, emphasizing interoperability and cybersecurity, as outlined in the latest guidelines ([Machine Learning for Energy Storage and Grid Integration](https://www.sciencedirect.com/science/article/pii/S2213146X2600015X)). These developments underscore AI’s role in making renewable energy systems more resilient and scalable.\n",
      "\n",
      "## Challenges and Future Directions\n",
      "\n",
      "AI’s integration into renewable energy systems has unlocked transformative potential, but persistent challenges require careful attention. **Model bias in energy forecasting** remains a critical risk, particularly for marginalized regions. Historical data often underrepresents these areas, leading to inaccurate predictions that can skew resource allocation. For example, studies highlight how biased training data may fail to account for localized weather patterns or grid infrastructure disparities, risking inequitable energy distribution. Addressing this requires auditable datasets and fairness-aware algorithms, as emphasized in foundational research from 2020.\n",
      "\n",
      "**Computational costs** of real-time AI deployment in low-resource areas pose another hurdle. While edge computing reduces latency, the energy and hardware demands of AI models can strain aging infrastructure. A 2026 study on grid integration underscores the need for lightweight, energy-efficient architectures, such as quantized neural networks, to balance performance with operational feasibility.\n",
      "\n",
      "**Privacy-preserving AI** presents a trade-off between centralized systems and federated learning. Centralized models offer scalability but risk data exposure, while federated learning enables decentralized training. However, the latter faces challenges in model accuracy and communication overhead. A 2024 review suggests hybrid approaches—combining federated learning for sensitive data with centralized systems for non-sensitive analytics—may offer a pragmatic middle ground.\n",
      "\n",
      "**Interoperability** between legacy systems and AI controllers remains a technical bottleneck. Many existing grids rely on outdated protocols, requiring middleware or standardized APIs for seamless integration. Developers must prioritize backward compatibility while designing AI controllers, ensuring they can adapt to legacy infrastructure without compromising performance.\n",
      "\n",
      "Finally, **ethical implications** of AI-driven energy rationing demand rigorous scrutiny. Automated policies, while efficient, risk exacerbating inequalities if not designed with transparency. For instance, prioritizing high-demand sectors without considering social equity could deepen energy poverty. Future work must embed fairness metrics into decision-making frameworks, ensuring AI aligns with both technical and societal goals.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response[\"merged_md\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449be65c",
   "metadata": {},
   "source": [
    "# Debug Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "25842f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "state = {\n",
    "    \"topic\": \"Building a Complex, Production-Ready RAG System\",\n",
    "    \"mode\": \"\",\n",
    "    \"needs_research\": False,\n",
    "    \"queries\": [],\n",
    "    \"evidence\": [],\n",
    "    \"plan\": None,\n",
    "    \"as_of\": None,\n",
    "    \"recency_days\": 7,\n",
    "    \"sections\": [],\n",
    "    \"merged_md\": \"\",\n",
    "    \"md_with_placeholders\": \"\",\n",
    "    \"image_specs\": [],\n",
    "    \"final\": \"\",\n",
    "}\n",
    "\n",
    "def apply_patch(state, patch):\n",
    "    if patch is None:\n",
    "        return state\n",
    "    for k, v in patch.items():\n",
    "        state[k] = v\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97735739",
   "metadata": {},
   "source": [
    "**router**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1ce5aa0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RUN router ===\n",
      "State after router:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'topic': 'Building a Complex, Production-Ready RAG System',\n",
       " 'mode': 'hybrid',\n",
       " 'needs_research': True,\n",
       " 'queries': ['Best practices for implementing a production-ready Retrieval-Augmented Generation system with current LLM models',\n",
       "  'Tools and frameworks for building scalable RAG pipelines, such as LangChain and Hugging Face',\n",
       "  'Performance optimization techniques for RAG systems, including vector database selection and query efficiency',\n",
       "  'Deployment strategies for RAG models in cloud environments like AWS or Google Cloud',\n",
       "  'Evaluation metrics and monitoring tools for production RAG systems to ensure accuracy and reliability',\n",
       "  'Common pitfalls and solutions when building RAG systems, based on recent case studies',\n",
       "  'Integration of RAG with external APIs and data sources for real-time information retrieval'],\n",
       " 'evidence': [],\n",
       " 'plan': None,\n",
       " 'as_of': None,\n",
       " 'recency_days': 7,\n",
       " 'sections': [],\n",
       " 'merged_md': '',\n",
       " 'md_with_placeholders': '',\n",
       " 'image_specs': [],\n",
       " 'final': ''}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"=== RUN router ===\")\n",
    "\n",
    "patch = router_node(state)\n",
    "state = apply_patch(state, patch)\n",
    "\n",
    "print(\"State after router:\")\n",
    "state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa98edcf",
   "metadata": {},
   "source": [
    "**router next**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d5ba78b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Router decided next node: research\n"
     ]
    }
   ],
   "source": [
    "next_node = route_next(state)\n",
    "print(\"Router decided next node:\", next_node)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70cacea",
   "metadata": {},
   "source": [
    "**Run Research or Else Run Orchestrator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7e382924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RUN research ===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'topic': 'Building a Complex, Production-Ready RAG System',\n",
       " 'mode': 'hybrid',\n",
       " 'needs_research': True,\n",
       " 'queries': ['Best practices for implementing a production-ready Retrieval-Augmented Generation system with current LLM models',\n",
       "  'Tools and frameworks for building scalable RAG pipelines, such as LangChain and Hugging Face',\n",
       "  'Performance optimization techniques for RAG systems, including vector database selection and query efficiency',\n",
       "  'Deployment strategies for RAG models in cloud environments like AWS or Google Cloud',\n",
       "  'Evaluation metrics and monitoring tools for production RAG systems to ensure accuracy and reliability',\n",
       "  'Common pitfalls and solutions when building RAG systems, based on recent case studies',\n",
       "  'Integration of RAG with external APIs and data sources for real-time information retrieval'],\n",
       " 'evidence': [EvidenceItem(title='Retrieval-Augmented Generation (RAG) with Azure OpenAI Realtime API', url='https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/rag', published_at='2023-10-10', snippet='Retrieval-Augmented Generation (RAG) combines retrieval of external data with generative models to provide more accurate and up-to-date answers. The Azure OpenAI Realtime API enables real-time data integration through external retrieval components.', source='Microsoft Learn'),\n",
       "  EvidenceItem(title='Realtime API for Voice and Chat Applications', url='https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/realtime-api', published_at='2023-11-01', snippet='The Realtime API allows for dynamic interaction with models, supporting features like voice and chat. It requires external systems to handle data retrieval and injection into conversations.', source='Microsoft Learn'),\n",
       "  EvidenceItem(title='Challenges in Real-Time Data Integration', url='https://www.microsoft.com/en-us/research/publication/real-time-data-integration-challenges/', published_at='2023-09-15', snippet='Real-time data integration faces challenges including latency, data freshness, and security. External retrieval systems must ensure efficient and secure data flow to generative models.', source='Microsoft Research')],\n",
       " 'plan': None,\n",
       " 'as_of': None,\n",
       " 'recency_days': 7,\n",
       " 'sections': [],\n",
       " 'merged_md': '',\n",
       " 'md_with_placeholders': '',\n",
       " 'image_specs': [],\n",
       " 'final': ''}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if next_node == \"research\":\n",
    "    print(\"=== RUN research ===\")\n",
    "    patch = research_node(state)\n",
    "    state = apply_patch(state, patch)\n",
    "\n",
    "elif next_node == \"orchestrator\":\n",
    "    print(\"=== RUN orchestrator ===\")\n",
    "    patch = orchestrator_node(state)\n",
    "    state = apply_patch(state, patch)\n",
    "\n",
    "state\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5dd986",
   "metadata": {},
   "source": [
    "**Run orchestrator if research Node ran previously**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5cfdff31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NExt Node :  research\n",
      "=== RUN orchestrator (after research) ===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'topic': 'Building a Complex, Production-Ready RAG System',\n",
       " 'mode': 'hybrid',\n",
       " 'needs_research': True,\n",
       " 'queries': ['Best practices for implementing a production-ready Retrieval-Augmented Generation system with current LLM models',\n",
       "  'Tools and frameworks for building scalable RAG pipelines, such as LangChain and Hugging Face',\n",
       "  'Performance optimization techniques for RAG systems, including vector database selection and query efficiency',\n",
       "  'Deployment strategies for RAG models in cloud environments like AWS or Google Cloud',\n",
       "  'Evaluation metrics and monitoring tools for production RAG systems to ensure accuracy and reliability',\n",
       "  'Common pitfalls and solutions when building RAG systems, based on recent case studies',\n",
       "  'Integration of RAG with external APIs and data sources for real-time information retrieval'],\n",
       " 'evidence': [EvidenceItem(title='Retrieval-Augmented Generation (RAG) with Azure OpenAI Realtime API', url='https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/rag', published_at='2023-10-10', snippet='Retrieval-Augmented Generation (RAG) combines retrieval of external data with generative models to provide more accurate and up-to-date answers. The Azure OpenAI Realtime API enables real-time data integration through external retrieval components.', source='Microsoft Learn'),\n",
       "  EvidenceItem(title='Realtime API for Voice and Chat Applications', url='https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/realtime-api', published_at='2023-11-01', snippet='The Realtime API allows for dynamic interaction with models, supporting features like voice and chat. It requires external systems to handle data retrieval and injection into conversations.', source='Microsoft Learn'),\n",
       "  EvidenceItem(title='Challenges in Real-Time Data Integration', url='https://www.microsoft.com/en-us/research/publication/real-time-data-integration-challenges/', published_at='2023-09-15', snippet='Real-time data integration faces challenges including latency, data freshness, and security. External retrieval systems must ensure efficient and secure data flow to generative models.', source='Microsoft Research')],\n",
       " 'plan': Plan(blog_title='Building a Complex, Production-Ready RAG System: A Hybrid Approach', audience='Developers and data scientists with intermediate to advanced AI/ML experience', tone='Technical yet approachable, balancing conceptual clarity with practical implementation details', blog_kind='tutorial', constraints=['Focus on hybrid architecture combining retrieval and generation', 'Prioritize production-ready considerations', 'Integrate evidence from provided sources'], tasks=[Task(id=1, title='Understanding RAG Fundamentals', goal='Define Retrieval-Augmented Generation (RAG) and its core components', bullets=['Explain how RAG combines retrieval systems with generative models', \"Highlight Azure OpenAI Realtime API's role in enabling hybrid workflows\", 'Discuss the tradeoffs between pure retrieval vs. pure generation approaches', \"Reference Microsoft Learn's RAG documentation for technical context\"], target_words=250, tags=['concept', 'architecture'], requires_research=True, requires_citations=True, requires_code=False), Task(id=2, title='Designing the Hybrid Architecture', goal='Outline the key components of a production-ready RAG system', bullets=['Break down the retrieval pipeline (indexing, querying, ranking)', 'Detail the generation pipeline (prompt engineering, model integration)', 'Explain real-time data injection via Azure Realtime API', 'Address latency and throughput considerations', 'Tag: system_design'], target_words=350, tags=['system_design', 'architecture'], requires_research=True, requires_citations=True, requires_code=False), Task(id=3, title='Implementing Data Retrieval Strategies', goal='Explore methods for efficient and secure data retrieval', bullets=['Compare vector databases vs. traditional search engines', 'Implement caching mechanisms for frequent queries', 'Address data freshness vs. latency tradeoffs', 'Secure data flow with encryption and access controls', 'Reference Microsoft Research on real-time integration challenges'], target_words=400, tags=['data_engineering', 'security'], requires_research=True, requires_citations=True, requires_code=False), Task(id=4, title='Integrating with Generative Models', goal='Demonstrate how to connect retrieval systems with LLMs', bullets=['Show prompt templating for context-aware generation', 'Implement response filtering and validation', 'Handle hallucination detection mechanisms', 'Optimize for real-time inference latency', 'Tag: implementation'], target_words=300, tags=['implementation', 'llm_integration'], requires_research=False, requires_citations=False, requires_code=True), Task(id=5, title='Production-Ready Considerations', goal='Address scalability and monitoring requirements', bullets=['Design for horizontal scaling with microservices', 'Implement observability with metrics and logging', 'Create rollback strategies for model updates', 'Plan for disaster recovery and data backups', 'Tag: operations'], target_words=350, tags=['operations', 'scaling'], requires_research=False, requires_citations=False, requires_code=False)]),\n",
       " 'as_of': None,\n",
       " 'recency_days': 7,\n",
       " 'sections': [],\n",
       " 'merged_md': '',\n",
       " 'md_with_placeholders': '',\n",
       " 'image_specs': [],\n",
       " 'final': ''}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"NExt Node : \" , next_node)\n",
    "if next_node == \"research\":\n",
    "    print(\"=== RUN orchestrator (after research) ===\")\n",
    "    patch = orchestrator_node(state)\n",
    "    state = apply_patch(state, patch)\n",
    "\n",
    "state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb48e5e8",
   "metadata": {},
   "source": [
    "**fanout : split tasks for workers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9c2253b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fanout returned: [Send(node='worker', arg={'task': {'id': 1, 'title': 'Understanding RAG Fundamentals', 'goal': 'Define Retrieval-Augmented Generation (RAG) and its core components', 'bullets': ['Explain how RAG combines retrieval systems with generative models', \"Highlight Azure OpenAI Realtime API's role in enabling hybrid workflows\", 'Discuss the tradeoffs between pure retrieval vs. pure generation approaches', \"Reference Microsoft Learn's RAG documentation for technical context\"], 'target_words': 250, 'tags': ['concept', 'architecture'], 'requires_research': True, 'requires_citations': True, 'requires_code': False}, 'topic': 'Building a Complex, Production-Ready RAG System', 'mode': 'hybrid', 'plan': {'blog_title': 'Building a Complex, Production-Ready RAG System: A Hybrid Approach', 'audience': 'Developers and data scientists with intermediate to advanced AI/ML experience', 'tone': 'Technical yet approachable, balancing conceptual clarity with practical implementation details', 'blog_kind': 'tutorial', 'constraints': ['Focus on hybrid architecture combining retrieval and generation', 'Prioritize production-ready considerations', 'Integrate evidence from provided sources'], 'tasks': [{'id': 1, 'title': 'Understanding RAG Fundamentals', 'goal': 'Define Retrieval-Augmented Generation (RAG) and its core components', 'bullets': ['Explain how RAG combines retrieval systems with generative models', \"Highlight Azure OpenAI Realtime API's role in enabling hybrid workflows\", 'Discuss the tradeoffs between pure retrieval vs. pure generation approaches', \"Reference Microsoft Learn's RAG documentation for technical context\"], 'target_words': 250, 'tags': ['concept', 'architecture'], 'requires_research': True, 'requires_citations': True, 'requires_code': False}, {'id': 2, 'title': 'Designing the Hybrid Architecture', 'goal': 'Outline the key components of a production-ready RAG system', 'bullets': ['Break down the retrieval pipeline (indexing, querying, ranking)', 'Detail the generation pipeline (prompt engineering, model integration)', 'Explain real-time data injection via Azure Realtime API', 'Address latency and throughput considerations', 'Tag: system_design'], 'target_words': 350, 'tags': ['system_design', 'architecture'], 'requires_research': True, 'requires_citations': True, 'requires_code': False}, {'id': 3, 'title': 'Implementing Data Retrieval Strategies', 'goal': 'Explore methods for efficient and secure data retrieval', 'bullets': ['Compare vector databases vs. traditional search engines', 'Implement caching mechanisms for frequent queries', 'Address data freshness vs. latency tradeoffs', 'Secure data flow with encryption and access controls', 'Reference Microsoft Research on real-time integration challenges'], 'target_words': 400, 'tags': ['data_engineering', 'security'], 'requires_research': True, 'requires_citations': True, 'requires_code': False}, {'id': 4, 'title': 'Integrating with Generative Models', 'goal': 'Demonstrate how to connect retrieval systems with LLMs', 'bullets': ['Show prompt templating for context-aware generation', 'Implement response filtering and validation', 'Handle hallucination detection mechanisms', 'Optimize for real-time inference latency', 'Tag: implementation'], 'target_words': 300, 'tags': ['implementation', 'llm_integration'], 'requires_research': False, 'requires_citations': False, 'requires_code': True}, {'id': 5, 'title': 'Production-Ready Considerations', 'goal': 'Address scalability and monitoring requirements', 'bullets': ['Design for horizontal scaling with microservices', 'Implement observability with metrics and logging', 'Create rollback strategies for model updates', 'Plan for disaster recovery and data backups', 'Tag: operations'], 'target_words': 350, 'tags': ['operations', 'scaling'], 'requires_research': False, 'requires_citations': False, 'requires_code': False}]}, 'evidence': [{'title': 'Retrieval-Augmented Generation (RAG) with Azure OpenAI Realtime API', 'url': 'https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/rag', 'published_at': '2023-10-10', 'snippet': 'Retrieval-Augmented Generation (RAG) combines retrieval of external data with generative models to provide more accurate and up-to-date answers. The Azure OpenAI Realtime API enables real-time data integration through external retrieval components.', 'source': 'Microsoft Learn'}, {'title': 'Realtime API for Voice and Chat Applications', 'url': 'https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/realtime-api', 'published_at': '2023-11-01', 'snippet': 'The Realtime API allows for dynamic interaction with models, supporting features like voice and chat. It requires external systems to handle data retrieval and injection into conversations.', 'source': 'Microsoft Learn'}, {'title': 'Challenges in Real-Time Data Integration', 'url': 'https://www.microsoft.com/en-us/research/publication/real-time-data-integration-challenges/', 'published_at': '2023-09-15', 'snippet': 'Real-time data integration faces challenges including latency, data freshness, and security. External retrieval systems must ensure efficient and secure data flow to generative models.', 'source': 'Microsoft Research'}]}), Send(node='worker', arg={'task': {'id': 2, 'title': 'Designing the Hybrid Architecture', 'goal': 'Outline the key components of a production-ready RAG system', 'bullets': ['Break down the retrieval pipeline (indexing, querying, ranking)', 'Detail the generation pipeline (prompt engineering, model integration)', 'Explain real-time data injection via Azure Realtime API', 'Address latency and throughput considerations', 'Tag: system_design'], 'target_words': 350, 'tags': ['system_design', 'architecture'], 'requires_research': True, 'requires_citations': True, 'requires_code': False}, 'topic': 'Building a Complex, Production-Ready RAG System', 'mode': 'hybrid', 'plan': {'blog_title': 'Building a Complex, Production-Ready RAG System: A Hybrid Approach', 'audience': 'Developers and data scientists with intermediate to advanced AI/ML experience', 'tone': 'Technical yet approachable, balancing conceptual clarity with practical implementation details', 'blog_kind': 'tutorial', 'constraints': ['Focus on hybrid architecture combining retrieval and generation', 'Prioritize production-ready considerations', 'Integrate evidence from provided sources'], 'tasks': [{'id': 1, 'title': 'Understanding RAG Fundamentals', 'goal': 'Define Retrieval-Augmented Generation (RAG) and its core components', 'bullets': ['Explain how RAG combines retrieval systems with generative models', \"Highlight Azure OpenAI Realtime API's role in enabling hybrid workflows\", 'Discuss the tradeoffs between pure retrieval vs. pure generation approaches', \"Reference Microsoft Learn's RAG documentation for technical context\"], 'target_words': 250, 'tags': ['concept', 'architecture'], 'requires_research': True, 'requires_citations': True, 'requires_code': False}, {'id': 2, 'title': 'Designing the Hybrid Architecture', 'goal': 'Outline the key components of a production-ready RAG system', 'bullets': ['Break down the retrieval pipeline (indexing, querying, ranking)', 'Detail the generation pipeline (prompt engineering, model integration)', 'Explain real-time data injection via Azure Realtime API', 'Address latency and throughput considerations', 'Tag: system_design'], 'target_words': 350, 'tags': ['system_design', 'architecture'], 'requires_research': True, 'requires_citations': True, 'requires_code': False}, {'id': 3, 'title': 'Implementing Data Retrieval Strategies', 'goal': 'Explore methods for efficient and secure data retrieval', 'bullets': ['Compare vector databases vs. traditional search engines', 'Implement caching mechanisms for frequent queries', 'Address data freshness vs. latency tradeoffs', 'Secure data flow with encryption and access controls', 'Reference Microsoft Research on real-time integration challenges'], 'target_words': 400, 'tags': ['data_engineering', 'security'], 'requires_research': True, 'requires_citations': True, 'requires_code': False}, {'id': 4, 'title': 'Integrating with Generative Models', 'goal': 'Demonstrate how to connect retrieval systems with LLMs', 'bullets': ['Show prompt templating for context-aware generation', 'Implement response filtering and validation', 'Handle hallucination detection mechanisms', 'Optimize for real-time inference latency', 'Tag: implementation'], 'target_words': 300, 'tags': ['implementation', 'llm_integration'], 'requires_research': False, 'requires_citations': False, 'requires_code': True}, {'id': 5, 'title': 'Production-Ready Considerations', 'goal': 'Address scalability and monitoring requirements', 'bullets': ['Design for horizontal scaling with microservices', 'Implement observability with metrics and logging', 'Create rollback strategies for model updates', 'Plan for disaster recovery and data backups', 'Tag: operations'], 'target_words': 350, 'tags': ['operations', 'scaling'], 'requires_research': False, 'requires_citations': False, 'requires_code': False}]}, 'evidence': [{'title': 'Retrieval-Augmented Generation (RAG) with Azure OpenAI Realtime API', 'url': 'https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/rag', 'published_at': '2023-10-10', 'snippet': 'Retrieval-Augmented Generation (RAG) combines retrieval of external data with generative models to provide more accurate and up-to-date answers. The Azure OpenAI Realtime API enables real-time data integration through external retrieval components.', 'source': 'Microsoft Learn'}, {'title': 'Realtime API for Voice and Chat Applications', 'url': 'https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/realtime-api', 'published_at': '2023-11-01', 'snippet': 'The Realtime API allows for dynamic interaction with models, supporting features like voice and chat. It requires external systems to handle data retrieval and injection into conversations.', 'source': 'Microsoft Learn'}, {'title': 'Challenges in Real-Time Data Integration', 'url': 'https://www.microsoft.com/en-us/research/publication/real-time-data-integration-challenges/', 'published_at': '2023-09-15', 'snippet': 'Real-time data integration faces challenges including latency, data freshness, and security. External retrieval systems must ensure efficient and secure data flow to generative models.', 'source': 'Microsoft Research'}]}), Send(node='worker', arg={'task': {'id': 3, 'title': 'Implementing Data Retrieval Strategies', 'goal': 'Explore methods for efficient and secure data retrieval', 'bullets': ['Compare vector databases vs. traditional search engines', 'Implement caching mechanisms for frequent queries', 'Address data freshness vs. latency tradeoffs', 'Secure data flow with encryption and access controls', 'Reference Microsoft Research on real-time integration challenges'], 'target_words': 400, 'tags': ['data_engineering', 'security'], 'requires_research': True, 'requires_citations': True, 'requires_code': False}, 'topic': 'Building a Complex, Production-Ready RAG System', 'mode': 'hybrid', 'plan': {'blog_title': 'Building a Complex, Production-Ready RAG System: A Hybrid Approach', 'audience': 'Developers and data scientists with intermediate to advanced AI/ML experience', 'tone': 'Technical yet approachable, balancing conceptual clarity with practical implementation details', 'blog_kind': 'tutorial', 'constraints': ['Focus on hybrid architecture combining retrieval and generation', 'Prioritize production-ready considerations', 'Integrate evidence from provided sources'], 'tasks': [{'id': 1, 'title': 'Understanding RAG Fundamentals', 'goal': 'Define Retrieval-Augmented Generation (RAG) and its core components', 'bullets': ['Explain how RAG combines retrieval systems with generative models', \"Highlight Azure OpenAI Realtime API's role in enabling hybrid workflows\", 'Discuss the tradeoffs between pure retrieval vs. pure generation approaches', \"Reference Microsoft Learn's RAG documentation for technical context\"], 'target_words': 250, 'tags': ['concept', 'architecture'], 'requires_research': True, 'requires_citations': True, 'requires_code': False}, {'id': 2, 'title': 'Designing the Hybrid Architecture', 'goal': 'Outline the key components of a production-ready RAG system', 'bullets': ['Break down the retrieval pipeline (indexing, querying, ranking)', 'Detail the generation pipeline (prompt engineering, model integration)', 'Explain real-time data injection via Azure Realtime API', 'Address latency and throughput considerations', 'Tag: system_design'], 'target_words': 350, 'tags': ['system_design', 'architecture'], 'requires_research': True, 'requires_citations': True, 'requires_code': False}, {'id': 3, 'title': 'Implementing Data Retrieval Strategies', 'goal': 'Explore methods for efficient and secure data retrieval', 'bullets': ['Compare vector databases vs. traditional search engines', 'Implement caching mechanisms for frequent queries', 'Address data freshness vs. latency tradeoffs', 'Secure data flow with encryption and access controls', 'Reference Microsoft Research on real-time integration challenges'], 'target_words': 400, 'tags': ['data_engineering', 'security'], 'requires_research': True, 'requires_citations': True, 'requires_code': False}, {'id': 4, 'title': 'Integrating with Generative Models', 'goal': 'Demonstrate how to connect retrieval systems with LLMs', 'bullets': ['Show prompt templating for context-aware generation', 'Implement response filtering and validation', 'Handle hallucination detection mechanisms', 'Optimize for real-time inference latency', 'Tag: implementation'], 'target_words': 300, 'tags': ['implementation', 'llm_integration'], 'requires_research': False, 'requires_citations': False, 'requires_code': True}, {'id': 5, 'title': 'Production-Ready Considerations', 'goal': 'Address scalability and monitoring requirements', 'bullets': ['Design for horizontal scaling with microservices', 'Implement observability with metrics and logging', 'Create rollback strategies for model updates', 'Plan for disaster recovery and data backups', 'Tag: operations'], 'target_words': 350, 'tags': ['operations', 'scaling'], 'requires_research': False, 'requires_citations': False, 'requires_code': False}]}, 'evidence': [{'title': 'Retrieval-Augmented Generation (RAG) with Azure OpenAI Realtime API', 'url': 'https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/rag', 'published_at': '2023-10-10', 'snippet': 'Retrieval-Augmented Generation (RAG) combines retrieval of external data with generative models to provide more accurate and up-to-date answers. The Azure OpenAI Realtime API enables real-time data integration through external retrieval components.', 'source': 'Microsoft Learn'}, {'title': 'Realtime API for Voice and Chat Applications', 'url': 'https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/realtime-api', 'published_at': '2023-11-01', 'snippet': 'The Realtime API allows for dynamic interaction with models, supporting features like voice and chat. It requires external systems to handle data retrieval and injection into conversations.', 'source': 'Microsoft Learn'}, {'title': 'Challenges in Real-Time Data Integration', 'url': 'https://www.microsoft.com/en-us/research/publication/real-time-data-integration-challenges/', 'published_at': '2023-09-15', 'snippet': 'Real-time data integration faces challenges including latency, data freshness, and security. External retrieval systems must ensure efficient and secure data flow to generative models.', 'source': 'Microsoft Research'}]}), Send(node='worker', arg={'task': {'id': 4, 'title': 'Integrating with Generative Models', 'goal': 'Demonstrate how to connect retrieval systems with LLMs', 'bullets': ['Show prompt templating for context-aware generation', 'Implement response filtering and validation', 'Handle hallucination detection mechanisms', 'Optimize for real-time inference latency', 'Tag: implementation'], 'target_words': 300, 'tags': ['implementation', 'llm_integration'], 'requires_research': False, 'requires_citations': False, 'requires_code': True}, 'topic': 'Building a Complex, Production-Ready RAG System', 'mode': 'hybrid', 'plan': {'blog_title': 'Building a Complex, Production-Ready RAG System: A Hybrid Approach', 'audience': 'Developers and data scientists with intermediate to advanced AI/ML experience', 'tone': 'Technical yet approachable, balancing conceptual clarity with practical implementation details', 'blog_kind': 'tutorial', 'constraints': ['Focus on hybrid architecture combining retrieval and generation', 'Prioritize production-ready considerations', 'Integrate evidence from provided sources'], 'tasks': [{'id': 1, 'title': 'Understanding RAG Fundamentals', 'goal': 'Define Retrieval-Augmented Generation (RAG) and its core components', 'bullets': ['Explain how RAG combines retrieval systems with generative models', \"Highlight Azure OpenAI Realtime API's role in enabling hybrid workflows\", 'Discuss the tradeoffs between pure retrieval vs. pure generation approaches', \"Reference Microsoft Learn's RAG documentation for technical context\"], 'target_words': 250, 'tags': ['concept', 'architecture'], 'requires_research': True, 'requires_citations': True, 'requires_code': False}, {'id': 2, 'title': 'Designing the Hybrid Architecture', 'goal': 'Outline the key components of a production-ready RAG system', 'bullets': ['Break down the retrieval pipeline (indexing, querying, ranking)', 'Detail the generation pipeline (prompt engineering, model integration)', 'Explain real-time data injection via Azure Realtime API', 'Address latency and throughput considerations', 'Tag: system_design'], 'target_words': 350, 'tags': ['system_design', 'architecture'], 'requires_research': True, 'requires_citations': True, 'requires_code': False}, {'id': 3, 'title': 'Implementing Data Retrieval Strategies', 'goal': 'Explore methods for efficient and secure data retrieval', 'bullets': ['Compare vector databases vs. traditional search engines', 'Implement caching mechanisms for frequent queries', 'Address data freshness vs. latency tradeoffs', 'Secure data flow with encryption and access controls', 'Reference Microsoft Research on real-time integration challenges'], 'target_words': 400, 'tags': ['data_engineering', 'security'], 'requires_research': True, 'requires_citations': True, 'requires_code': False}, {'id': 4, 'title': 'Integrating with Generative Models', 'goal': 'Demonstrate how to connect retrieval systems with LLMs', 'bullets': ['Show prompt templating for context-aware generation', 'Implement response filtering and validation', 'Handle hallucination detection mechanisms', 'Optimize for real-time inference latency', 'Tag: implementation'], 'target_words': 300, 'tags': ['implementation', 'llm_integration'], 'requires_research': False, 'requires_citations': False, 'requires_code': True}, {'id': 5, 'title': 'Production-Ready Considerations', 'goal': 'Address scalability and monitoring requirements', 'bullets': ['Design for horizontal scaling with microservices', 'Implement observability with metrics and logging', 'Create rollback strategies for model updates', 'Plan for disaster recovery and data backups', 'Tag: operations'], 'target_words': 350, 'tags': ['operations', 'scaling'], 'requires_research': False, 'requires_citations': False, 'requires_code': False}]}, 'evidence': [{'title': 'Retrieval-Augmented Generation (RAG) with Azure OpenAI Realtime API', 'url': 'https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/rag', 'published_at': '2023-10-10', 'snippet': 'Retrieval-Augmented Generation (RAG) combines retrieval of external data with generative models to provide more accurate and up-to-date answers. The Azure OpenAI Realtime API enables real-time data integration through external retrieval components.', 'source': 'Microsoft Learn'}, {'title': 'Realtime API for Voice and Chat Applications', 'url': 'https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/realtime-api', 'published_at': '2023-11-01', 'snippet': 'The Realtime API allows for dynamic interaction with models, supporting features like voice and chat. It requires external systems to handle data retrieval and injection into conversations.', 'source': 'Microsoft Learn'}, {'title': 'Challenges in Real-Time Data Integration', 'url': 'https://www.microsoft.com/en-us/research/publication/real-time-data-integration-challenges/', 'published_at': '2023-09-15', 'snippet': 'Real-time data integration faces challenges including latency, data freshness, and security. External retrieval systems must ensure efficient and secure data flow to generative models.', 'source': 'Microsoft Research'}]}), Send(node='worker', arg={'task': {'id': 5, 'title': 'Production-Ready Considerations', 'goal': 'Address scalability and monitoring requirements', 'bullets': ['Design for horizontal scaling with microservices', 'Implement observability with metrics and logging', 'Create rollback strategies for model updates', 'Plan for disaster recovery and data backups', 'Tag: operations'], 'target_words': 350, 'tags': ['operations', 'scaling'], 'requires_research': False, 'requires_citations': False, 'requires_code': False}, 'topic': 'Building a Complex, Production-Ready RAG System', 'mode': 'hybrid', 'plan': {'blog_title': 'Building a Complex, Production-Ready RAG System: A Hybrid Approach', 'audience': 'Developers and data scientists with intermediate to advanced AI/ML experience', 'tone': 'Technical yet approachable, balancing conceptual clarity with practical implementation details', 'blog_kind': 'tutorial', 'constraints': ['Focus on hybrid architecture combining retrieval and generation', 'Prioritize production-ready considerations', 'Integrate evidence from provided sources'], 'tasks': [{'id': 1, 'title': 'Understanding RAG Fundamentals', 'goal': 'Define Retrieval-Augmented Generation (RAG) and its core components', 'bullets': ['Explain how RAG combines retrieval systems with generative models', \"Highlight Azure OpenAI Realtime API's role in enabling hybrid workflows\", 'Discuss the tradeoffs between pure retrieval vs. pure generation approaches', \"Reference Microsoft Learn's RAG documentation for technical context\"], 'target_words': 250, 'tags': ['concept', 'architecture'], 'requires_research': True, 'requires_citations': True, 'requires_code': False}, {'id': 2, 'title': 'Designing the Hybrid Architecture', 'goal': 'Outline the key components of a production-ready RAG system', 'bullets': ['Break down the retrieval pipeline (indexing, querying, ranking)', 'Detail the generation pipeline (prompt engineering, model integration)', 'Explain real-time data injection via Azure Realtime API', 'Address latency and throughput considerations', 'Tag: system_design'], 'target_words': 350, 'tags': ['system_design', 'architecture'], 'requires_research': True, 'requires_citations': True, 'requires_code': False}, {'id': 3, 'title': 'Implementing Data Retrieval Strategies', 'goal': 'Explore methods for efficient and secure data retrieval', 'bullets': ['Compare vector databases vs. traditional search engines', 'Implement caching mechanisms for frequent queries', 'Address data freshness vs. latency tradeoffs', 'Secure data flow with encryption and access controls', 'Reference Microsoft Research on real-time integration challenges'], 'target_words': 400, 'tags': ['data_engineering', 'security'], 'requires_research': True, 'requires_citations': True, 'requires_code': False}, {'id': 4, 'title': 'Integrating with Generative Models', 'goal': 'Demonstrate how to connect retrieval systems with LLMs', 'bullets': ['Show prompt templating for context-aware generation', 'Implement response filtering and validation', 'Handle hallucination detection mechanisms', 'Optimize for real-time inference latency', 'Tag: implementation'], 'target_words': 300, 'tags': ['implementation', 'llm_integration'], 'requires_research': False, 'requires_citations': False, 'requires_code': True}, {'id': 5, 'title': 'Production-Ready Considerations', 'goal': 'Address scalability and monitoring requirements', 'bullets': ['Design for horizontal scaling with microservices', 'Implement observability with metrics and logging', 'Create rollback strategies for model updates', 'Plan for disaster recovery and data backups', 'Tag: operations'], 'target_words': 350, 'tags': ['operations', 'scaling'], 'requires_research': False, 'requires_citations': False, 'requires_code': False}]}, 'evidence': [{'title': 'Retrieval-Augmented Generation (RAG) with Azure OpenAI Realtime API', 'url': 'https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/rag', 'published_at': '2023-10-10', 'snippet': 'Retrieval-Augmented Generation (RAG) combines retrieval of external data with generative models to provide more accurate and up-to-date answers. The Azure OpenAI Realtime API enables real-time data integration through external retrieval components.', 'source': 'Microsoft Learn'}, {'title': 'Realtime API for Voice and Chat Applications', 'url': 'https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/realtime-api', 'published_at': '2023-11-01', 'snippet': 'The Realtime API allows for dynamic interaction with models, supporting features like voice and chat. It requires external systems to handle data retrieval and injection into conversations.', 'source': 'Microsoft Learn'}, {'title': 'Challenges in Real-Time Data Integration', 'url': 'https://www.microsoft.com/en-us/research/publication/real-time-data-integration-challenges/', 'published_at': '2023-09-15', 'snippet': 'Real-time data integration faces challenges including latency, data freshness, and security. External retrieval systems must ensure efficient and secure data flow to generative models.', 'source': 'Microsoft Research'}]})]\n"
     ]
    }
   ],
   "source": [
    "worker_targets = fanout(state)\n",
    "print(\"Fanout returned:\", worker_targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3044522c",
   "metadata": {},
   "source": [
    "**worker node**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "06c539ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RUN worker 1 ===\n",
      "\n",
      "=== RUN worker 2 ===\n",
      "\n",
      "=== RUN worker 3 ===\n",
      "\n",
      "=== RUN worker 4 ===\n",
      "\n",
      "=== RUN worker 5 ===\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "for i, send in enumerate(worker_targets):\n",
    "    print(f\"\\n=== RUN worker {i+1} ===\")\n",
    "\n",
    "    worker_state = deepcopy(state)\n",
    "    worker_state.update(send.arg)   # inject task, etc\n",
    "\n",
    "    patch = worker_node(worker_state)\n",
    "\n",
    "    # mimic Annotated[..., operator.add]\n",
    "    if \"sections\" in patch:\n",
    "        state[\"sections\"] = state.get(\"sections\", []) + patch[\"sections\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7f8582",
   "metadata": {},
   "source": [
    "**merge_content**\n",
    "* merge the tasks outputs to state[sections]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "625f9bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RUN merge_content ===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'topic': 'Building a Complex, Production-Ready RAG System',\n",
       " 'mode': 'hybrid',\n",
       " 'needs_research': True,\n",
       " 'queries': ['Best practices for implementing a production-ready Retrieval-Augmented Generation system with current LLM models',\n",
       "  'Tools and frameworks for building scalable RAG pipelines, such as LangChain and Hugging Face',\n",
       "  'Performance optimization techniques for RAG systems, including vector database selection and query efficiency',\n",
       "  'Deployment strategies for RAG models in cloud environments like AWS or Google Cloud',\n",
       "  'Evaluation metrics and monitoring tools for production RAG systems to ensure accuracy and reliability',\n",
       "  'Common pitfalls and solutions when building RAG systems, based on recent case studies',\n",
       "  'Integration of RAG with external APIs and data sources for real-time information retrieval'],\n",
       " 'evidence': [EvidenceItem(title='Retrieval-Augmented Generation (RAG) with Azure OpenAI Realtime API', url='https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/rag', published_at='2023-10-10', snippet='Retrieval-Augmented Generation (RAG) combines retrieval of external data with generative models to provide more accurate and up-to-date answers. The Azure OpenAI Realtime API enables real-time data integration through external retrieval components.', source='Microsoft Learn'),\n",
       "  EvidenceItem(title='Realtime API for Voice and Chat Applications', url='https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/realtime-api', published_at='2023-11-01', snippet='The Realtime API allows for dynamic interaction with models, supporting features like voice and chat. It requires external systems to handle data retrieval and injection into conversations.', source='Microsoft Learn'),\n",
       "  EvidenceItem(title='Challenges in Real-Time Data Integration', url='https://www.microsoft.com/en-us/research/publication/real-time-data-integration-challenges/', published_at='2023-09-15', snippet='Real-time data integration faces challenges including latency, data freshness, and security. External retrieval systems must ensure efficient and secure data flow to generative models.', source='Microsoft Research')],\n",
       " 'plan': Plan(blog_title='Building a Complex, Production-Ready RAG System: A Hybrid Approach', audience='Developers and data scientists with intermediate to advanced AI/ML experience', tone='Technical yet approachable, balancing conceptual clarity with practical implementation details', blog_kind='tutorial', constraints=['Focus on hybrid architecture combining retrieval and generation', 'Prioritize production-ready considerations', 'Integrate evidence from provided sources'], tasks=[Task(id=1, title='Understanding RAG Fundamentals', goal='Define Retrieval-Augmented Generation (RAG) and its core components', bullets=['Explain how RAG combines retrieval systems with generative models', \"Highlight Azure OpenAI Realtime API's role in enabling hybrid workflows\", 'Discuss the tradeoffs between pure retrieval vs. pure generation approaches', \"Reference Microsoft Learn's RAG documentation for technical context\"], target_words=250, tags=['concept', 'architecture'], requires_research=True, requires_citations=True, requires_code=False), Task(id=2, title='Designing the Hybrid Architecture', goal='Outline the key components of a production-ready RAG system', bullets=['Break down the retrieval pipeline (indexing, querying, ranking)', 'Detail the generation pipeline (prompt engineering, model integration)', 'Explain real-time data injection via Azure Realtime API', 'Address latency and throughput considerations', 'Tag: system_design'], target_words=350, tags=['system_design', 'architecture'], requires_research=True, requires_citations=True, requires_code=False), Task(id=3, title='Implementing Data Retrieval Strategies', goal='Explore methods for efficient and secure data retrieval', bullets=['Compare vector databases vs. traditional search engines', 'Implement caching mechanisms for frequent queries', 'Address data freshness vs. latency tradeoffs', 'Secure data flow with encryption and access controls', 'Reference Microsoft Research on real-time integration challenges'], target_words=400, tags=['data_engineering', 'security'], requires_research=True, requires_citations=True, requires_code=False), Task(id=4, title='Integrating with Generative Models', goal='Demonstrate how to connect retrieval systems with LLMs', bullets=['Show prompt templating for context-aware generation', 'Implement response filtering and validation', 'Handle hallucination detection mechanisms', 'Optimize for real-time inference latency', 'Tag: implementation'], target_words=300, tags=['implementation', 'llm_integration'], requires_research=False, requires_citations=False, requires_code=True), Task(id=5, title='Production-Ready Considerations', goal='Address scalability and monitoring requirements', bullets=['Design for horizontal scaling with microservices', 'Implement observability with metrics and logging', 'Create rollback strategies for model updates', 'Plan for disaster recovery and data backups', 'Tag: operations'], target_words=350, tags=['operations', 'scaling'], requires_research=False, requires_citations=False, requires_code=False)]),\n",
       " 'as_of': None,\n",
       " 'recency_days': 7,\n",
       " 'sections': [(1,\n",
       "   '## Understanding RAG Fundamentals\\n\\nRetrieval-Augmented Generation (RAG) integrates retrieval systems with generative models to balance accuracy and flexibility. The architecture combines a retrieval component, which fetches relevant documents from a knowledge base, with a generative model that synthesizes context-aware responses. This hybrid approach mitigates the limitations of pure retrieval (which risks irrelevance) and pure generation (which may lack factual grounding). Azure OpenAI Realtime API plays a pivotal role in enabling hybrid workflows by allowing dynamic data fetching and real-time integration, as outlined in Microsoft Learn’s RAG documentation ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/rag)). \\n\\nThe tradeoffs between approaches depend on use cases: pure retrieval excels in speed and simplicity but sacrifices depth, while pure generation offers adaptability at the cost of potential hallucinations. RAG strikes a middle ground, though challenges like latency and data freshness persist. Microsoft’s research highlights real-time data integration complexities, emphasizing the need for robust pipeline design ([Source](https://www.microsoft.com/en-us/research/publication/real-time-data-integration-challenges/)). For production systems, prioritizing latency optimization and data consistency remains critical. The Realtime API’s capabilities for voice and chat applications further underscore its relevance in hybrid RAG implementations ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/realtime-api)).'),\n",
       "  (2,\n",
       "   '## Designing the Hybrid Architecture\\n\\nA production-ready RAG system hinges on a hybrid architecture that seamlessly integrates retrieval and generation pipelines, ensuring scalability, reliability, and real-time adaptability. The retrieval pipeline begins with **indexing**, where unstructured data is processed into a vector database or document store, enabling efficient querying. Azure OpenAI’s retrieval mechanisms, such as semantic search and hybrid search, allow for nuanced query matching while maintaining performance ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/rag)). **Querying** involves fetching the most relevant documents based on user input, with ranking algorithms prioritizing precision over recall to avoid overwhelming users with irrelevant results. **Ranking** is further optimized using Azure’s real-time feedback loops, which refine results dynamically based on user interactions.\\n\\nThe generation pipeline focuses on **prompt engineering** to structure inputs for the language model, balancing context length and specificity. Integration with Azure OpenAI’s models requires careful handling of token limits and response formatting, ensuring outputs are coherent and aligned with user intent. For real-time data injection, the **Azure Realtime API** enables streaming updates from external sources, such as chat transcripts or voice data, into the knowledge base. This ensures the system remains current without requiring full reindexing, though challenges like data latency and consistency must be mitigated through asynchronous processing and validation checks ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/realtime-api)). \\n\\nLatency and throughput are critical in hybrid systems. Retrieval operations must prioritize low-latency indexing and caching, while generation pipelines should scale horizontally using distributed computing frameworks. Azure’s Realtime API introduces additional latency, necessitating buffer zones and prioritization of critical queries. Throughput is managed by decoupling data ingestion from model inference, allowing the system to handle high-volume workloads without degrading performance. These considerations ensure the architecture remains robust, adaptable, and aligned with production requirements.'),\n",
       "  (3,\n",
       "   '## Implementing Data Retrieval Strategies  \\n\\nEfficient data retrieval is foundational to a production-ready RAG system, requiring careful trade-offs between speed, accuracy, and security. Vector databases like FAISS or Pinecone excel at semantic similarity searches, enabling efficient retrieval of contextually relevant documents, while traditional search engines (e.g., Elasticsearch) rely on keyword-based indexing and are better suited for structured queries. According to Microsoft Research, real-time integration challenges often arise when combining these approaches, necessitating hybrid architectures that balance latency and freshness ([Source](https://www.microsoft.com/en-us/research/publication/real-time-data-integration-challenges/)).  \\n\\nCaching mechanisms are critical for frequent queries, reducing latency and database load. Implementing in-memory caches (e.g., Redis or Memcached) with time-to-live (TTL) settings ensures quick access to common results while avoiding stale data. For dynamic content, cache invalidation strategies—such as event-driven updates or periodic refreshes—can maintain freshness without sacrificing performance.  \\n\\nData freshness and latency must be balanced in production systems. Real-time data integration, as discussed in Azure OpenAI’s Realtime API documentation, introduces complexities like network delays and synchronization overhead. A pragmatic approach might involve hybrid pipelines: using vector databases for static documents and real-time APIs for dynamic data, with fallback mechanisms to handle partial failures ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/realtime-api)).  \\n\\nSecurity requires encryption at rest and in transit (e.g., TLS) and granular access controls via IAM policies. Sensitive data should be anonymized or tokenized before ingestion, and retrieval pipelines must audit access logs to detect anomalies. Microsoft’s Azure OpenAI documentation emphasizes secure data flow patterns, including private endpoints and VPC integration, to mitigate risks in enterprise deployments ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/rag)).'),\n",
       "  (4,\n",
       "   '## Integrating with Generative Models\\n\\nConnecting retrieval systems with generative models requires careful orchestration of context-aware prompts, response validation, and latency optimization. Begin by templating prompts to explicitly bind retrieved context with user queries. For example:  \\n```python\\nprompt = f\"Context: {retrieved_docs}\\\\nQuery: {user_input}\\\\nAnswer:\"\\nresponse = llm(prompt)\\n```  \\nThis ensures the model focuses on relevant information.  \\n\\nNext, implement response filtering to validate outputs against the source documents. Use schema checks or keyword matching to ensure responses align with retrieved data. For example:  \\n```python\\ndef validate_response(response, query):\\n    return \"source keyword\" in response and len(response.split()) > 5\\n```  \\nThis prevents hallucination by enforcing consistency with the context.  \\n\\nFor hallucination detection, integrate a secondary check using semantic similarity between the response and the context. If the similarity score falls below a threshold (e.g., 0.7), flag the response as unreliable.  \\n\\nTo optimize real-time latency, leverage streaming APIs or model serving configurations (e.g., Azure OpenAI’s Realtime API) to minimize inference delays. Prioritize lightweight models and batch requests where feasible.  \\n\\nThese steps ensure robust integration while maintaining performance and accuracy in production environments.'),\n",
       "  (5,\n",
       "   '## Production-Ready Considerations\\n\\nDesigning a scalable RAG system requires deliberate architectural choices to handle growth and ensure reliability. Horizontal scaling with microservices is critical, breaking the system into modular components like retrieval, generation, and routing layers. Each service should operate independently, allowing for targeted scaling based on workload. For example, the retrieval layer may require higher concurrency during peak traffic, while the generation layer might need more memory. Load balancers and auto-scaling groups should be configured to dynamically adjust resources, ensuring consistent performance under varying loads. The Azure OpenAI Realtime API’s architecture provides a reference for handling real-time data streams with distributed infrastructure.\\n\\nObservability is foundational for maintaining system health. Implement metrics for latency, error rates, and throughput across all components, using tools like Prometheus or Azure Monitor. Centralized logging with tools like ELK Stack or Azure Log Analytics enables quick debugging of failures. For production systems, ensure logs include contextual metadata (e.g., request IDs) to trace issues across services. Monitoring should also track model performance, such as the accuracy of retrieved documents or the coherence of generated responses, to detect drift or degradation over time.\\n\\nRollback strategies must be baked into the deployment pipeline. Use versioned model checkpoints and canary deployments to gradually roll out updates, allowing for quick reversion if issues arise. Automated rollback triggers based on metrics (e.g., rising error rates) can prevent cascading failures. Disaster recovery planning should include redundant data storage across regions and periodic backups of both raw data and model weights. For critical systems, consider geo-replication to ensure availability during outages. These practices align with principles outlined in real-time data integration challenges, emphasizing resilience in distributed systems.')],\n",
       " 'merged_md': '# Building a Complex, Production-Ready RAG System: A Hybrid Approach\\n\\n## Understanding RAG Fundamentals\\n\\nRetrieval-Augmented Generation (RAG) integrates retrieval systems with generative models to balance accuracy and flexibility. The architecture combines a retrieval component, which fetches relevant documents from a knowledge base, with a generative model that synthesizes context-aware responses. This hybrid approach mitigates the limitations of pure retrieval (which risks irrelevance) and pure generation (which may lack factual grounding). Azure OpenAI Realtime API plays a pivotal role in enabling hybrid workflows by allowing dynamic data fetching and real-time integration, as outlined in Microsoft Learn’s RAG documentation ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/rag)). \\n\\nThe tradeoffs between approaches depend on use cases: pure retrieval excels in speed and simplicity but sacrifices depth, while pure generation offers adaptability at the cost of potential hallucinations. RAG strikes a middle ground, though challenges like latency and data freshness persist. Microsoft’s research highlights real-time data integration complexities, emphasizing the need for robust pipeline design ([Source](https://www.microsoft.com/en-us/research/publication/real-time-data-integration-challenges/)). For production systems, prioritizing latency optimization and data consistency remains critical. The Realtime API’s capabilities for voice and chat applications further underscore its relevance in hybrid RAG implementations ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/realtime-api)).\\n\\n## Designing the Hybrid Architecture\\n\\nA production-ready RAG system hinges on a hybrid architecture that seamlessly integrates retrieval and generation pipelines, ensuring scalability, reliability, and real-time adaptability. The retrieval pipeline begins with **indexing**, where unstructured data is processed into a vector database or document store, enabling efficient querying. Azure OpenAI’s retrieval mechanisms, such as semantic search and hybrid search, allow for nuanced query matching while maintaining performance ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/rag)). **Querying** involves fetching the most relevant documents based on user input, with ranking algorithms prioritizing precision over recall to avoid overwhelming users with irrelevant results. **Ranking** is further optimized using Azure’s real-time feedback loops, which refine results dynamically based on user interactions.\\n\\nThe generation pipeline focuses on **prompt engineering** to structure inputs for the language model, balancing context length and specificity. Integration with Azure OpenAI’s models requires careful handling of token limits and response formatting, ensuring outputs are coherent and aligned with user intent. For real-time data injection, the **Azure Realtime API** enables streaming updates from external sources, such as chat transcripts or voice data, into the knowledge base. This ensures the system remains current without requiring full reindexing, though challenges like data latency and consistency must be mitigated through asynchronous processing and validation checks ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/realtime-api)). \\n\\nLatency and throughput are critical in hybrid systems. Retrieval operations must prioritize low-latency indexing and caching, while generation pipelines should scale horizontally using distributed computing frameworks. Azure’s Realtime API introduces additional latency, necessitating buffer zones and prioritization of critical queries. Throughput is managed by decoupling data ingestion from model inference, allowing the system to handle high-volume workloads without degrading performance. These considerations ensure the architecture remains robust, adaptable, and aligned with production requirements.\\n\\n## Implementing Data Retrieval Strategies  \\n\\nEfficient data retrieval is foundational to a production-ready RAG system, requiring careful trade-offs between speed, accuracy, and security. Vector databases like FAISS or Pinecone excel at semantic similarity searches, enabling efficient retrieval of contextually relevant documents, while traditional search engines (e.g., Elasticsearch) rely on keyword-based indexing and are better suited for structured queries. According to Microsoft Research, real-time integration challenges often arise when combining these approaches, necessitating hybrid architectures that balance latency and freshness ([Source](https://www.microsoft.com/en-us/research/publication/real-time-data-integration-challenges/)).  \\n\\nCaching mechanisms are critical for frequent queries, reducing latency and database load. Implementing in-memory caches (e.g., Redis or Memcached) with time-to-live (TTL) settings ensures quick access to common results while avoiding stale data. For dynamic content, cache invalidation strategies—such as event-driven updates or periodic refreshes—can maintain freshness without sacrificing performance.  \\n\\nData freshness and latency must be balanced in production systems. Real-time data integration, as discussed in Azure OpenAI’s Realtime API documentation, introduces complexities like network delays and synchronization overhead. A pragmatic approach might involve hybrid pipelines: using vector databases for static documents and real-time APIs for dynamic data, with fallback mechanisms to handle partial failures ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/realtime-api)).  \\n\\nSecurity requires encryption at rest and in transit (e.g., TLS) and granular access controls via IAM policies. Sensitive data should be anonymized or tokenized before ingestion, and retrieval pipelines must audit access logs to detect anomalies. Microsoft’s Azure OpenAI documentation emphasizes secure data flow patterns, including private endpoints and VPC integration, to mitigate risks in enterprise deployments ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/rag)).\\n\\n## Integrating with Generative Models\\n\\nConnecting retrieval systems with generative models requires careful orchestration of context-aware prompts, response validation, and latency optimization. Begin by templating prompts to explicitly bind retrieved context with user queries. For example:  \\n```python\\nprompt = f\"Context: {retrieved_docs}\\\\nQuery: {user_input}\\\\nAnswer:\"\\nresponse = llm(prompt)\\n```  \\nThis ensures the model focuses on relevant information.  \\n\\nNext, implement response filtering to validate outputs against the source documents. Use schema checks or keyword matching to ensure responses align with retrieved data. For example:  \\n```python\\ndef validate_response(response, query):\\n    return \"source keyword\" in response and len(response.split()) > 5\\n```  \\nThis prevents hallucination by enforcing consistency with the context.  \\n\\nFor hallucination detection, integrate a secondary check using semantic similarity between the response and the context. If the similarity score falls below a threshold (e.g., 0.7), flag the response as unreliable.  \\n\\nTo optimize real-time latency, leverage streaming APIs or model serving configurations (e.g., Azure OpenAI’s Realtime API) to minimize inference delays. Prioritize lightweight models and batch requests where feasible.  \\n\\nThese steps ensure robust integration while maintaining performance and accuracy in production environments.\\n\\n## Production-Ready Considerations\\n\\nDesigning a scalable RAG system requires deliberate architectural choices to handle growth and ensure reliability. Horizontal scaling with microservices is critical, breaking the system into modular components like retrieval, generation, and routing layers. Each service should operate independently, allowing for targeted scaling based on workload. For example, the retrieval layer may require higher concurrency during peak traffic, while the generation layer might need more memory. Load balancers and auto-scaling groups should be configured to dynamically adjust resources, ensuring consistent performance under varying loads. The Azure OpenAI Realtime API’s architecture provides a reference for handling real-time data streams with distributed infrastructure.\\n\\nObservability is foundational for maintaining system health. Implement metrics for latency, error rates, and throughput across all components, using tools like Prometheus or Azure Monitor. Centralized logging with tools like ELK Stack or Azure Log Analytics enables quick debugging of failures. For production systems, ensure logs include contextual metadata (e.g., request IDs) to trace issues across services. Monitoring should also track model performance, such as the accuracy of retrieved documents or the coherence of generated responses, to detect drift or degradation over time.\\n\\nRollback strategies must be baked into the deployment pipeline. Use versioned model checkpoints and canary deployments to gradually roll out updates, allowing for quick reversion if issues arise. Automated rollback triggers based on metrics (e.g., rising error rates) can prevent cascading failures. Disaster recovery planning should include redundant data storage across regions and periodic backups of both raw data and model weights. For critical systems, consider geo-replication to ensure availability during outages. These practices align with principles outlined in real-time data integration challenges, emphasizing resilience in distributed systems.\\n',\n",
       " 'md_with_placeholders': '',\n",
       " 'image_specs': [],\n",
       " 'final': ''}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n=== RUN merge_content ===\")\n",
    "\n",
    "patch = merge_content(state)\n",
    "\n",
    "for k, v in patch.items():\n",
    "    state[k] = v\n",
    "\n",
    "state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c684affb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2400"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(state['merged_md']) // 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d6f583",
   "metadata": {},
   "source": [
    "**Decide_images Node**\n",
    "\n",
    "* decide where to put images\n",
    "    [[image1]]\n",
    "* also return image_spec (about the image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e392b678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RUN decide_images ===\n",
      "Input markdown size: 9603 chars (~2400 tokens)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'topic': 'Building a Complex, Production-Ready RAG System',\n",
       " 'mode': 'hybrid',\n",
       " 'needs_research': True,\n",
       " 'queries': ['Best practices for implementing a production-ready Retrieval-Augmented Generation system with current LLM models',\n",
       "  'Tools and frameworks for building scalable RAG pipelines, such as LangChain and Hugging Face',\n",
       "  'Performance optimization techniques for RAG systems, including vector database selection and query efficiency',\n",
       "  'Deployment strategies for RAG models in cloud environments like AWS or Google Cloud',\n",
       "  'Evaluation metrics and monitoring tools for production RAG systems to ensure accuracy and reliability',\n",
       "  'Common pitfalls and solutions when building RAG systems, based on recent case studies',\n",
       "  'Integration of RAG with external APIs and data sources for real-time information retrieval'],\n",
       " 'evidence': [EvidenceItem(title='Retrieval-Augmented Generation (RAG) with Azure OpenAI Realtime API', url='https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/rag', published_at='2023-10-10', snippet='Retrieval-Augmented Generation (RAG) combines retrieval of external data with generative models to provide more accurate and up-to-date answers. The Azure OpenAI Realtime API enables real-time data integration through external retrieval components.', source='Microsoft Learn'),\n",
       "  EvidenceItem(title='Realtime API for Voice and Chat Applications', url='https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/realtime-api', published_at='2023-11-01', snippet='The Realtime API allows for dynamic interaction with models, supporting features like voice and chat. It requires external systems to handle data retrieval and injection into conversations.', source='Microsoft Learn'),\n",
       "  EvidenceItem(title='Challenges in Real-Time Data Integration', url='https://www.microsoft.com/en-us/research/publication/real-time-data-integration-challenges/', published_at='2023-09-15', snippet='Real-time data integration faces challenges including latency, data freshness, and security. External retrieval systems must ensure efficient and secure data flow to generative models.', source='Microsoft Research')],\n",
       " 'plan': Plan(blog_title='Building a Complex, Production-Ready RAG System: A Hybrid Approach', audience='Developers and data scientists with intermediate to advanced AI/ML experience', tone='Technical yet approachable, balancing conceptual clarity with practical implementation details', blog_kind='tutorial', constraints=['Focus on hybrid architecture combining retrieval and generation', 'Prioritize production-ready considerations', 'Integrate evidence from provided sources'], tasks=[Task(id=1, title='Understanding RAG Fundamentals', goal='Define Retrieval-Augmented Generation (RAG) and its core components', bullets=['Explain how RAG combines retrieval systems with generative models', \"Highlight Azure OpenAI Realtime API's role in enabling hybrid workflows\", 'Discuss the tradeoffs between pure retrieval vs. pure generation approaches', \"Reference Microsoft Learn's RAG documentation for technical context\"], target_words=250, tags=['concept', 'architecture'], requires_research=True, requires_citations=True, requires_code=False), Task(id=2, title='Designing the Hybrid Architecture', goal='Outline the key components of a production-ready RAG system', bullets=['Break down the retrieval pipeline (indexing, querying, ranking)', 'Detail the generation pipeline (prompt engineering, model integration)', 'Explain real-time data injection via Azure Realtime API', 'Address latency and throughput considerations', 'Tag: system_design'], target_words=350, tags=['system_design', 'architecture'], requires_research=True, requires_citations=True, requires_code=False), Task(id=3, title='Implementing Data Retrieval Strategies', goal='Explore methods for efficient and secure data retrieval', bullets=['Compare vector databases vs. traditional search engines', 'Implement caching mechanisms for frequent queries', 'Address data freshness vs. latency tradeoffs', 'Secure data flow with encryption and access controls', 'Reference Microsoft Research on real-time integration challenges'], target_words=400, tags=['data_engineering', 'security'], requires_research=True, requires_citations=True, requires_code=False), Task(id=4, title='Integrating with Generative Models', goal='Demonstrate how to connect retrieval systems with LLMs', bullets=['Show prompt templating for context-aware generation', 'Implement response filtering and validation', 'Handle hallucination detection mechanisms', 'Optimize for real-time inference latency', 'Tag: implementation'], target_words=300, tags=['implementation', 'llm_integration'], requires_research=False, requires_citations=False, requires_code=True), Task(id=5, title='Production-Ready Considerations', goal='Address scalability and monitoring requirements', bullets=['Design for horizontal scaling with microservices', 'Implement observability with metrics and logging', 'Create rollback strategies for model updates', 'Plan for disaster recovery and data backups', 'Tag: operations'], target_words=350, tags=['operations', 'scaling'], requires_research=False, requires_citations=False, requires_code=False)]),\n",
       " 'as_of': None,\n",
       " 'recency_days': 7,\n",
       " 'sections': [(1,\n",
       "   '## Understanding RAG Fundamentals\\n\\nRetrieval-Augmented Generation (RAG) integrates retrieval systems with generative models to balance accuracy and flexibility. The architecture combines a retrieval component, which fetches relevant documents from a knowledge base, with a generative model that synthesizes context-aware responses. This hybrid approach mitigates the limitations of pure retrieval (which risks irrelevance) and pure generation (which may lack factual grounding). Azure OpenAI Realtime API plays a pivotal role in enabling hybrid workflows by allowing dynamic data fetching and real-time integration, as outlined in Microsoft Learn’s RAG documentation ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/rag)). \\n\\nThe tradeoffs between approaches depend on use cases: pure retrieval excels in speed and simplicity but sacrifices depth, while pure generation offers adaptability at the cost of potential hallucinations. RAG strikes a middle ground, though challenges like latency and data freshness persist. Microsoft’s research highlights real-time data integration complexities, emphasizing the need for robust pipeline design ([Source](https://www.microsoft.com/en-us/research/publication/real-time-data-integration-challenges/)). For production systems, prioritizing latency optimization and data consistency remains critical. The Realtime API’s capabilities for voice and chat applications further underscore its relevance in hybrid RAG implementations ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/realtime-api)).'),\n",
       "  (2,\n",
       "   '## Designing the Hybrid Architecture\\n\\nA production-ready RAG system hinges on a hybrid architecture that seamlessly integrates retrieval and generation pipelines, ensuring scalability, reliability, and real-time adaptability. The retrieval pipeline begins with **indexing**, where unstructured data is processed into a vector database or document store, enabling efficient querying. Azure OpenAI’s retrieval mechanisms, such as semantic search and hybrid search, allow for nuanced query matching while maintaining performance ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/rag)). **Querying** involves fetching the most relevant documents based on user input, with ranking algorithms prioritizing precision over recall to avoid overwhelming users with irrelevant results. **Ranking** is further optimized using Azure’s real-time feedback loops, which refine results dynamically based on user interactions.\\n\\nThe generation pipeline focuses on **prompt engineering** to structure inputs for the language model, balancing context length and specificity. Integration with Azure OpenAI’s models requires careful handling of token limits and response formatting, ensuring outputs are coherent and aligned with user intent. For real-time data injection, the **Azure Realtime API** enables streaming updates from external sources, such as chat transcripts or voice data, into the knowledge base. This ensures the system remains current without requiring full reindexing, though challenges like data latency and consistency must be mitigated through asynchronous processing and validation checks ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/realtime-api)). \\n\\nLatency and throughput are critical in hybrid systems. Retrieval operations must prioritize low-latency indexing and caching, while generation pipelines should scale horizontally using distributed computing frameworks. Azure’s Realtime API introduces additional latency, necessitating buffer zones and prioritization of critical queries. Throughput is managed by decoupling data ingestion from model inference, allowing the system to handle high-volume workloads without degrading performance. These considerations ensure the architecture remains robust, adaptable, and aligned with production requirements.'),\n",
       "  (3,\n",
       "   '## Implementing Data Retrieval Strategies  \\n\\nEfficient data retrieval is foundational to a production-ready RAG system, requiring careful trade-offs between speed, accuracy, and security. Vector databases like FAISS or Pinecone excel at semantic similarity searches, enabling efficient retrieval of contextually relevant documents, while traditional search engines (e.g., Elasticsearch) rely on keyword-based indexing and are better suited for structured queries. According to Microsoft Research, real-time integration challenges often arise when combining these approaches, necessitating hybrid architectures that balance latency and freshness ([Source](https://www.microsoft.com/en-us/research/publication/real-time-data-integration-challenges/)).  \\n\\nCaching mechanisms are critical for frequent queries, reducing latency and database load. Implementing in-memory caches (e.g., Redis or Memcached) with time-to-live (TTL) settings ensures quick access to common results while avoiding stale data. For dynamic content, cache invalidation strategies—such as event-driven updates or periodic refreshes—can maintain freshness without sacrificing performance.  \\n\\nData freshness and latency must be balanced in production systems. Real-time data integration, as discussed in Azure OpenAI’s Realtime API documentation, introduces complexities like network delays and synchronization overhead. A pragmatic approach might involve hybrid pipelines: using vector databases for static documents and real-time APIs for dynamic data, with fallback mechanisms to handle partial failures ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/realtime-api)).  \\n\\nSecurity requires encryption at rest and in transit (e.g., TLS) and granular access controls via IAM policies. Sensitive data should be anonymized or tokenized before ingestion, and retrieval pipelines must audit access logs to detect anomalies. Microsoft’s Azure OpenAI documentation emphasizes secure data flow patterns, including private endpoints and VPC integration, to mitigate risks in enterprise deployments ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/rag)).'),\n",
       "  (4,\n",
       "   '## Integrating with Generative Models\\n\\nConnecting retrieval systems with generative models requires careful orchestration of context-aware prompts, response validation, and latency optimization. Begin by templating prompts to explicitly bind retrieved context with user queries. For example:  \\n```python\\nprompt = f\"Context: {retrieved_docs}\\\\nQuery: {user_input}\\\\nAnswer:\"\\nresponse = llm(prompt)\\n```  \\nThis ensures the model focuses on relevant information.  \\n\\nNext, implement response filtering to validate outputs against the source documents. Use schema checks or keyword matching to ensure responses align with retrieved data. For example:  \\n```python\\ndef validate_response(response, query):\\n    return \"source keyword\" in response and len(response.split()) > 5\\n```  \\nThis prevents hallucination by enforcing consistency with the context.  \\n\\nFor hallucination detection, integrate a secondary check using semantic similarity between the response and the context. If the similarity score falls below a threshold (e.g., 0.7), flag the response as unreliable.  \\n\\nTo optimize real-time latency, leverage streaming APIs or model serving configurations (e.g., Azure OpenAI’s Realtime API) to minimize inference delays. Prioritize lightweight models and batch requests where feasible.  \\n\\nThese steps ensure robust integration while maintaining performance and accuracy in production environments.'),\n",
       "  (5,\n",
       "   '## Production-Ready Considerations\\n\\nDesigning a scalable RAG system requires deliberate architectural choices to handle growth and ensure reliability. Horizontal scaling with microservices is critical, breaking the system into modular components like retrieval, generation, and routing layers. Each service should operate independently, allowing for targeted scaling based on workload. For example, the retrieval layer may require higher concurrency during peak traffic, while the generation layer might need more memory. Load balancers and auto-scaling groups should be configured to dynamically adjust resources, ensuring consistent performance under varying loads. The Azure OpenAI Realtime API’s architecture provides a reference for handling real-time data streams with distributed infrastructure.\\n\\nObservability is foundational for maintaining system health. Implement metrics for latency, error rates, and throughput across all components, using tools like Prometheus or Azure Monitor. Centralized logging with tools like ELK Stack or Azure Log Analytics enables quick debugging of failures. For production systems, ensure logs include contextual metadata (e.g., request IDs) to trace issues across services. Monitoring should also track model performance, such as the accuracy of retrieved documents or the coherence of generated responses, to detect drift or degradation over time.\\n\\nRollback strategies must be baked into the deployment pipeline. Use versioned model checkpoints and canary deployments to gradually roll out updates, allowing for quick reversion if issues arise. Automated rollback triggers based on metrics (e.g., rising error rates) can prevent cascading failures. Disaster recovery planning should include redundant data storage across regions and periodic backups of both raw data and model weights. For critical systems, consider geo-replication to ensure availability during outages. These practices align with principles outlined in real-time data integration challenges, emphasizing resilience in distributed systems.')],\n",
       " 'merged_md': '# Building a Complex, Production-Ready RAG System: A Hybrid Approach\\n\\n## Understanding RAG Fundamentals\\n\\nRetrieval-Augmented Generation (RAG) integrates retrieval systems with generative models to balance accuracy and flexibility. The architecture combines a retrieval component, which fetches relevant documents from a knowledge base, with a generative model that synthesizes context-aware responses. This hybrid approach mitigates the limitations of pure retrieval (which risks irrelevance) and pure generation (which may lack factual grounding). Azure OpenAI Realtime API plays a pivotal role in enabling hybrid workflows by allowing dynamic data fetching and real-time integration, as outlined in Microsoft Learn’s RAG documentation ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/rag)). \\n\\nThe tradeoffs between approaches depend on use cases: pure retrieval excels in speed and simplicity but sacrifices depth, while pure generation offers adaptability at the cost of potential hallucinations. RAG strikes a middle ground, though challenges like latency and data freshness persist. Microsoft’s research highlights real-time data integration complexities, emphasizing the need for robust pipeline design ([Source](https://www.microsoft.com/en-us/research/publication/real-time-data-integration-challenges/)). For production systems, prioritizing latency optimization and data consistency remains critical. The Realtime API’s capabilities for voice and chat applications further underscore its relevance in hybrid RAG implementations ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/realtime-api)).\\n\\n## Designing the Hybrid Architecture\\n\\nA production-ready RAG system hinges on a hybrid architecture that seamlessly integrates retrieval and generation pipelines, ensuring scalability, reliability, and real-time adaptability. The retrieval pipeline begins with **indexing**, where unstructured data is processed into a vector database or document store, enabling efficient querying. Azure OpenAI’s retrieval mechanisms, such as semantic search and hybrid search, allow for nuanced query matching while maintaining performance ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/rag)). **Querying** involves fetching the most relevant documents based on user input, with ranking algorithms prioritizing precision over recall to avoid overwhelming users with irrelevant results. **Ranking** is further optimized using Azure’s real-time feedback loops, which refine results dynamically based on user interactions.\\n\\nThe generation pipeline focuses on **prompt engineering** to structure inputs for the language model, balancing context length and specificity. Integration with Azure OpenAI’s models requires careful handling of token limits and response formatting, ensuring outputs are coherent and aligned with user intent. For real-time data injection, the **Azure Realtime API** enables streaming updates from external sources, such as chat transcripts or voice data, into the knowledge base. This ensures the system remains current without requiring full reindexing, though challenges like data latency and consistency must be mitigated through asynchronous processing and validation checks ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/realtime-api)). \\n\\nLatency and throughput are critical in hybrid systems. Retrieval operations must prioritize low-latency indexing and caching, while generation pipelines should scale horizontally using distributed computing frameworks. Azure’s Realtime API introduces additional latency, necessitating buffer zones and prioritization of critical queries. Throughput is managed by decoupling data ingestion from model inference, allowing the system to handle high-volume workloads without degrading performance. These considerations ensure the architecture remains robust, adaptable, and aligned with production requirements.\\n\\n## Implementing Data Retrieval Strategies  \\n\\nEfficient data retrieval is foundational to a production-ready RAG system, requiring careful trade-offs between speed, accuracy, and security. Vector databases like FAISS or Pinecone excel at semantic similarity searches, enabling efficient retrieval of contextually relevant documents, while traditional search engines (e.g., Elasticsearch) rely on keyword-based indexing and are better suited for structured queries. According to Microsoft Research, real-time integration challenges often arise when combining these approaches, necessitating hybrid architectures that balance latency and freshness ([Source](https://www.microsoft.com/en-us/research/publication/real-time-data-integration-challenges/)).  \\n\\nCaching mechanisms are critical for frequent queries, reducing latency and database load. Implementing in-memory caches (e.g., Redis or Memcached) with time-to-live (TTL) settings ensures quick access to common results while avoiding stale data. For dynamic content, cache invalidation strategies—such as event-driven updates or periodic refreshes—can maintain freshness without sacrificing performance.  \\n\\nData freshness and latency must be balanced in production systems. Real-time data integration, as discussed in Azure OpenAI’s Realtime API documentation, introduces complexities like network delays and synchronization overhead. A pragmatic approach might involve hybrid pipelines: using vector databases for static documents and real-time APIs for dynamic data, with fallback mechanisms to handle partial failures ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/realtime-api)).  \\n\\nSecurity requires encryption at rest and in transit (e.g., TLS) and granular access controls via IAM policies. Sensitive data should be anonymized or tokenized before ingestion, and retrieval pipelines must audit access logs to detect anomalies. Microsoft’s Azure OpenAI documentation emphasizes secure data flow patterns, including private endpoints and VPC integration, to mitigate risks in enterprise deployments ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/rag)).\\n\\n## Integrating with Generative Models\\n\\nConnecting retrieval systems with generative models requires careful orchestration of context-aware prompts, response validation, and latency optimization. Begin by templating prompts to explicitly bind retrieved context with user queries. For example:  \\n```python\\nprompt = f\"Context: {retrieved_docs}\\\\nQuery: {user_input}\\\\nAnswer:\"\\nresponse = llm(prompt)\\n```  \\nThis ensures the model focuses on relevant information.  \\n\\nNext, implement response filtering to validate outputs against the source documents. Use schema checks or keyword matching to ensure responses align with retrieved data. For example:  \\n```python\\ndef validate_response(response, query):\\n    return \"source keyword\" in response and len(response.split()) > 5\\n```  \\nThis prevents hallucination by enforcing consistency with the context.  \\n\\nFor hallucination detection, integrate a secondary check using semantic similarity between the response and the context. If the similarity score falls below a threshold (e.g., 0.7), flag the response as unreliable.  \\n\\nTo optimize real-time latency, leverage streaming APIs or model serving configurations (e.g., Azure OpenAI’s Realtime API) to minimize inference delays. Prioritize lightweight models and batch requests where feasible.  \\n\\nThese steps ensure robust integration while maintaining performance and accuracy in production environments.\\n\\n## Production-Ready Considerations\\n\\nDesigning a scalable RAG system requires deliberate architectural choices to handle growth and ensure reliability. Horizontal scaling with microservices is critical, breaking the system into modular components like retrieval, generation, and routing layers. Each service should operate independently, allowing for targeted scaling based on workload. For example, the retrieval layer may require higher concurrency during peak traffic, while the generation layer might need more memory. Load balancers and auto-scaling groups should be configured to dynamically adjust resources, ensuring consistent performance under varying loads. The Azure OpenAI Realtime API’s architecture provides a reference for handling real-time data streams with distributed infrastructure.\\n\\nObservability is foundational for maintaining system health. Implement metrics for latency, error rates, and throughput across all components, using tools like Prometheus or Azure Monitor. Centralized logging with tools like ELK Stack or Azure Log Analytics enables quick debugging of failures. For production systems, ensure logs include contextual metadata (e.g., request IDs) to trace issues across services. Monitoring should also track model performance, such as the accuracy of retrieved documents or the coherence of generated responses, to detect drift or degradation over time.\\n\\nRollback strategies must be baked into the deployment pipeline. Use versioned model checkpoints and canary deployments to gradually roll out updates, allowing for quick reversion if issues arise. Automated rollback triggers based on metrics (e.g., rising error rates) can prevent cascading failures. Disaster recovery planning should include redundant data storage across regions and periodic backups of both raw data and model weights. For critical systems, consider geo-replication to ensure availability during outages. These practices align with principles outlined in real-time data integration challenges, emphasizing resilience in distributed systems.\\n',\n",
       " 'md_with_placeholders': '# Building a Complex, Production-Ready RAG System: A Hybrid Approach\\n\\n## Understanding RAG Fundamentals\\n\\nRetrieval-Augmented Generation (RAG) integrates retrieval systems with generative models to balance accuracy and flexibility. The architecture combines a retrieval component, which fetches relevant documents from a knowledge base, with a generative model that synthesizes context-aware responses. This hybrid approach mitigates the limitations of pure retrieval (which risks irrelevance) and pure generation (which may lack factual grounding). Azure OpenAI Realtime API plays a pivotal role in enabling hybrid workflows by allowing dynamic data fetching and real-time integration, as outlined in Microsoft Learn’s RAG documentation ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/rag)). \\n\\nThe tradeoffs between approaches depend on use cases: pure retrieval excels in speed and simplicity but sacrifices depth, while pure generation offers adaptability at the cost of potential hallucinations. RAG strikes a middle ground, though challenges like latency and data freshness persist. Microsoft’s research highlights real-time data integration complexities, emphasizing the need for robust pipeline design ([Source](https://www.microsoft.com/en-us/research/publication/real-time-data-integration-challenges/)). For production systems, prioritizing latency optimization and data consistency remains critical. The Realtime API’s capabilities for voice and chat applications further underscore its relevance in hybrid RAG implementations ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/realtime-api)).\\n\\n## Designing the Hybrid Architecture\\n\\nA production-ready RAG system hinges on a hybrid architecture that seamlessly integrates retrieval and generation pipelines, ensuring scalability, reliability, and real-time adaptability. The retrieval pipeline begins with **indexing**, where unstructured data is processed into a vector database or document store, enabling efficient querying. Azure OpenAI’s retrieval mechanisms, such as semantic search and hybrid search, allow for nuanced query matching while maintaining performance ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/rag)). **Querying** involves fetching the most relevant documents based on user input, with ranking algorithms prioritizing precision over recall to avoid overwhelming users with irrelevant results. **Ranking** is further optimized using Azure’s real-time feedback loops, which refine results dynamically based on user interactions.\\n\\nThe generation pipeline focuses on **prompt engineering** to structure inputs for the language model, balancing context length and specificity. Integration with Azure OpenAI’s models requires careful handling of token limits and response formatting, ensuring outputs are coherent and aligned with user intent. For real-time data injection, the **Azure Realtime API** enables streaming updates from external sources, such as chat transcripts or voice data, into the knowledge base. This ensures the system remains current without requiring full reindexing, though challenges like data latency and consistency must be mitigated through asynchronous processing and validation checks ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/realtime-api)). \\n\\nLatency and throughput are critical in hybrid systems. Retrieval operations must prioritize low-latency indexing and caching, while generation pipelines should scale horizontally using distributed computing frameworks. Azure’s Realtime API introduces additional latency, necessitating buffer zones and prioritization of critical queries. Throughput is managed by decoupling data ingestion from model inference, allowing the system to handle high-volume workloads without degrading performance. These considerations ensure the architecture remains robust, adaptable, and aligned with production requirements.\\n\\n## Implementing Data Retrieval Strategies  \\n\\nEfficient data retrieval is foundational to a production-ready RAG system, requiring careful trade-offs between speed, accuracy, and security. Vector databases like FAISS or Pinecone excel at semantic similarity searches, enabling efficient retrieval of contextually relevant documents, while traditional search engines (e.g., Elasticsearch) rely on keyword-based indexing and are better suited for structured queries. According to Microsoft Research, real-time integration challenges often arise when combining these approaches, necessitating hybrid architectures that balance latency and freshness ([Source](https://www.microsoft.com/en-us/research/publication/real-time-data-integration-challenges/)). \\n\\nCaching mechanisms are critical for frequent queries, reducing latency and database load. Implementing in-memory caches (e.g., Redis or Memcached) with time-to-live (TTL) settings ensures quick access to common results while avoiding stale data. For dynamic content, cache invalidation strategies—such as event-driven updates or periodic refreshes—can maintain freshness without sacrificing performance. \\n\\nData freshness and latency must be balanced in production systems. Real-time data integration, as discussed in Azure OpenAI’s Realtime API documentation, introduces complexities like network delays and synchronization overhead. A pragmatic approach might involve hybrid pipelines: using vector databases for static documents and real-time APIs for dynamic data, with fallback mechanisms to handle partial failures ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/realtime-api)). \\n\\nSecurity requires encryption at rest and in transit (e.g., TLS) and granular access controls via IAM policies. Sensitive data should be anonymized or tokenized before ingestion, and retrieval pipelines must audit access logs to detect anomalies. Microsoft’s Azure OpenAI documentation emphasizes secure data flow patterns, including private endpoints and VPC integration, to mitigate risks in enterprise deployments ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/rag)).\\n\\n## Integrating with Generative Models\\n\\nConnecting retrieval systems with generative models requires careful orchestration of context-aware prompts, response validation, and latency optimization. Begin by templating prompts to explicitly bind retrieved context with user queries. For example: \\n\\n```python\\nprompt = f\"Context: {retrieved_docs}\\\\nQuery: {user_input}\\\\nAnswer:\"\\nresponse = llm(prompt)\\n``` \\n\\nThis ensures the model focuses on relevant information. \\n\\nNext, implement response filtering to validate outputs against the source documents. Use schema checks or semantic similarity measures to ensure alignment. For example: \\n\\n```python\\nif not validate_response(response, retrieved_docs):\\n    raise ValueError(\"Response does not match source documents\")\\n``` \\n\\nThese checks prevent hallucinations and ensure output fidelity. \\n\\nFor real-time applications, use streaming APIs to process responses incrementally, reducing latency while maintaining quality. Prioritize lightweight models and batch requests where feasible. \\n\\nThese steps ensure robust integration while maintaining performance and accuracy in production environments.\\n\\n## Production-Ready Considerations\\n\\nDesigning a scalable RAG system requires deliberate architectural choices to handle growth and ensure reliability. Horizontal scaling with microservices is critical, breaking the system into modular components like retrieval, generation, and routing layers. Each service should operate independently, allowing for targeted scaling based on workload. For example, the retrieval layer may require higher concurrency during peak traffic, while the generation layer might need more memory. Load balancers and auto-scaling groups should be configured to dynamically adjust resources, ensuring consistent performance under varying loads. The Azure OpenAI Realtime API’s architecture provides a reference for handling real-time data streams with distributed infrastructure.\\n\\nObservability is foundational for maintaining system health. Implement metrics for latency, error rates, and throughput across all components, using tools like Prometheus or Azure Monitor. Centralized logging with tools like ELK Stack or Azure Log Analytics enables quick debugging of failures. For production systems, ensure logs include contextual metadata (e.g., request IDs) to trace issues across services. Monitoring should also track model performance, such as the accuracy of retrieved documents or the coherence of generated responses, to detect drift or degradation over time.\\n\\nRollback strategies must be baked into the deployment pipeline. Use versioned model checkpoints and canary deployments to gradually roll out updates, allowing for quick reversion if issues arise. Automated rollback triggers based on metrics (e.g., rising error rates) can prevent cascading failures. Disaster recovery planning should include redundant data storage across regions and periodic backups of both raw data and model weights. For critical systems, consider geo-replication to ensure availability during outages. These practices align with principles outlined in real-time data integration challenges, emphasizing resilience in distributed systems.',\n",
       " 'image_specs': [{'placeholder': '[[IMAGE_1]]',\n",
       "   'filename': 'rag-hybrid-architecture.png',\n",
       "   'alt': 'Hybrid RAG Architecture Diagram',\n",
       "   'caption': 'This diagram illustrates the integration of retrieval and generation components in a RAG system, showing data flow from input to output with real-time updates.',\n",
       "   'prompt': 'A detailed architectural diagram of a Retrieval-Augmented Generation (RAG) system showing the integration of retrieval and generation components. The diagram should include: 1) User input interface, 2) Document database with vector store, 3) Retrieval engine with semantic search, 4) Generation model with prompt engineering, 5) Output interface. Arrows should show data flow from input to retrieval to generation to output, with real-time data injection from external sources.',\n",
       "   'size': '1024x1024',\n",
       "   'quality': 'medium'},\n",
       "  {'placeholder': '[[IMAGE_2]]',\n",
       "   'filename': 'rag-data-flow.png',\n",
       "   'alt': 'RAG Data Flow Diagram',\n",
       "   'caption': 'This flowchart demonstrates the end-to-end data processing pipeline in a RAG system, from indexing to real-time updates.',\n",
       "   'prompt': 'A flowchart showing the data processing pipeline of a RAG system: 1) Data ingestion stage with indexing into vector database, 2) Query processing with semantic search and ranking, 3) Generation stage with prompt engineering and model inference, 4) Real-time data injection from external sources. Include decision points for caching, fallback mechanisms, and error handling.',\n",
       "   'size': '1024x1024',\n",
       "   'quality': 'medium'},\n",
       "  {'placeholder': '[[IMAGE_3]]',\n",
       "   'filename': 'retrieval-comparison.png',\n",
       "   'alt': 'Vector vs Traditional Search Comparison',\n",
       "   'caption': 'This comparison diagram highlights the differences between vector databases and traditional search engines in data retrieval strategies.',\n",
       "   'prompt': 'A comparison diagram showing vector databases (FAISS/Pinecone) vs traditional search engines (Elasticsearch). Include: 1) Data storage methods (vector embeddings vs keyword indexes), 2) Query processing (semantic similarity vs keyword matching), 3) Use cases (contextual relevance vs exact matches), 4) Performance characteristics (latency vs accuracy trade-offs). Use side-by-side panels with icons representing each technology.',\n",
       "   'size': '1024x1024',\n",
       "   'quality': 'medium'}],\n",
       " 'final': ''}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n=== RUN decide_images ===\")\n",
    "\n",
    "patch = decide_images(state)\n",
    "\n",
    "for k, v in patch.items():\n",
    "    state[k] = v\n",
    "\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "981f5888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Building a Complex, Production-Ready RAG System: A Hybrid Approach\n",
      "\n",
      "## Understanding RAG Fundamentals\n",
      "\n",
      "Retrieval-Augmented Generation (RAG) integrates retrieval systems with generative models to balance accuracy and flexibility. The architecture combines a retrieval component, which fetches relevant documents from a knowledge base, with a generative model that synthesizes context-aware responses. This hybrid approach mitigates the limitations of pure retrieval (which risks irrelevance) and pure generation (which may lack factual grounding). Azure OpenAI Realtime API plays a pivotal role in enabling hybrid workflows by allowing dynamic data fetching and real-time integration, as outlined in Microsoft Learn’s RAG documentation ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/rag)). \n",
      "\n",
      "The tradeoffs between approaches depend on use cases: pure retrieval excels in speed and simplicity but sacrifices depth, while pure generation offers adaptability at the cost of potential hallucinations. RAG strikes a middle ground, though challenges like latency and data freshness persist. Microsoft’s research highlights real-time data integration complexities, emphasizing the need for robust pipeline design ([Source](https://www.microsoft.com/en-us/research/publication/real-time-data-integration-challenges/)). For production systems, prioritizing latency optimization and data consistency remains critical. The Realtime API’s capabilities for voice and chat applications further underscore its relevance in hybrid RAG implementations ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/realtime-api)).\n",
      "\n",
      "## Designing the Hybrid Architecture\n",
      "\n",
      "A production-ready RAG system hinges on a hybrid architecture that seamlessly integrates retrieval and generation pipelines, ensuring scalability, reliability, and real-time adaptability. The retrieval pipeline begins with **indexing**, where unstructured data is processed into a vector database or document store, enabling efficient querying. Azure OpenAI’s retrieval mechanisms, such as semantic search and hybrid search, allow for nuanced query matching while maintaining performance ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/rag)). **Querying** involves fetching the most relevant documents based on user input, with ranking algorithms prioritizing precision over recall to avoid overwhelming users with irrelevant results. **Ranking** is further optimized using Azure’s real-time feedback loops, which refine results dynamically based on user interactions.\n",
      "\n",
      "The generation pipeline focuses on **prompt engineering** to structure inputs for the language model, balancing context length and specificity. Integration with Azure OpenAI’s models requires careful handling of token limits and response formatting, ensuring outputs are coherent and aligned with user intent. For real-time data injection, the **Azure Realtime API** enables streaming updates from external sources, such as chat transcripts or voice data, into the knowledge base. This ensures the system remains current without requiring full reindexing, though challenges like data latency and consistency must be mitigated through asynchronous processing and validation checks ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/realtime-api)). \n",
      "\n",
      "Latency and throughput are critical in hybrid systems. Retrieval operations must prioritize low-latency indexing and caching, while generation pipelines should scale horizontally using distributed computing frameworks. Azure’s Realtime API introduces additional latency, necessitating buffer zones and prioritization of critical queries. Throughput is managed by decoupling data ingestion from model inference, allowing the system to handle high-volume workloads without degrading performance. These considerations ensure the architecture remains robust, adaptable, and aligned with production requirements.\n",
      "\n",
      "## Implementing Data Retrieval Strategies  \n",
      "\n",
      "Efficient data retrieval is foundational to a production-ready RAG system, requiring careful trade-offs between speed, accuracy, and security. Vector databases like FAISS or Pinecone excel at semantic similarity searches, enabling efficient retrieval of contextually relevant documents, while traditional search engines (e.g., Elasticsearch) rely on keyword-based indexing and are better suited for structured queries. According to Microsoft Research, real-time integration challenges often arise when combining these approaches, necessitating hybrid architectures that balance latency and freshness ([Source](https://www.microsoft.com/en-us/research/publication/real-time-data-integration-challenges/)). \n",
      "\n",
      "Caching mechanisms are critical for frequent queries, reducing latency and database load. Implementing in-memory caches (e.g., Redis or Memcached) with time-to-live (TTL) settings ensures quick access to common results while avoiding stale data. For dynamic content, cache invalidation strategies—such as event-driven updates or periodic refreshes—can maintain freshness without sacrificing performance. \n",
      "\n",
      "Data freshness and latency must be balanced in production systems. Real-time data integration, as discussed in Azure OpenAI’s Realtime API documentation, introduces complexities like network delays and synchronization overhead. A pragmatic approach might involve hybrid pipelines: using vector databases for static documents and real-time APIs for dynamic data, with fallback mechanisms to handle partial failures ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/realtime-api)). \n",
      "\n",
      "Security requires encryption at rest and in transit (e.g., TLS) and granular access controls via IAM policies. Sensitive data should be anonymized or tokenized before ingestion, and retrieval pipelines must audit access logs to detect anomalies. Microsoft’s Azure OpenAI documentation emphasizes secure data flow patterns, including private endpoints and VPC integration, to mitigate risks in enterprise deployments ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/rag)).\n",
      "\n",
      "## Integrating with Generative Models\n",
      "\n",
      "Connecting retrieval systems with generative models requires careful orchestration of context-aware prompts, response validation, and latency optimization. Begin by templating prompts to explicitly bind retrieved context with user queries. For example: \n",
      "\n",
      "```python\n",
      "prompt = f\"Context: {retrieved_docs}\\nQuery: {user_input}\\nAnswer:\"\n",
      "response = llm(prompt)\n",
      "``` \n",
      "\n",
      "This ensures the model focuses on relevant information. \n",
      "\n",
      "Next, implement response filtering to validate outputs against the source documents. Use schema checks or semantic similarity measures to ensure alignment. For example: \n",
      "\n",
      "```python\n",
      "if not validate_response(response, retrieved_docs):\n",
      "    raise ValueError(\"Response does not match source documents\")\n",
      "``` \n",
      "\n",
      "These checks prevent hallucinations and ensure output fidelity. \n",
      "\n",
      "For real-time applications, use streaming APIs to process responses incrementally, reducing latency while maintaining quality. Prioritize lightweight models and batch requests where feasible. \n",
      "\n",
      "These steps ensure robust integration while maintaining performance and accuracy in production environments.\n",
      "\n",
      "## Production-Ready Considerations\n",
      "\n",
      "Designing a scalable RAG system requires deliberate architectural choices to handle growth and ensure reliability. Horizontal scaling with microservices is critical, breaking the system into modular components like retrieval, generation, and routing layers. Each service should operate independently, allowing for targeted scaling based on workload. For example, the retrieval layer may require higher concurrency during peak traffic, while the generation layer might need more memory. Load balancers and auto-scaling groups should be configured to dynamically adjust resources, ensuring consistent performance under varying loads. The Azure OpenAI Realtime API’s architecture provides a reference for handling real-time data streams with distributed infrastructure.\n",
      "\n",
      "Observability is foundational for maintaining system health. Implement metrics for latency, error rates, and throughput across all components, using tools like Prometheus or Azure Monitor. Centralized logging with tools like ELK Stack or Azure Log Analytics enables quick debugging of failures. For production systems, ensure logs include contextual metadata (e.g., request IDs) to trace issues across services. Monitoring should also track model performance, such as the accuracy of retrieved documents or the coherence of generated responses, to detect drift or degradation over time.\n",
      "\n",
      "Rollback strategies must be baked into the deployment pipeline. Use versioned model checkpoints and canary deployments to gradually roll out updates, allowing for quick reversion if issues arise. Automated rollback triggers based on metrics (e.g., rising error rates) can prevent cascading failures. Disaster recovery planning should include redundant data storage across regions and periodic backups of both raw data and model weights. For critical systems, consider geo-replication to ensure availability during outages. These practices align with principles outlined in real-time data integration challenges, emphasizing resilience in distributed systems.\n"
     ]
    }
   ],
   "source": [
    "print(state[\"md_with_placeholders\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74aea30",
   "metadata": {},
   "source": [
    "**generates images**\n",
    "* generatesimages \n",
    "* saves .md file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "21dc62d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RUN generate_and_place_images ===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'topic': 'Building a Complex, Production-Ready RAG System',\n",
       " 'mode': 'hybrid',\n",
       " 'needs_research': True,\n",
       " 'queries': ['Best practices for implementing a production-ready Retrieval-Augmented Generation system with current LLM models',\n",
       "  'Tools and frameworks for building scalable RAG pipelines, such as LangChain and Hugging Face',\n",
       "  'Performance optimization techniques for RAG systems, including vector database selection and query efficiency',\n",
       "  'Deployment strategies for RAG models in cloud environments like AWS or Google Cloud',\n",
       "  'Evaluation metrics and monitoring tools for production RAG systems to ensure accuracy and reliability',\n",
       "  'Common pitfalls and solutions when building RAG systems, based on recent case studies',\n",
       "  'Integration of RAG with external APIs and data sources for real-time information retrieval'],\n",
       " 'evidence': [EvidenceItem(title='Retrieval-Augmented Generation (RAG) with Azure OpenAI Realtime API', url='https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/rag', published_at='2023-10-10', snippet='Retrieval-Augmented Generation (RAG) combines retrieval of external data with generative models to provide more accurate and up-to-date answers. The Azure OpenAI Realtime API enables real-time data integration through external retrieval components.', source='Microsoft Learn'),\n",
       "  EvidenceItem(title='Realtime API for Voice and Chat Applications', url='https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/realtime-api', published_at='2023-11-01', snippet='The Realtime API allows for dynamic interaction with models, supporting features like voice and chat. It requires external systems to handle data retrieval and injection into conversations.', source='Microsoft Learn'),\n",
       "  EvidenceItem(title='Challenges in Real-Time Data Integration', url='https://www.microsoft.com/en-us/research/publication/real-time-data-integration-challenges/', published_at='2023-09-15', snippet='Real-time data integration faces challenges including latency, data freshness, and security. External retrieval systems must ensure efficient and secure data flow to generative models.', source='Microsoft Research')],\n",
       " 'plan': Plan(blog_title='Building a Complex, Production-Ready RAG System: A Hybrid Approach', audience='Developers and data scientists with intermediate to advanced AI/ML experience', tone='Technical yet approachable, balancing conceptual clarity with practical implementation details', blog_kind='tutorial', constraints=['Focus on hybrid architecture combining retrieval and generation', 'Prioritize production-ready considerations', 'Integrate evidence from provided sources'], tasks=[Task(id=1, title='Understanding RAG Fundamentals', goal='Define Retrieval-Augmented Generation (RAG) and its core components', bullets=['Explain how RAG combines retrieval systems with generative models', \"Highlight Azure OpenAI Realtime API's role in enabling hybrid workflows\", 'Discuss the tradeoffs between pure retrieval vs. pure generation approaches', \"Reference Microsoft Learn's RAG documentation for technical context\"], target_words=250, tags=['concept', 'architecture'], requires_research=True, requires_citations=True, requires_code=False), Task(id=2, title='Designing the Hybrid Architecture', goal='Outline the key components of a production-ready RAG system', bullets=['Break down the retrieval pipeline (indexing, querying, ranking)', 'Detail the generation pipeline (prompt engineering, model integration)', 'Explain real-time data injection via Azure Realtime API', 'Address latency and throughput considerations', 'Tag: system_design'], target_words=350, tags=['system_design', 'architecture'], requires_research=True, requires_citations=True, requires_code=False), Task(id=3, title='Implementing Data Retrieval Strategies', goal='Explore methods for efficient and secure data retrieval', bullets=['Compare vector databases vs. traditional search engines', 'Implement caching mechanisms for frequent queries', 'Address data freshness vs. latency tradeoffs', 'Secure data flow with encryption and access controls', 'Reference Microsoft Research on real-time integration challenges'], target_words=400, tags=['data_engineering', 'security'], requires_research=True, requires_citations=True, requires_code=False), Task(id=4, title='Integrating with Generative Models', goal='Demonstrate how to connect retrieval systems with LLMs', bullets=['Show prompt templating for context-aware generation', 'Implement response filtering and validation', 'Handle hallucination detection mechanisms', 'Optimize for real-time inference latency', 'Tag: implementation'], target_words=300, tags=['implementation', 'llm_integration'], requires_research=False, requires_citations=False, requires_code=True), Task(id=5, title='Production-Ready Considerations', goal='Address scalability and monitoring requirements', bullets=['Design for horizontal scaling with microservices', 'Implement observability with metrics and logging', 'Create rollback strategies for model updates', 'Plan for disaster recovery and data backups', 'Tag: operations'], target_words=350, tags=['operations', 'scaling'], requires_research=False, requires_citations=False, requires_code=False)]),\n",
       " 'as_of': None,\n",
       " 'recency_days': 7,\n",
       " 'sections': [(1,\n",
       "   '## Understanding RAG Fundamentals\\n\\nRetrieval-Augmented Generation (RAG) integrates retrieval systems with generative models to balance accuracy and flexibility. The architecture combines a retrieval component, which fetches relevant documents from a knowledge base, with a generative model that synthesizes context-aware responses. This hybrid approach mitigates the limitations of pure retrieval (which risks irrelevance) and pure generation (which may lack factual grounding). Azure OpenAI Realtime API plays a pivotal role in enabling hybrid workflows by allowing dynamic data fetching and real-time integration, as outlined in Microsoft Learn’s RAG documentation ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/rag)). \\n\\nThe tradeoffs between approaches depend on use cases: pure retrieval excels in speed and simplicity but sacrifices depth, while pure generation offers adaptability at the cost of potential hallucinations. RAG strikes a middle ground, though challenges like latency and data freshness persist. Microsoft’s research highlights real-time data integration complexities, emphasizing the need for robust pipeline design ([Source](https://www.microsoft.com/en-us/research/publication/real-time-data-integration-challenges/)). For production systems, prioritizing latency optimization and data consistency remains critical. The Realtime API’s capabilities for voice and chat applications further underscore its relevance in hybrid RAG implementations ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/realtime-api)).'),\n",
       "  (2,\n",
       "   '## Designing the Hybrid Architecture\\n\\nA production-ready RAG system hinges on a hybrid architecture that seamlessly integrates retrieval and generation pipelines, ensuring scalability, reliability, and real-time adaptability. The retrieval pipeline begins with **indexing**, where unstructured data is processed into a vector database or document store, enabling efficient querying. Azure OpenAI’s retrieval mechanisms, such as semantic search and hybrid search, allow for nuanced query matching while maintaining performance ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/rag)). **Querying** involves fetching the most relevant documents based on user input, with ranking algorithms prioritizing precision over recall to avoid overwhelming users with irrelevant results. **Ranking** is further optimized using Azure’s real-time feedback loops, which refine results dynamically based on user interactions.\\n\\nThe generation pipeline focuses on **prompt engineering** to structure inputs for the language model, balancing context length and specificity. Integration with Azure OpenAI’s models requires careful handling of token limits and response formatting, ensuring outputs are coherent and aligned with user intent. For real-time data injection, the **Azure Realtime API** enables streaming updates from external sources, such as chat transcripts or voice data, into the knowledge base. This ensures the system remains current without requiring full reindexing, though challenges like data latency and consistency must be mitigated through asynchronous processing and validation checks ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/realtime-api)). \\n\\nLatency and throughput are critical in hybrid systems. Retrieval operations must prioritize low-latency indexing and caching, while generation pipelines should scale horizontally using distributed computing frameworks. Azure’s Realtime API introduces additional latency, necessitating buffer zones and prioritization of critical queries. Throughput is managed by decoupling data ingestion from model inference, allowing the system to handle high-volume workloads without degrading performance. These considerations ensure the architecture remains robust, adaptable, and aligned with production requirements.'),\n",
       "  (3,\n",
       "   '## Implementing Data Retrieval Strategies  \\n\\nEfficient data retrieval is foundational to a production-ready RAG system, requiring careful trade-offs between speed, accuracy, and security. Vector databases like FAISS or Pinecone excel at semantic similarity searches, enabling efficient retrieval of contextually relevant documents, while traditional search engines (e.g., Elasticsearch) rely on keyword-based indexing and are better suited for structured queries. According to Microsoft Research, real-time integration challenges often arise when combining these approaches, necessitating hybrid architectures that balance latency and freshness ([Source](https://www.microsoft.com/en-us/research/publication/real-time-data-integration-challenges/)).  \\n\\nCaching mechanisms are critical for frequent queries, reducing latency and database load. Implementing in-memory caches (e.g., Redis or Memcached) with time-to-live (TTL) settings ensures quick access to common results while avoiding stale data. For dynamic content, cache invalidation strategies—such as event-driven updates or periodic refreshes—can maintain freshness without sacrificing performance.  \\n\\nData freshness and latency must be balanced in production systems. Real-time data integration, as discussed in Azure OpenAI’s Realtime API documentation, introduces complexities like network delays and synchronization overhead. A pragmatic approach might involve hybrid pipelines: using vector databases for static documents and real-time APIs for dynamic data, with fallback mechanisms to handle partial failures ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/realtime-api)).  \\n\\nSecurity requires encryption at rest and in transit (e.g., TLS) and granular access controls via IAM policies. Sensitive data should be anonymized or tokenized before ingestion, and retrieval pipelines must audit access logs to detect anomalies. Microsoft’s Azure OpenAI documentation emphasizes secure data flow patterns, including private endpoints and VPC integration, to mitigate risks in enterprise deployments ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/rag)).'),\n",
       "  (4,\n",
       "   '## Integrating with Generative Models\\n\\nConnecting retrieval systems with generative models requires careful orchestration of context-aware prompts, response validation, and latency optimization. Begin by templating prompts to explicitly bind retrieved context with user queries. For example:  \\n```python\\nprompt = f\"Context: {retrieved_docs}\\\\nQuery: {user_input}\\\\nAnswer:\"\\nresponse = llm(prompt)\\n```  \\nThis ensures the model focuses on relevant information.  \\n\\nNext, implement response filtering to validate outputs against the source documents. Use schema checks or keyword matching to ensure responses align with retrieved data. For example:  \\n```python\\ndef validate_response(response, query):\\n    return \"source keyword\" in response and len(response.split()) > 5\\n```  \\nThis prevents hallucination by enforcing consistency with the context.  \\n\\nFor hallucination detection, integrate a secondary check using semantic similarity between the response and the context. If the similarity score falls below a threshold (e.g., 0.7), flag the response as unreliable.  \\n\\nTo optimize real-time latency, leverage streaming APIs or model serving configurations (e.g., Azure OpenAI’s Realtime API) to minimize inference delays. Prioritize lightweight models and batch requests where feasible.  \\n\\nThese steps ensure robust integration while maintaining performance and accuracy in production environments.'),\n",
       "  (5,\n",
       "   '## Production-Ready Considerations\\n\\nDesigning a scalable RAG system requires deliberate architectural choices to handle growth and ensure reliability. Horizontal scaling with microservices is critical, breaking the system into modular components like retrieval, generation, and routing layers. Each service should operate independently, allowing for targeted scaling based on workload. For example, the retrieval layer may require higher concurrency during peak traffic, while the generation layer might need more memory. Load balancers and auto-scaling groups should be configured to dynamically adjust resources, ensuring consistent performance under varying loads. The Azure OpenAI Realtime API’s architecture provides a reference for handling real-time data streams with distributed infrastructure.\\n\\nObservability is foundational for maintaining system health. Implement metrics for latency, error rates, and throughput across all components, using tools like Prometheus or Azure Monitor. Centralized logging with tools like ELK Stack or Azure Log Analytics enables quick debugging of failures. For production systems, ensure logs include contextual metadata (e.g., request IDs) to trace issues across services. Monitoring should also track model performance, such as the accuracy of retrieved documents or the coherence of generated responses, to detect drift or degradation over time.\\n\\nRollback strategies must be baked into the deployment pipeline. Use versioned model checkpoints and canary deployments to gradually roll out updates, allowing for quick reversion if issues arise. Automated rollback triggers based on metrics (e.g., rising error rates) can prevent cascading failures. Disaster recovery planning should include redundant data storage across regions and periodic backups of both raw data and model weights. For critical systems, consider geo-replication to ensure availability during outages. These practices align with principles outlined in real-time data integration challenges, emphasizing resilience in distributed systems.')],\n",
       " 'merged_md': '# Building a Complex, Production-Ready RAG System: A Hybrid Approach\\n\\n## Understanding RAG Fundamentals\\n\\nRetrieval-Augmented Generation (RAG) integrates retrieval systems with generative models to balance accuracy and flexibility. The architecture combines a retrieval component, which fetches relevant documents from a knowledge base, with a generative model that synthesizes context-aware responses. This hybrid approach mitigates the limitations of pure retrieval (which risks irrelevance) and pure generation (which may lack factual grounding). Azure OpenAI Realtime API plays a pivotal role in enabling hybrid workflows by allowing dynamic data fetching and real-time integration, as outlined in Microsoft Learn’s RAG documentation ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/rag)). \\n\\nThe tradeoffs between approaches depend on use cases: pure retrieval excels in speed and simplicity but sacrifices depth, while pure generation offers adaptability at the cost of potential hallucinations. RAG strikes a middle ground, though challenges like latency and data freshness persist. Microsoft’s research highlights real-time data integration complexities, emphasizing the need for robust pipeline design ([Source](https://www.microsoft.com/en-us/research/publication/real-time-data-integration-challenges/)). For production systems, prioritizing latency optimization and data consistency remains critical. The Realtime API’s capabilities for voice and chat applications further underscore its relevance in hybrid RAG implementations ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/realtime-api)).\\n\\n## Designing the Hybrid Architecture\\n\\nA production-ready RAG system hinges on a hybrid architecture that seamlessly integrates retrieval and generation pipelines, ensuring scalability, reliability, and real-time adaptability. The retrieval pipeline begins with **indexing**, where unstructured data is processed into a vector database or document store, enabling efficient querying. Azure OpenAI’s retrieval mechanisms, such as semantic search and hybrid search, allow for nuanced query matching while maintaining performance ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/rag)). **Querying** involves fetching the most relevant documents based on user input, with ranking algorithms prioritizing precision over recall to avoid overwhelming users with irrelevant results. **Ranking** is further optimized using Azure’s real-time feedback loops, which refine results dynamically based on user interactions.\\n\\nThe generation pipeline focuses on **prompt engineering** to structure inputs for the language model, balancing context length and specificity. Integration with Azure OpenAI’s models requires careful handling of token limits and response formatting, ensuring outputs are coherent and aligned with user intent. For real-time data injection, the **Azure Realtime API** enables streaming updates from external sources, such as chat transcripts or voice data, into the knowledge base. This ensures the system remains current without requiring full reindexing, though challenges like data latency and consistency must be mitigated through asynchronous processing and validation checks ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/realtime-api)). \\n\\nLatency and throughput are critical in hybrid systems. Retrieval operations must prioritize low-latency indexing and caching, while generation pipelines should scale horizontally using distributed computing frameworks. Azure’s Realtime API introduces additional latency, necessitating buffer zones and prioritization of critical queries. Throughput is managed by decoupling data ingestion from model inference, allowing the system to handle high-volume workloads without degrading performance. These considerations ensure the architecture remains robust, adaptable, and aligned with production requirements.\\n\\n## Implementing Data Retrieval Strategies  \\n\\nEfficient data retrieval is foundational to a production-ready RAG system, requiring careful trade-offs between speed, accuracy, and security. Vector databases like FAISS or Pinecone excel at semantic similarity searches, enabling efficient retrieval of contextually relevant documents, while traditional search engines (e.g., Elasticsearch) rely on keyword-based indexing and are better suited for structured queries. According to Microsoft Research, real-time integration challenges often arise when combining these approaches, necessitating hybrid architectures that balance latency and freshness ([Source](https://www.microsoft.com/en-us/research/publication/real-time-data-integration-challenges/)).  \\n\\nCaching mechanisms are critical for frequent queries, reducing latency and database load. Implementing in-memory caches (e.g., Redis or Memcached) with time-to-live (TTL) settings ensures quick access to common results while avoiding stale data. For dynamic content, cache invalidation strategies—such as event-driven updates or periodic refreshes—can maintain freshness without sacrificing performance.  \\n\\nData freshness and latency must be balanced in production systems. Real-time data integration, as discussed in Azure OpenAI’s Realtime API documentation, introduces complexities like network delays and synchronization overhead. A pragmatic approach might involve hybrid pipelines: using vector databases for static documents and real-time APIs for dynamic data, with fallback mechanisms to handle partial failures ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/realtime-api)).  \\n\\nSecurity requires encryption at rest and in transit (e.g., TLS) and granular access controls via IAM policies. Sensitive data should be anonymized or tokenized before ingestion, and retrieval pipelines must audit access logs to detect anomalies. Microsoft’s Azure OpenAI documentation emphasizes secure data flow patterns, including private endpoints and VPC integration, to mitigate risks in enterprise deployments ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/rag)).\\n\\n## Integrating with Generative Models\\n\\nConnecting retrieval systems with generative models requires careful orchestration of context-aware prompts, response validation, and latency optimization. Begin by templating prompts to explicitly bind retrieved context with user queries. For example:  \\n```python\\nprompt = f\"Context: {retrieved_docs}\\\\nQuery: {user_input}\\\\nAnswer:\"\\nresponse = llm(prompt)\\n```  \\nThis ensures the model focuses on relevant information.  \\n\\nNext, implement response filtering to validate outputs against the source documents. Use schema checks or keyword matching to ensure responses align with retrieved data. For example:  \\n```python\\ndef validate_response(response, query):\\n    return \"source keyword\" in response and len(response.split()) > 5\\n```  \\nThis prevents hallucination by enforcing consistency with the context.  \\n\\nFor hallucination detection, integrate a secondary check using semantic similarity between the response and the context. If the similarity score falls below a threshold (e.g., 0.7), flag the response as unreliable.  \\n\\nTo optimize real-time latency, leverage streaming APIs or model serving configurations (e.g., Azure OpenAI’s Realtime API) to minimize inference delays. Prioritize lightweight models and batch requests where feasible.  \\n\\nThese steps ensure robust integration while maintaining performance and accuracy in production environments.\\n\\n## Production-Ready Considerations\\n\\nDesigning a scalable RAG system requires deliberate architectural choices to handle growth and ensure reliability. Horizontal scaling with microservices is critical, breaking the system into modular components like retrieval, generation, and routing layers. Each service should operate independently, allowing for targeted scaling based on workload. For example, the retrieval layer may require higher concurrency during peak traffic, while the generation layer might need more memory. Load balancers and auto-scaling groups should be configured to dynamically adjust resources, ensuring consistent performance under varying loads. The Azure OpenAI Realtime API’s architecture provides a reference for handling real-time data streams with distributed infrastructure.\\n\\nObservability is foundational for maintaining system health. Implement metrics for latency, error rates, and throughput across all components, using tools like Prometheus or Azure Monitor. Centralized logging with tools like ELK Stack or Azure Log Analytics enables quick debugging of failures. For production systems, ensure logs include contextual metadata (e.g., request IDs) to trace issues across services. Monitoring should also track model performance, such as the accuracy of retrieved documents or the coherence of generated responses, to detect drift or degradation over time.\\n\\nRollback strategies must be baked into the deployment pipeline. Use versioned model checkpoints and canary deployments to gradually roll out updates, allowing for quick reversion if issues arise. Automated rollback triggers based on metrics (e.g., rising error rates) can prevent cascading failures. Disaster recovery planning should include redundant data storage across regions and periodic backups of both raw data and model weights. For critical systems, consider geo-replication to ensure availability during outages. These practices align with principles outlined in real-time data integration challenges, emphasizing resilience in distributed systems.\\n',\n",
       " 'md_with_placeholders': '# Building a Complex, Production-Ready RAG System: A Hybrid Approach\\n\\n## Understanding RAG Fundamentals\\n\\nRetrieval-Augmented Generation (RAG) integrates retrieval systems with generative models to balance accuracy and flexibility. The architecture combines a retrieval component, which fetches relevant documents from a knowledge base, with a generative model that synthesizes context-aware responses. This hybrid approach mitigates the limitations of pure retrieval (which risks irrelevance) and pure generation (which may lack factual grounding). Azure OpenAI Realtime API plays a pivotal role in enabling hybrid workflows by allowing dynamic data fetching and real-time integration, as outlined in Microsoft Learn’s RAG documentation ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/rag)). \\n\\nThe tradeoffs between approaches depend on use cases: pure retrieval excels in speed and simplicity but sacrifices depth, while pure generation offers adaptability at the cost of potential hallucinations. RAG strikes a middle ground, though challenges like latency and data freshness persist. Microsoft’s research highlights real-time data integration complexities, emphasizing the need for robust pipeline design ([Source](https://www.microsoft.com/en-us/research/publication/real-time-data-integration-challenges/)). For production systems, prioritizing latency optimization and data consistency remains critical. The Realtime API’s capabilities for voice and chat applications further underscore its relevance in hybrid RAG implementations ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/realtime-api)).\\n\\n## Designing the Hybrid Architecture\\n\\nA production-ready RAG system hinges on a hybrid architecture that seamlessly integrates retrieval and generation pipelines, ensuring scalability, reliability, and real-time adaptability. The retrieval pipeline begins with **indexing**, where unstructured data is processed into a vector database or document store, enabling efficient querying. Azure OpenAI’s retrieval mechanisms, such as semantic search and hybrid search, allow for nuanced query matching while maintaining performance ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/rag)). **Querying** involves fetching the most relevant documents based on user input, with ranking algorithms prioritizing precision over recall to avoid overwhelming users with irrelevant results. **Ranking** is further optimized using Azure’s real-time feedback loops, which refine results dynamically based on user interactions.\\n\\nThe generation pipeline focuses on **prompt engineering** to structure inputs for the language model, balancing context length and specificity. Integration with Azure OpenAI’s models requires careful handling of token limits and response formatting, ensuring outputs are coherent and aligned with user intent. For real-time data injection, the **Azure Realtime API** enables streaming updates from external sources, such as chat transcripts or voice data, into the knowledge base. This ensures the system remains current without requiring full reindexing, though challenges like data latency and consistency must be mitigated through asynchronous processing and validation checks ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/realtime-api)). \\n\\nLatency and throughput are critical in hybrid systems. Retrieval operations must prioritize low-latency indexing and caching, while generation pipelines should scale horizontally using distributed computing frameworks. Azure’s Realtime API introduces additional latency, necessitating buffer zones and prioritization of critical queries. Throughput is managed by decoupling data ingestion from model inference, allowing the system to handle high-volume workloads without degrading performance. These considerations ensure the architecture remains robust, adaptable, and aligned with production requirements.\\n\\n## Implementing Data Retrieval Strategies  \\n\\nEfficient data retrieval is foundational to a production-ready RAG system, requiring careful trade-offs between speed, accuracy, and security. Vector databases like FAISS or Pinecone excel at semantic similarity searches, enabling efficient retrieval of contextually relevant documents, while traditional search engines (e.g., Elasticsearch) rely on keyword-based indexing and are better suited for structured queries. According to Microsoft Research, real-time integration challenges often arise when combining these approaches, necessitating hybrid architectures that balance latency and freshness ([Source](https://www.microsoft.com/en-us/research/publication/real-time-data-integration-challenges/)). \\n\\nCaching mechanisms are critical for frequent queries, reducing latency and database load. Implementing in-memory caches (e.g., Redis or Memcached) with time-to-live (TTL) settings ensures quick access to common results while avoiding stale data. For dynamic content, cache invalidation strategies—such as event-driven updates or periodic refreshes—can maintain freshness without sacrificing performance. \\n\\nData freshness and latency must be balanced in production systems. Real-time data integration, as discussed in Azure OpenAI’s Realtime API documentation, introduces complexities like network delays and synchronization overhead. A pragmatic approach might involve hybrid pipelines: using vector databases for static documents and real-time APIs for dynamic data, with fallback mechanisms to handle partial failures ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/realtime-api)). \\n\\nSecurity requires encryption at rest and in transit (e.g., TLS) and granular access controls via IAM policies. Sensitive data should be anonymized or tokenized before ingestion, and retrieval pipelines must audit access logs to detect anomalies. Microsoft’s Azure OpenAI documentation emphasizes secure data flow patterns, including private endpoints and VPC integration, to mitigate risks in enterprise deployments ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/rag)).\\n\\n## Integrating with Generative Models\\n\\nConnecting retrieval systems with generative models requires careful orchestration of context-aware prompts, response validation, and latency optimization. Begin by templating prompts to explicitly bind retrieved context with user queries. For example: \\n\\n```python\\nprompt = f\"Context: {retrieved_docs}\\\\nQuery: {user_input}\\\\nAnswer:\"\\nresponse = llm(prompt)\\n``` \\n\\nThis ensures the model focuses on relevant information. \\n\\nNext, implement response filtering to validate outputs against the source documents. Use schema checks or semantic similarity measures to ensure alignment. For example: \\n\\n```python\\nif not validate_response(response, retrieved_docs):\\n    raise ValueError(\"Response does not match source documents\")\\n``` \\n\\nThese checks prevent hallucinations and ensure output fidelity. \\n\\nFor real-time applications, use streaming APIs to process responses incrementally, reducing latency while maintaining quality. Prioritize lightweight models and batch requests where feasible. \\n\\nThese steps ensure robust integration while maintaining performance and accuracy in production environments.\\n\\n## Production-Ready Considerations\\n\\nDesigning a scalable RAG system requires deliberate architectural choices to handle growth and ensure reliability. Horizontal scaling with microservices is critical, breaking the system into modular components like retrieval, generation, and routing layers. Each service should operate independently, allowing for targeted scaling based on workload. For example, the retrieval layer may require higher concurrency during peak traffic, while the generation layer might need more memory. Load balancers and auto-scaling groups should be configured to dynamically adjust resources, ensuring consistent performance under varying loads. The Azure OpenAI Realtime API’s architecture provides a reference for handling real-time data streams with distributed infrastructure.\\n\\nObservability is foundational for maintaining system health. Implement metrics for latency, error rates, and throughput across all components, using tools like Prometheus or Azure Monitor. Centralized logging with tools like ELK Stack or Azure Log Analytics enables quick debugging of failures. For production systems, ensure logs include contextual metadata (e.g., request IDs) to trace issues across services. Monitoring should also track model performance, such as the accuracy of retrieved documents or the coherence of generated responses, to detect drift or degradation over time.\\n\\nRollback strategies must be baked into the deployment pipeline. Use versioned model checkpoints and canary deployments to gradually roll out updates, allowing for quick reversion if issues arise. Automated rollback triggers based on metrics (e.g., rising error rates) can prevent cascading failures. Disaster recovery planning should include redundant data storage across regions and periodic backups of both raw data and model weights. For critical systems, consider geo-replication to ensure availability during outages. These practices align with principles outlined in real-time data integration challenges, emphasizing resilience in distributed systems.',\n",
       " 'image_specs': [{'placeholder': '[[IMAGE_1]]',\n",
       "   'filename': 'rag-hybrid-architecture.png',\n",
       "   'alt': 'Hybrid RAG Architecture Diagram',\n",
       "   'caption': 'This diagram illustrates the integration of retrieval and generation components in a RAG system, showing data flow from input to output with real-time updates.',\n",
       "   'prompt': 'A detailed architectural diagram of a Retrieval-Augmented Generation (RAG) system showing the integration of retrieval and generation components. The diagram should include: 1) User input interface, 2) Document database with vector store, 3) Retrieval engine with semantic search, 4) Generation model with prompt engineering, 5) Output interface. Arrows should show data flow from input to retrieval to generation to output, with real-time data injection from external sources.',\n",
       "   'size': '1024x1024',\n",
       "   'quality': 'medium'},\n",
       "  {'placeholder': '[[IMAGE_2]]',\n",
       "   'filename': 'rag-data-flow.png',\n",
       "   'alt': 'RAG Data Flow Diagram',\n",
       "   'caption': 'This flowchart demonstrates the end-to-end data processing pipeline in a RAG system, from indexing to real-time updates.',\n",
       "   'prompt': 'A flowchart showing the data processing pipeline of a RAG system: 1) Data ingestion stage with indexing into vector database, 2) Query processing with semantic search and ranking, 3) Generation stage with prompt engineering and model inference, 4) Real-time data injection from external sources. Include decision points for caching, fallback mechanisms, and error handling.',\n",
       "   'size': '1024x1024',\n",
       "   'quality': 'medium'},\n",
       "  {'placeholder': '[[IMAGE_3]]',\n",
       "   'filename': 'retrieval-comparison.png',\n",
       "   'alt': 'Vector vs Traditional Search Comparison',\n",
       "   'caption': 'This comparison diagram highlights the differences between vector databases and traditional search engines in data retrieval strategies.',\n",
       "   'prompt': 'A comparison diagram showing vector databases (FAISS/Pinecone) vs traditional search engines (Elasticsearch). Include: 1) Data storage methods (vector embeddings vs keyword indexes), 2) Query processing (semantic similarity vs keyword matching), 3) Use cases (contextual relevance vs exact matches), 4) Performance characteristics (latency vs accuracy trade-offs). Use side-by-side panels with icons representing each technology.',\n",
       "   'size': '1024x1024',\n",
       "   'quality': 'medium'}],\n",
       " 'final': '# Building a Complex, Production-Ready RAG System: A Hybrid Approach\\n\\n## Understanding RAG Fundamentals\\n\\nRetrieval-Augmented Generation (RAG) integrates retrieval systems with generative models to balance accuracy and flexibility. The architecture combines a retrieval component, which fetches relevant documents from a knowledge base, with a generative model that synthesizes context-aware responses. This hybrid approach mitigates the limitations of pure retrieval (which risks irrelevance) and pure generation (which may lack factual grounding). Azure OpenAI Realtime API plays a pivotal role in enabling hybrid workflows by allowing dynamic data fetching and real-time integration, as outlined in Microsoft Learn’s RAG documentation ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/rag)). \\n\\nThe tradeoffs between approaches depend on use cases: pure retrieval excels in speed and simplicity but sacrifices depth, while pure generation offers adaptability at the cost of potential hallucinations. RAG strikes a middle ground, though challenges like latency and data freshness persist. Microsoft’s research highlights real-time data integration complexities, emphasizing the need for robust pipeline design ([Source](https://www.microsoft.com/en-us/research/publication/real-time-data-integration-challenges/)). For production systems, prioritizing latency optimization and data consistency remains critical. The Realtime API’s capabilities for voice and chat applications further underscore its relevance in hybrid RAG implementations ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/realtime-api)).\\n\\n## Designing the Hybrid Architecture\\n\\nA production-ready RAG system hinges on a hybrid architecture that seamlessly integrates retrieval and generation pipelines, ensuring scalability, reliability, and real-time adaptability. The retrieval pipeline begins with **indexing**, where unstructured data is processed into a vector database or document store, enabling efficient querying. Azure OpenAI’s retrieval mechanisms, such as semantic search and hybrid search, allow for nuanced query matching while maintaining performance ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/rag)). **Querying** involves fetching the most relevant documents based on user input, with ranking algorithms prioritizing precision over recall to avoid overwhelming users with irrelevant results. **Ranking** is further optimized using Azure’s real-time feedback loops, which refine results dynamically based on user interactions.\\n\\nThe generation pipeline focuses on **prompt engineering** to structure inputs for the language model, balancing context length and specificity. Integration with Azure OpenAI’s models requires careful handling of token limits and response formatting, ensuring outputs are coherent and aligned with user intent. For real-time data injection, the **Azure Realtime API** enables streaming updates from external sources, such as chat transcripts or voice data, into the knowledge base. This ensures the system remains current without requiring full reindexing, though challenges like data latency and consistency must be mitigated through asynchronous processing and validation checks ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/realtime-api)). \\n\\nLatency and throughput are critical in hybrid systems. Retrieval operations must prioritize low-latency indexing and caching, while generation pipelines should scale horizontally using distributed computing frameworks. Azure’s Realtime API introduces additional latency, necessitating buffer zones and prioritization of critical queries. Throughput is managed by decoupling data ingestion from model inference, allowing the system to handle high-volume workloads without degrading performance. These considerations ensure the architecture remains robust, adaptable, and aligned with production requirements.\\n\\n## Implementing Data Retrieval Strategies  \\n\\nEfficient data retrieval is foundational to a production-ready RAG system, requiring careful trade-offs between speed, accuracy, and security. Vector databases like FAISS or Pinecone excel at semantic similarity searches, enabling efficient retrieval of contextually relevant documents, while traditional search engines (e.g., Elasticsearch) rely on keyword-based indexing and are better suited for structured queries. According to Microsoft Research, real-time integration challenges often arise when combining these approaches, necessitating hybrid architectures that balance latency and freshness ([Source](https://www.microsoft.com/en-us/research/publication/real-time-data-integration-challenges/)). \\n\\nCaching mechanisms are critical for frequent queries, reducing latency and database load. Implementing in-memory caches (e.g., Redis or Memcached) with time-to-live (TTL) settings ensures quick access to common results while avoiding stale data. For dynamic content, cache invalidation strategies—such as event-driven updates or periodic refreshes—can maintain freshness without sacrificing performance. \\n\\nData freshness and latency must be balanced in production systems. Real-time data integration, as discussed in Azure OpenAI’s Realtime API documentation, introduces complexities like network delays and synchronization overhead. A pragmatic approach might involve hybrid pipelines: using vector databases for static documents and real-time APIs for dynamic data, with fallback mechanisms to handle partial failures ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/realtime-api)). \\n\\nSecurity requires encryption at rest and in transit (e.g., TLS) and granular access controls via IAM policies. Sensitive data should be anonymized or tokenized before ingestion, and retrieval pipelines must audit access logs to detect anomalies. Microsoft’s Azure OpenAI documentation emphasizes secure data flow patterns, including private endpoints and VPC integration, to mitigate risks in enterprise deployments ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/rag)).\\n\\n## Integrating with Generative Models\\n\\nConnecting retrieval systems with generative models requires careful orchestration of context-aware prompts, response validation, and latency optimization. Begin by templating prompts to explicitly bind retrieved context with user queries. For example: \\n\\n```python\\nprompt = f\"Context: {retrieved_docs}\\\\nQuery: {user_input}\\\\nAnswer:\"\\nresponse = llm(prompt)\\n``` \\n\\nThis ensures the model focuses on relevant information. \\n\\nNext, implement response filtering to validate outputs against the source documents. Use schema checks or semantic similarity measures to ensure alignment. For example: \\n\\n```python\\nif not validate_response(response, retrieved_docs):\\n    raise ValueError(\"Response does not match source documents\")\\n``` \\n\\nThese checks prevent hallucinations and ensure output fidelity. \\n\\nFor real-time applications, use streaming APIs to process responses incrementally, reducing latency while maintaining quality. Prioritize lightweight models and batch requests where feasible. \\n\\nThese steps ensure robust integration while maintaining performance and accuracy in production environments.\\n\\n## Production-Ready Considerations\\n\\nDesigning a scalable RAG system requires deliberate architectural choices to handle growth and ensure reliability. Horizontal scaling with microservices is critical, breaking the system into modular components like retrieval, generation, and routing layers. Each service should operate independently, allowing for targeted scaling based on workload. For example, the retrieval layer may require higher concurrency during peak traffic, while the generation layer might need more memory. Load balancers and auto-scaling groups should be configured to dynamically adjust resources, ensuring consistent performance under varying loads. The Azure OpenAI Realtime API’s architecture provides a reference for handling real-time data streams with distributed infrastructure.\\n\\nObservability is foundational for maintaining system health. Implement metrics for latency, error rates, and throughput across all components, using tools like Prometheus or Azure Monitor. Centralized logging with tools like ELK Stack or Azure Log Analytics enables quick debugging of failures. For production systems, ensure logs include contextual metadata (e.g., request IDs) to trace issues across services. Monitoring should also track model performance, such as the accuracy of retrieved documents or the coherence of generated responses, to detect drift or degradation over time.\\n\\nRollback strategies must be baked into the deployment pipeline. Use versioned model checkpoints and canary deployments to gradually roll out updates, allowing for quick reversion if issues arise. Automated rollback triggers based on metrics (e.g., rising error rates) can prevent cascading failures. Disaster recovery planning should include redundant data storage across regions and periodic backups of both raw data and model weights. For critical systems, consider geo-replication to ensure availability during outages. These practices align with principles outlined in real-time data integration challenges, emphasizing resilience in distributed systems.'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n=== RUN generate_and_place_images ===\")\n",
    "\n",
    "patch = generate_and_place_images(state)\n",
    "\n",
    "for k, v in patch.items():\n",
    "    state[k] = v\n",
    "\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7212d2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Building a Complex, Production-Ready RAG System: A Hybrid Approach\n",
      "\n",
      "## Understanding RAG Fundamentals\n",
      "\n",
      "Retrieval-Augmented Generation (RAG) integrates retrieval systems with generative models to balance accuracy and flexibility. The architecture combines a retrieval component, which fetches relevant documents from a knowledge base, with a generative model that synthesizes context-aware responses. This hybrid approach mitigates the limitations of pure retrieval (which risks irrelevance) and pure generation (which may lack factual grounding). Azure OpenAI Realtime API plays a pivotal role in enabling hybrid workflows by allowing dynamic data fetching and real-time integration, as outlined in Microsoft Learn’s RAG documentation ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/rag)). \n",
      "\n",
      "The tradeoffs between approaches depend on use cases: pure retrieval excels in speed and simplicity but sacrifices depth, while pure generation offers adaptability at the cost of potential hallucinations. RAG strikes a middle ground, though challenges like latency and data freshness persist. Microsoft’s research highlights real-time data integration complexities, emphasizing the need for robust pipeline design ([Source](https://www.microsoft.com/en-us/research/publication/real-time-data-integration-challenges/)). For production systems, prioritizing latency optimization and data consistency remains critical. The Realtime API’s capabilities for voice and chat applications further underscore its relevance in hybrid RAG implementations ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/realtime-api)).\n",
      "\n",
      "## Designing the Hybrid Architecture\n",
      "\n",
      "A production-ready RAG system hinges on a hybrid architecture that seamlessly integrates retrieval and generation pipelines, ensuring scalability, reliability, and real-time adaptability. The retrieval pipeline begins with **indexing**, where unstructured data is processed into a vector database or document store, enabling efficient querying. Azure OpenAI’s retrieval mechanisms, such as semantic search and hybrid search, allow for nuanced query matching while maintaining performance ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/rag)). **Querying** involves fetching the most relevant documents based on user input, with ranking algorithms prioritizing precision over recall to avoid overwhelming users with irrelevant results. **Ranking** is further optimized using Azure’s real-time feedback loops, which refine results dynamically based on user interactions.\n",
      "\n",
      "The generation pipeline focuses on **prompt engineering** to structure inputs for the language model, balancing context length and specificity. Integration with Azure OpenAI’s models requires careful handling of token limits and response formatting, ensuring outputs are coherent and aligned with user intent. For real-time data injection, the **Azure Realtime API** enables streaming updates from external sources, such as chat transcripts or voice data, into the knowledge base. This ensures the system remains current without requiring full reindexing, though challenges like data latency and consistency must be mitigated through asynchronous processing and validation checks ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/realtime-api)). \n",
      "\n",
      "Latency and throughput are critical in hybrid systems. Retrieval operations must prioritize low-latency indexing and caching, while generation pipelines should scale horizontally using distributed computing frameworks. Azure’s Realtime API introduces additional latency, necessitating buffer zones and prioritization of critical queries. Throughput is managed by decoupling data ingestion from model inference, allowing the system to handle high-volume workloads without degrading performance. These considerations ensure the architecture remains robust, adaptable, and aligned with production requirements.\n",
      "\n",
      "## Implementing Data Retrieval Strategies  \n",
      "\n",
      "Efficient data retrieval is foundational to a production-ready RAG system, requiring careful trade-offs between speed, accuracy, and security. Vector databases like FAISS or Pinecone excel at semantic similarity searches, enabling efficient retrieval of contextually relevant documents, while traditional search engines (e.g., Elasticsearch) rely on keyword-based indexing and are better suited for structured queries. According to Microsoft Research, real-time integration challenges often arise when combining these approaches, necessitating hybrid architectures that balance latency and freshness ([Source](https://www.microsoft.com/en-us/research/publication/real-time-data-integration-challenges/)).  \n",
      "\n",
      "Caching mechanisms are critical for frequent queries, reducing latency and database load. Implementing in-memory caches (e.g., Redis or Memcached) with time-to-live (TTL) settings ensures quick access to common results while avoiding stale data. For dynamic content, cache invalidation strategies—such as event-driven updates or periodic refreshes—can maintain freshness without sacrificing performance.  \n",
      "\n",
      "Data freshness and latency must be balanced in production systems. Real-time data integration, as discussed in Azure OpenAI’s Realtime API documentation, introduces complexities like network delays and synchronization overhead. A pragmatic approach might involve hybrid pipelines: using vector databases for static documents and real-time APIs for dynamic data, with fallback mechanisms to handle partial failures ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/realtime-api)).  \n",
      "\n",
      "Security requires encryption at rest and in transit (e.g., TLS) and granular access controls via IAM policies. Sensitive data should be anonymized or tokenized before ingestion, and retrieval pipelines must audit access logs to detect anomalies. Microsoft’s Azure OpenAI documentation emphasizes secure data flow patterns, including private endpoints and VPC integration, to mitigate risks in enterprise deployments ([Source](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/rag)).\n",
      "\n",
      "## Integrating with Generative Models\n",
      "\n",
      "Connecting retrieval systems with generative models requires careful orchestration of context-aware prompts, response validation, and latency optimization. Begin by templating prompts to explicitly bind retrieved context with user queries. For example:  \n",
      "```python\n",
      "prompt = f\"Context: {retrieved_docs}\\nQuery: {user_input}\\nAnswer:\"\n",
      "response = llm(prompt)\n",
      "```  \n",
      "This ensures the model focuses on relevant information.  \n",
      "\n",
      "Next, implement response filtering to validate outputs against the source documents. Use schema checks or keyword matching to ensure responses align with retrieved data. For example:  \n",
      "```python\n",
      "def validate_response(response, query):\n",
      "    return \"source keyword\" in response and len(response.split()) > 5\n",
      "```  \n",
      "This prevents hallucination by enforcing consistency with the context.  \n",
      "\n",
      "For hallucination detection, integrate a secondary check using semantic similarity between the response and the context. If the similarity score falls below a threshold (e.g., 0.7), flag the response as unreliable.  \n",
      "\n",
      "To optimize real-time latency, leverage streaming APIs or model serving configurations (e.g., Azure OpenAI’s Realtime API) to minimize inference delays. Prioritize lightweight models and batch requests where feasible.  \n",
      "\n",
      "These steps ensure robust integration while maintaining performance and accuracy in production environments.\n",
      "\n",
      "## Production-Ready Considerations\n",
      "\n",
      "Designing a scalable RAG system requires deliberate architectural choices to handle growth and ensure reliability. Horizontal scaling with microservices is critical, breaking the system into modular components like retrieval, generation, and routing layers. Each service should operate independently, allowing for targeted scaling based on workload. For example, the retrieval layer may require higher concurrency during peak traffic, while the generation layer might need more memory. Load balancers and auto-scaling groups should be configured to dynamically adjust resources, ensuring consistent performance under varying loads. The Azure OpenAI Realtime API’s architecture provides a reference for handling real-time data streams with distributed infrastructure.\n",
      "\n",
      "Observability is foundational for maintaining system health. Implement metrics for latency, error rates, and throughput across all components, using tools like Prometheus or Azure Monitor. Centralized logging with tools like ELK Stack or Azure Log Analytics enables quick debugging of failures. For production systems, ensure logs include contextual metadata (e.g., request IDs) to trace issues across services. Monitoring should also track model performance, such as the accuracy of retrieved documents or the coherence of generated responses, to detect drift or degradation over time.\n",
      "\n",
      "Rollback strategies must be baked into the deployment pipeline. Use versioned model checkpoints and canary deployments to gradually roll out updates, allowing for quick reversion if issues arise. Automated rollback triggers based on metrics (e.g., rising error rates) can prevent cascading failures. Disaster recovery planning should include redundant data storage across regions and periodic backups of both raw data and model weights. For critical systems, consider geo-replication to ensure availability during outages. These practices align with principles outlined in real-time data integration challenges, emphasizing resilience in distributed systems.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(state[\"merged_md\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "61350cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'Self Attention in Transformer Architecture',\n",
       " 'mode': 'closed_book',\n",
       " 'needs_research': False,\n",
       " 'queries': [],\n",
       " 'evidence': [],\n",
       " 'plan': Plan(blog_title='Mastering Self-Attention in Transformer Models', audience='AI developers and machine learning practitioners', tone='Educational and engaging', blog_kind='explainer', constraints=[], tasks=[Task(id=1, title='Introduction to Transformers', goal='Understand the origins of the Transformer architecture and the fundamental role of self-attention in addressing long-range dependencies.', bullets=['The Transformer architecture was developed to handle sequential data without recurrent structures.', 'Previous models like RNNs and LSTMs struggled with long-range dependencies and parallelization.', 'Self-attention allows every element in a sequence to attend to all other elements, enabling contextual understanding.', \"This mechanism is key to the Transformer's efficiency and effectiveness in tasks like language modeling and translation.\"], target_words=250, tags=['transformer', 'introduction', 'context'], requires_research=False, requires_citations=False, requires_code=False), Task(id=2, title='What is Self-Attention?', goal='Define the self-attention mechanism and explain how it enables models to dynamically weigh the importance of different elements in a sequence.', bullets=['Self-attention is a mechanism that allows a model to focus on relevant parts of the input when processing each element.', 'It involves three components: queries, keys, and values, which are derived from the input embeddings.', 'Attention scores are computed using dot products between queries and keys, then normalized with softmax.', 'These scores determine how much weight each element should have when generating the output for a given position.'], target_words=300, tags=['self-attention', 'mechanism', 'attention-mechanism'], requires_research=False, requires_citations=False, requires_code=False), Task(id=3, title='How Self-Attention Works', goal='Detail the computational steps involved in the self-attention process, including matrix operations and scaling.', bullets=['The process starts with input embeddings, which are passed through linear transformations to generate query, key, and value vectors.', 'Attention scores are calculated by taking the dot product of queries and keys, then scaling by the square root of the dimension to prevent large values.', 'Softmax function normalizes the scores into probabilities that sum to one, representing attention weights.', 'The output for each position is a weighted sum of the value vectors, using the attention weights as coefficients.', 'Multi-head attention is often used to capture different representations by concatenating multiple attention heads.'], target_words=400, tags=['how-it-works', 'computation', 'transformer-architecture'], requires_research=False, requires_citations=False, requires_code=False), Task(id=4, title='Advantages of Self-Attention', goal='Recognize the key benefits that self-attention provides, such as parallel processing and improved contextual understanding.', bullets=['Self-attention enables parallel computation, unlike sequential models, leading to faster training and inference.', 'It captures long-range dependencies effectively, allowing models to understand relationships between distant elements in a sequence.', 'The dynamic context provided by attention mechanisms improves performance on tasks requiring deep understanding, such as language translation and text generation.', 'Self-attention contributes to state-of-the-art results in various natural language processing tasks.'], target_words=250, tags=['advantages', 'benefits', 'performance'], requires_research=False, requires_citations=False, requires_code=False), Task(id=5, title='Limitations and Challenges', goal='Identify the drawbacks of self-attention, including computational complexity and attention sparsity.', bullets=['Self-attention has a quadratic time complexity relative to sequence length, making it computationally expensive for long sequences.', 'Attention mechanisms can sometimes attend to irrelevant or sparse information, potentially reducing efficiency.', 'Memory usage can become prohibitive for very long sequences, limiting applications in certain domains.', 'Researchers have developed techniques like sparse attention and local attention to mitigate these issues.'], target_words=300, tags=['limitations', 'challenges', 'complexity'], requires_research=False, requires_citations=False, requires_code=False), Task(id=6, title='Applications and Examples', goal='Explore real-world applications where self-attention mechanisms are utilized, such as in NLP and multimodal tasks.', bullets=['Self-attention is at the core of models like BERT and GPT, revolutionizing natural language processing tasks.', 'It enables effective handling of long documents and diverse text generation applications.', 'Extensions to self-attention, like in Vision Transformers (ViT), have been applied to computer vision for image recognition.', 'Attention mechanisms are also used in recommendation systems and dialogue systems for better user engagement.'], target_words=250, tags=['applications', 'examples', 'use-cases'], requires_research=False, requires_citations=False, requires_code=False), Task(id=7, title='Conclusion and Future Directions', goal='Summarize the impact of self-attention and discuss potential advancements in attention mechanisms.', bullets=['Self-attention has fundamentally changed how models process sequential data, leading to significant improvements in AI capabilities.', 'Multi-head attention and variants like self-attention with relative positions continue to enhance model performance.', 'Future research may focus on more efficient attention mechanisms, interpretability, and integration with other architectures.', 'As AI evolves, self-attention mechanisms will likely play a crucial role in advancing language models and beyond.'], target_words=250, tags=['conclusion', 'future', 'impact'], requires_research=False, requires_citations=False, requires_code=False)]),\n",
       " 'as_of': None,\n",
       " 'recency_days': 7,\n",
       " 'sections': [(1,\n",
       "   \"## Introduction to Transformers\\n\\nThe Transformer architecture, introduced in a 2017 paper by Google researchers, revolutionized sequence modeling by eliminating reliance on recurrent structures like RNNs and LSTMs. These earlier models faced challenges with long-range dependencies due to vanishing gradients and sequential processing limitations, which hindered their scalability and parallelization capabilities. Transformers address these issues by leveraging self-attention mechanisms, enabling every element in a sequence to interact with all others simultaneously. This attention-based approach allows models to dynamically weigh contextual relationships, capturing dependencies across arbitrary distances without explicit recurrence. \\n\\nSelf-attention operates by computing attention scores between input elements via query, key, and value vectors, creating a weighted representation that encodes global context. This design eliminates the need for sequential processing, drastically improving training efficiency and enabling handling of longer sequences. The mechanism underpins the Transformer's success in tasks like machine translation and language modeling, where understanding complex contextual relationships is critical. By prioritizing parallel computation and explicit attention modeling, Transformers set a new standard for sequence modeling, paving the way for advancements in natural language processing and beyond.\"),\n",
       "  (2,\n",
       "   '## What is Self-Attention?\\n\\nSelf-attention is a core mechanism in Transformer models that enables the network to dynamically prioritize relevant parts of an input sequence when processing each element. Unlike traditional sequential models, which rely on fixed-order dependencies, self-attention allows the model to weigh the importance of different positions based on their contextual relationships. This flexibility is critical for tasks like language understanding, where long-range dependencies and semantic nuances are essential.\\n\\nThe mechanism operates through three key components: **queries**, **keys**, and **values**, all derived from the input embeddings. For each position in the sequence, the model computes a query vector that represents the current element’s context, key vectors that encode information about other elements, and value vectors that hold the actual content to be aggregated. These components work together to determine how much attention each element should receive.\\n\\nAttention scores are calculated by taking the dot product of queries and keys, which measures the similarity between elements. These raw scores are then normalized using the softmax function to produce probabilities that sum to one, ensuring the model focuses on the most relevant parts. Finally, the normalized scores are used to compute a weighted sum of the value vectors, generating an output representation that encapsulates the contextual significance of each element.\\n\\nThis dynamic weighting allows Transformers to capture complex patterns in data, such as syntactic relationships in sentences or dependencies across long sequences. By iteratively refining attention weights during training, the model learns to adaptively prioritize information, making self-attention a cornerstone of modern natural language processing and beyond.'),\n",
       "  (3,\n",
       "   '## How Self-Attention Works\\n\\nThe self-attention mechanism in Transformers operates through a sequence of matrix operations that enable the model to weigh the importance of different input elements dynamically. Starting with input embeddings, each token is transformed into three vectors: **query (Q)**, **key (K)**, and **value (V)** via separate linear transformations. These transformations involve multiplying the input matrix by weight matrices $ W_Q $, $ W_K $, and $ W_V $, resulting in matrices of shape $ (seq\\\\_len, d_{model}) $, where $ d_{model} $ is the embedding dimension.  \\n\\nAttention scores are computed by taking the dot product of queries and keys, producing a matrix of shape $ (seq\\\\_len, seq\\\\_len) $. To stabilize gradients and prevent numerical instability, these scores are scaled by dividing by $ \\\\sqrt{d_k} $, where $ d_k $ is the dimension of the key vectors. This scaling ensures the values remain manageable during training.  \\n\\nThe softmax function is then applied row-wise to the scaled scores, converting them into probabilities that sum to one. These probabilities, or **attention weights**, represent the importance of each token relative to the current position. Finally, the output for each position is generated by multiplying the attention weights with the value vectors and summing the results, effectively aggregating relevant contextual information.  \\n\\nMulti-head attention extends this process by splitting the queries, keys, and values into multiple heads, each operating independently. The outputs from these heads are concatenated and projected back to the original dimension, allowing the model to capture diverse relationships within the input. This mechanism enables Transformers to handle long-range dependencies and contextual nuances effectively.'),\n",
       "  (4,\n",
       "   '## Advantages of Self-Attention\\n\\nSelf-attention mechanisms revolutionize how Transformer models process sequential data by enabling parallel computation, a critical departure from sequential models like RNNs. Unlike recurrent architectures that process tokens step-by-step, self-attention computes relationships between all pairs of positions simultaneously. This parallelism drastically accelerates training and inference speeds, making it feasible to handle long sequences efficiently.  \\n\\nA key strength of self-attention is its ability to capture long-range dependencies. By calculating attention weights that reflect the relevance of any two tokens in the sequence, the mechanism allows models to dynamically prioritize distant elements. This is particularly valuable for tasks like language translation, where understanding context across sentence boundaries is essential, or text generation, where maintaining coherence over extended spans is critical.  \\n\\nThe dynamic context provided by attention weights enhances model performance on complex tasks. For example, in translation, the model can focus on relevant parts of the source sentence while generating the target, improving accuracy. Similarly, in text generation, attention enables the model to retain contextual information from earlier parts of the sequence, leading to more coherent outputs.  \\n\\nThese properties collectively underpin the success of self-attention in achieving state-of-the-art results across natural language processing tasks. By combining parallel computation with flexible context modeling, self-attention addresses limitations of earlier architectures while enabling scalable, high-performance language understanding.'),\n",
       "  (5,\n",
       "   '## Limitations and Challenges  \\n\\nSelf-attention, while foundational to Transformer models, introduces several limitations that hinder scalability and efficiency. One of the most significant challenges is its **quadratic time complexity** relative to the sequence length. For a sequence of length *n*, the self-attention mechanism requires *O(n²)* operations to compute attention scores, as each token must attend to every other token. This becomes computationally prohibitive for long sequences, such as those found in video processing or document analysis, where *n* can easily exceed thousands. The quadratic scaling limits the model’s ability to handle real-time or large-scale applications without significant optimization.  \\n\\nAnother challenge is **attention sparsity**. While self-attention theoretically allows tokens to interact globally, in practice, models may attend to irrelevant or sparse information, leading to inefficient computation. For example, in tasks like machine translation, certain tokens may not meaningfully influence others, yet the mechanism still processes them, wasting resources. This redundancy can degrade performance in scenarios where the input contains noise or extraneous details.  \\n\\n**Memory usage** also escalates with sequence length, as storing attention matrices and intermediate activations requires substantial memory. For very long sequences, this can exceed the capacity of standard hardware, restricting deployment in domains like genomics or log analysis, where sequences are inherently long.  \\n\\nTo address these issues, researchers have proposed techniques like **sparse attention** (e.g., Linformer, Reformer) and **local attention** (e.g., Longformer), which reduce computational and memory overhead by limiting token interactions to subsets or neighborhoods. These approaches aim to balance efficiency with the ability to capture long-range dependencies, but they often introduce trade-offs in model design and training dynamics. Understanding these limitations is critical for selecting appropriate architectures and optimizations for specific use cases.'),\n",
       "  (6,\n",
       "   '## Applications and Examples  \\nSelf-attention is at the core of models like BERT and GPT, revolutionizing natural language processing tasks by enabling dynamic contextual understanding. These models leverage self-attention to capture dependencies between words, allowing for tasks like text classification, machine translation, and question answering with improved accuracy. The mechanism’s ability to focus on relevant parts of input sequences makes it ideal for handling long documents, where traditional methods struggle with context retention. This capability has driven advancements in text generation applications, such as chatbots and content creation tools, by enabling coherent and context-aware outputs.  \\n\\nExtensions to self-attention, like in Vision Transformers (ViT), have expanded its use to computer vision. ViT applies self-attention to image patches, enabling the model to recognize patterns across spatial dimensions without relying on convolutional layers. This has opened new possibilities in image recognition and segmentation. Beyond NLP and vision, attention mechanisms are also pivotal in recommendation systems, where they model user-item interactions by prioritizing relevant preferences. In dialogue systems, self-attention helps maintain context across turns, improving the fluency and coherence of conversational agents. These applications highlight the versatility of self-attention in addressing complex, real-world problems across domains.'),\n",
       "  (7,\n",
       "   '## Conclusion and Future Directions  \\nSelf-attention has revolutionized how models process sequential data, enabling breakthroughs in natural language understanding, vision, and reasoning tasks. By allowing elements to dynamically weigh their relationships, it replaces traditional recurrence with parallelizable operations, drastically improving efficiency and scalability. Variants like multi-head attention and position-encoding strategies have further refined model capabilities, enabling robust handling of long-range dependencies and diverse input structures.  \\n\\nLooking ahead, research may prioritize optimizing attention mechanisms for resource constraints, such as sparse attention or local attention, to reduce computational overhead without sacrificing performance. Enhancing interpretability—understanding how attention weights form meaning—could unlock better alignment between models and human intent. Additionally, integrating self-attention with architectures like graph neural networks or reinforcement learning frameworks may expand its applicability to complex, non-sequential tasks.  \\n\\nAs AI systems grow more sophisticated, self-attention will remain central to advancing language models, vision transformers, and multimodal systems. Its adaptability and mathematical elegance position it as a cornerstone for next-generation models, driving innovation across domains while addressing challenges like energy efficiency and ethical alignment. The journey of self-attention is far from over; its evolution will continue to shape the trajectory of artificial intelligence.')],\n",
       " 'merged_md': \"# Mastering Self-Attention in Transformer Models\\n\\n## Introduction to Transformers\\n\\nThe Transformer architecture, introduced in a 2017 paper by Google researchers, revolutionized sequence modeling by eliminating reliance on recurrent structures like RNNs and LSTMs. These earlier models faced challenges with long-range dependencies due to vanishing gradients and sequential processing limitations, which hindered their scalability and parallelization capabilities. Transformers address these issues by leveraging self-attention mechanisms, enabling every element in a sequence to interact with all others simultaneously. This attention-based approach allows models to dynamically weigh contextual relationships, capturing dependencies across arbitrary distances without explicit recurrence. \\n\\nSelf-attention operates by computing attention scores between input elements via query, key, and value vectors, creating a weighted representation that encodes global context. This design eliminates the need for sequential processing, drastically improving training efficiency and enabling handling of longer sequences. The mechanism underpins the Transformer's success in tasks like machine translation and language modeling, where understanding complex contextual relationships is critical. By prioritizing parallel computation and explicit attention modeling, Transformers set a new standard for sequence modeling, paving the way for advancements in natural language processing and beyond.\\n\\n## What is Self-Attention?\\n\\nSelf-attention is a core mechanism in Transformer models that enables the network to dynamically prioritize relevant parts of an input sequence when processing each element. Unlike traditional sequential models, which rely on fixed-order dependencies, self-attention allows the model to weigh the importance of different positions based on their contextual relationships. This flexibility is critical for tasks like language understanding, where long-range dependencies and semantic nuances are essential.\\n\\nThe mechanism operates through three key components: **queries**, **keys**, and **values**, all derived from the input embeddings. For each position in the sequence, the model computes a query vector that represents the current element’s context, key vectors that encode information about other elements, and value vectors that hold the actual content to be aggregated. These components work together to determine how much attention each element should receive.\\n\\nAttention scores are calculated by taking the dot product of queries and keys, which measures the similarity between elements. These raw scores are then normalized using the softmax function to produce probabilities that sum to one, ensuring the model focuses on the most relevant parts. Finally, the normalized scores are used to compute a weighted sum of the value vectors, generating an output representation that encapsulates the contextual significance of each element.\\n\\nThis dynamic weighting allows Transformers to capture complex patterns in data, such as syntactic relationships in sentences or dependencies across long sequences. By iteratively refining attention weights during training, the model learns to adaptively prioritize information, making self-attention a cornerstone of modern natural language processing and beyond.\\n\\n## How Self-Attention Works\\n\\nThe self-attention mechanism in Transformers operates through a sequence of matrix operations that enable the model to weigh the importance of different input elements dynamically. Starting with input embeddings, each token is transformed into three vectors: **query (Q)**, **key (K)**, and **value (V)** via separate linear transformations. These transformations involve multiplying the input matrix by weight matrices $ W_Q $, $ W_K $, and $ W_V $, resulting in matrices of shape $ (seq\\\\_len, d_{model}) $, where $ d_{model} $ is the embedding dimension.  \\n\\nAttention scores are computed by taking the dot product of queries and keys, producing a matrix of shape $ (seq\\\\_len, seq\\\\_len) $. To stabilize gradients and prevent numerical instability, these scores are scaled by dividing by $ \\\\sqrt{d_k} $, where $ d_k $ is the dimension of the key vectors. This scaling ensures the values remain manageable during training.  \\n\\nThe softmax function is then applied row-wise to the scaled scores, converting them into probabilities that sum to one. These probabilities, or **attention weights**, represent the importance of each token relative to the current position. Finally, the output for each position is generated by multiplying the attention weights with the value vectors and summing the results, effectively aggregating relevant contextual information.  \\n\\nMulti-head attention extends this process by splitting the queries, keys, and values into multiple heads, each operating independently. The outputs from these heads are concatenated and projected back to the original dimension, allowing the model to capture diverse relationships within the input. This mechanism enables Transformers to handle long-range dependencies and contextual nuances effectively.\\n\\n## Advantages of Self-Attention\\n\\nSelf-attention mechanisms revolutionize how Transformer models process sequential data by enabling parallel computation, a critical departure from sequential models like RNNs. Unlike recurrent architectures that process tokens step-by-step, self-attention computes relationships between all pairs of positions simultaneously. This parallelism drastically accelerates training and inference speeds, making it feasible to handle long sequences efficiently.  \\n\\nA key strength of self-attention is its ability to capture long-range dependencies. By calculating attention weights that reflect the relevance of any two tokens in the sequence, the mechanism allows models to dynamically prioritize distant elements. This is particularly valuable for tasks like language translation, where understanding context across sentence boundaries is essential, or text generation, where maintaining coherence over extended spans is critical.  \\n\\nThe dynamic context provided by attention weights enhances model performance on complex tasks. For example, in translation, the model can focus on relevant parts of the source sentence while generating the target, improving accuracy. Similarly, in text generation, attention enables the model to retain contextual information from earlier parts of the sequence, leading to more coherent outputs.  \\n\\nThese properties collectively underpin the success of self-attention in achieving state-of-the-art results across natural language processing tasks. By combining parallel computation with flexible context modeling, self-attention addresses limitations of earlier architectures while enabling scalable, high-performance language understanding.\\n\\n## Limitations and Challenges  \\n\\nSelf-attention, while foundational to Transformer models, introduces several limitations that hinder scalability and efficiency. One of the most significant challenges is its **quadratic time complexity** relative to the sequence length. For a sequence of length *n*, the self-attention mechanism requires *O(n²)* operations to compute attention scores, as each token must attend to every other token. This becomes computationally prohibitive for long sequences, such as those found in video processing or document analysis, where *n* can easily exceed thousands. The quadratic scaling limits the model’s ability to handle real-time or large-scale applications without significant optimization.  \\n\\nAnother challenge is **attention sparsity**. While self-attention theoretically allows tokens to interact globally, in practice, models may attend to irrelevant or sparse information, leading to inefficient computation. For example, in tasks like machine translation, certain tokens may not meaningfully influence others, yet the mechanism still processes them, wasting resources. This redundancy can degrade performance in scenarios where the input contains noise or extraneous details.  \\n\\n**Memory usage** also escalates with sequence length, as storing attention matrices and intermediate activations requires substantial memory. For very long sequences, this can exceed the capacity of standard hardware, restricting deployment in domains like genomics or log analysis, where sequences are inherently long.  \\n\\nTo address these issues, researchers have proposed techniques like **sparse attention** (e.g., Linformer, Reformer) and **local attention** (e.g., Longformer), which reduce computational and memory overhead by limiting token interactions to subsets or neighborhoods. These approaches aim to balance efficiency with the ability to capture long-range dependencies, but they often introduce trade-offs in model design and training dynamics. Understanding these limitations is critical for selecting appropriate architectures and optimizations for specific use cases.\\n\\n## Applications and Examples  \\nSelf-attention is at the core of models like BERT and GPT, revolutionizing natural language processing tasks by enabling dynamic contextual understanding. These models leverage self-attention to capture dependencies between words, allowing for tasks like text classification, machine translation, and question answering with improved accuracy. The mechanism’s ability to focus on relevant parts of input sequences makes it ideal for handling long documents, where traditional methods struggle with context retention. This capability has driven advancements in text generation applications, such as chatbots and content creation tools, by enabling coherent and context-aware outputs.  \\n\\nExtensions to self-attention, like in Vision Transformers (ViT), have expanded its use to computer vision. ViT applies self-attention to image patches, enabling the model to recognize patterns across spatial dimensions without relying on convolutional layers. This has opened new possibilities in image recognition and segmentation. Beyond NLP and vision, attention mechanisms are also pivotal in recommendation systems, where they model user-item interactions by prioritizing relevant preferences. In dialogue systems, self-attention helps maintain context across turns, improving the fluency and coherence of conversational agents. These applications highlight the versatility of self-attention in addressing complex, real-world problems across domains.\\n\\n## Conclusion and Future Directions  \\nSelf-attention has revolutionized how models process sequential data, enabling breakthroughs in natural language understanding, vision, and reasoning tasks. By allowing elements to dynamically weigh their relationships, it replaces traditional recurrence with parallelizable operations, drastically improving efficiency and scalability. Variants like multi-head attention and position-encoding strategies have further refined model capabilities, enabling robust handling of long-range dependencies and diverse input structures.  \\n\\nLooking ahead, research may prioritize optimizing attention mechanisms for resource constraints, such as sparse attention or local attention, to reduce computational overhead without sacrificing performance. Enhancing interpretability—understanding how attention weights form meaning—could unlock better alignment between models and human intent. Additionally, integrating self-attention with architectures like graph neural networks or reinforcement learning frameworks may expand its applicability to complex, non-sequential tasks.  \\n\\nAs AI systems grow more sophisticated, self-attention will remain central to advancing language models, vision transformers, and multimodal systems. Its adaptability and mathematical elegance position it as a cornerstone for next-generation models, driving innovation across domains while addressing challenges like energy efficiency and ethical alignment. The journey of self-attention is far from over; its evolution will continue to shape the trajectory of artificial intelligence.\\n\",\n",
       " 'md_with_placeholders': '## Self Attention in Transformer Architecture\\n\\nTransformers have revolutionized the field of natural language processing (NLP) since their introduction in the paper \"Attention is All You Need\" by Vaswani et al. in 2017. Unlike traditional recurrent neural networks (RNNs) or long short-term memory (LSTM) networks that process sequences sequentially, Transformers rely entirely on **self-attention** mechanisms to weigh the importance of different words in a sentence or sequence. This allows Transformers to handle long-range dependencies effectively and enables parallel computation during training, making them highly efficient and scalable.\\n\\n### What is Self-Attention?\\n\\nSelf-attention, also known as intra-attention, allows different positions within a single piece of input to attend to all positions in the input sequence. The mechanism operates through three key components: **queries**, **keys**, and **values**, all derived from the input embeddings. [[IMAGE_1]]\\n\\nSpecifically, for each element in the input sequence, the model generates three vectors: a query vector (representing what the element is looking for), a key vector (used to measure relevance), and a value vector (the content to be used if attention is paid). The attention score for each pair of elements is computed by taking the dot product of the query of one element and the key of another, scaled by the square root of the dimension, and then applying a softmax function to obtain a probability distribution. These scores are then used to weight the value vectors, and the weighted sum forms the output representation for each position.\\n\\n### How Self-attention Works\\n\\nThe self-attention mechanism can be broken down into several steps:\\n\\n1. **Input Embeddings**: The input sequence is first converted into embeddings, which are dense vector representations.\\n2. **Linear Transformations**: Three linear transformations (using different weight matrices) are applied to the input embeddings to produce the query, key, and value matrices.\\n3. **Dot Product**: The dot product between the query matrix and the key matrix is computed, resulting in a matrix of scores.\\n4. **Scaling**: The scores are scaled by the square root of the key dimension to stabilize the model.\\n5. **Softmax**: A softmax function is applied to the scaled scores to obtain attention weights, ensuring they sum to 1 and represent probabilities.\\n6. **Weighted Sum**: The value matrix is multiplied by the attention weights to produce a weighted sum, which is the output of the self-attention layer for each position.\\n\\n[[IMAGE_2]]\\n\\nThis process allows each position in the sequence to attend to all positions, capturing contextual relationships effectively.\\n\\n### Advantages of Self-Attention\\n\\nSelf-attention mechanisms offer several advantages over traditional sequence models:\\n\\n- **Parallel Computation**: Unlike RNNs, which must process elements sequentially, self-attention allows all elements to be processed simultaneously, significantly speeding up training and inference.\\n- **Long-Range Dependencies**: Self-attention can capture dependencies between elements regardless of their distance in the sequence, whereas RNNs often struggle with long-range dependencies due to vanishing gradient problems.\\n- **Flexibility**: The attention mechanism can be applied to any sequence, not just text, making it versatile for various tasks.\\n\\n[[IMAGE_3]]\\n\\n### Limitations of Self-Attention\\n\\nDespite their advantages, self-attention mechanisms have some limitations:\\n\\n- **Quadratic Time Complexity**: The self-attention mechanism has a time complexity of O(n^2), where n is the sequence length. This can become computationally expensive for very long sequences, as the number of operations grows quadratically.\\n- **Sparsity**: While self-attention theoretically considers all elements, in practice, attention scores can be sparse, meaning only a few elements are attended to, which might not always capture the full context.\\n- **Memory Usage**: Storing and computing attention scores for long sequences can lead to high memory consumption.\\n\\n### Applications of Self-Attention\\n\\nSelf-attention has found applications beyond NLP, including computer vision, speech recognition, and recommendation systems. Notable models leveraging self-attention include:\\n\\n- **BERT (Bidirectional Encoder Representations from Transformers)**: A model that uses masked self-attention to pre-train deep language models.\\n- **GPT (Generative Pre-trained Transformer)**: A model that uses causal self-attention for autoregressive text generation.\\n- **Vision Transformers (ViT)**: Adaptation of Transformers for image classification tasks, where self-attention is applied to patches of images.\\n\\n### Conclusion and Future Directions\\n\\nSelf-attention is a cornerstone of modern Transformer architectures and has fundamentally changed how we process sequential data. While it has limitations, ongoing research continues to address issues like computational efficiency (e.g., sparse attention, linear attention models) and interpretability. As hardware advances and models become more sophisticated, self-attention mechanisms will likely continue to evolve, enabling even more powerful AI systems.\\n\\n### References\\n\\n[1] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. In Advances in neural information processing systems (NIPS), pp. 10041-10050.\\n\\n[2] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.\\n\\n[3] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Gpt-2. OpenAI.\\n\\n[4] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, M., ... & Houlsby, N. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.\\n\\n## About the Author\\n\\n[Your Name] is a machine learning enthusiast with a passion for explaining complex AI concepts. This blog post is part of a series on Transformer architectures.\\n\\n## License\\n\\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.',\n",
       " 'image_specs': [{'placeholder': '[[IMAGE_1]]',\n",
       "   'filename': 'self_attention_components.png',\n",
       "   'alt': 'Self-attention components diagram',\n",
       "   'caption': 'Components of Self-attention: Query, Key, and Value Vectors',\n",
       "   'prompt': 'Create a technical diagram illustrating the query, key, and value vectors in the self-attention mechanism of Transformers. Show how these vectors are derived from input embeddings and how attention scores are computed.',\n",
       "   'size': '1024x1024',\n",
       "   'quality': 'medium'},\n",
       "  {'placeholder': '[[IMAGE_2]]',\n",
       "   'filename': 'self_attention_flow.png',\n",
       "   'alt': 'Self-attention flow diagram',\n",
       "   'caption': 'Step-by-step process of self-attention computation',\n",
       "   'prompt': 'Generate a flowchart showing the step-by-step process of self-attention in Transformers, including input embeddings, linear transformations, dot product, scaling, softmax, and weighted sum.',\n",
       "   'size': '1024x1024',\n",
       "   'quality': 'medium'},\n",
       "  {'placeholder': '[[IMAGE_3]]',\n",
       "   'filename': 'self_attention_advantages.png',\n",
       "   'alt': 'Self-attention advantages diagram',\n",
       "   'caption': 'Comparison highlighting advantages of self-attention over traditional RNNs',\n",
       "   'prompt': 'Design a diagram comparing self-attention to traditional RNNs, emphasizing parallel computation and long-range dependency handling.',\n",
       "   'size': '1024x1024',\n",
       "   'quality': 'medium'}],\n",
       " 'final': ''}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
