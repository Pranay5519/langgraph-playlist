{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "090392b0",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c7af781",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from langchain_core.output_parsers import JsonOutputParser , PydanticOutputParser\n",
    "import operator\n",
    "from pathlib import Path\n",
    "from typing import TypedDict, List, Annotated, Literal\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import Send\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import SystemMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "151789ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task(BaseModel):\n",
    "    id: int\n",
    "    title: str\n",
    "\n",
    "    goal: str = Field(\n",
    "        ...,\n",
    "        description=\"One sentence describing what the reader should be able to do/understand after this section.\",\n",
    "    )\n",
    "    bullets: List[str] = Field(\n",
    "        ...,\n",
    "        min_length=3,\n",
    "        max_length=5,\n",
    "        description=\"3–5 concrete, non-overlapping subpoints to cover in this section.\",\n",
    "    )\n",
    "    target_words: int = Field(\n",
    "        ...,\n",
    "        description=\"Target word count for this section (120–450).\",\n",
    "    )\n",
    "    section_type: Literal[\n",
    "        \"intro\", \"core\", \"examples\", \"checklist\", \"common_mistakes\", \"conclusion\"\n",
    "    ] = Field(\n",
    "        ...,\n",
    "        description=\"Use 'common_mistakes' exactly once in the plan.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "431ad10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plan(BaseModel):\n",
    "    blog_title: str\n",
    "    audience: str = Field(..., description=\"Who this blog is for.\")\n",
    "    tone: str = Field(..., description=\"Writing tone (e.g., practical, crisp).\")\n",
    "    tasks: List[Task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5972a764",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    plan: Plan\n",
    "    sections: Annotated[List[str], operator.add]  # reducer concatenates worker outputs\n",
    "    final: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfab707d",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_llm = ChatOllama(\n",
    "    model=\"deepseek-r1:latest\",\n",
    "    format=\"json\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# LLM for content generation (without JSON mode)\n",
    "content_llm = ChatOllama(\n",
    "    model=\"llama3.2:latest\",\n",
    "    temperature=0.7\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c25ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def orchestrator(state: State) -> dict:\n",
    "    \"\"\"Creates a blog outline with sections\"\"\"\n",
    "    \n",
    "    # Setup parser for Plan schema\n",
    "    parser = PydanticOutputParser(pydantic_object=Plan)\n",
    "    \n",
    "    # Create prompt\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"\n",
    "         \"You are a senior technical writer and developer advocate. Your job is to produce a \"\n",
    "        \"highly actionable outline for a technical blog post.\\n\\n\"\n",
    "        \"Hard requirements:\\n\"\n",
    "        \"- Create 2-4 sections (tasks) that fit a technical blog.\\n\"\n",
    "        \"- Each section must include:\\n\"\n",
    "        \"  1) goal (1 sentence: what the reader can do/understand after the section)\\n\"\n",
    "        \"  2) 3–5 bullets that are concrete, specific, and non-overlapping\\n\"\n",
    "        \"  3) target word count (120–450)\\n\"\n",
    "        \"- Include EXACTLY ONE section with section_type='common_mistakes'.\\n\\n\"\n",
    "        \"Make it technical (not generic):\\n\"\n",
    "        \"- Assume the reader is a developer; use correct terminology.\\n\"\n",
    "        \"- Prefer design/engineering structure: problem → intuition → approach → implementation → \"\n",
    "        \"trade-offs → testing/observability → conclusion.\\n\"\n",
    "        \"- Bullets must be actionable and testable (e.g., 'Show a minimal code snippet for X', \"\n",
    "        \"'Explain why Y fails under Z condition', 'Add a checklist for production readiness').\\n\"\n",
    "        \"- Explicitly include at least ONE of the following somewhere in the plan (as bullets):\\n\"\n",
    "        \"  * a minimal working example (MWE) or code sketch\\n\"\n",
    "        \"  * edge cases / failure modes\\n\"\n",
    "        \"  * performance/cost considerations\\n\"\n",
    "        \"  * security/privacy considerations (if relevant)\\n\"\n",
    "        \"  * debugging tips / observability (logs, metrics, traces)\\n\"\n",
    "        \"- Avoid vague bullets like 'Explain X' or 'Discuss Y'. Every bullet should state what \"\n",
    "        \"to build/compare/measure/verify.\\n\\n\"\n",
    "        \"Ordering guidance:\\n\"\n",
    "        \"- Start with a crisp intro and problem framing.\\n\"\n",
    "        \"- Build core concepts before advanced details.\\n\"\n",
    "        \"- Include one section for common mistakes and how to avoid them.\\n\"\n",
    "        \"- End with a practical summary/checklist and next steps.\\n\\n\"\n",
    "        \"Output must strictly match the Plan schema.\"\n",
    "        \\\\{format_instructions}\n",
    "                    \"\"\"),\n",
    "        (\"human\", \"Topic: {topic}\")\n",
    "    ])\n",
    "    prompt = prompt.partial(format_instructions=parser.get_format_instructions()) # tell the output rules to the prompt automatically\n",
    "    \n",
    "    # Build and run chain\n",
    "    chain = prompt | structured_llm | parser\n",
    "    plan_dict = chain.invoke({\"topic\": state[\"topic\"]})\n",
    "    \n",
    "    # CRITICAL FIX: Convert dict to Pydantic Plan object\n",
    "    # The parser returns a dict, but State expects a Plan object\n",
    "    plan_object = Plan(**plan_dict)\n",
    "    \n",
    "    return {\"plan\": plan_object}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa89106f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fanout(state: State):\n",
    "    return [\n",
    "        Send(\n",
    "            \"worker\",\n",
    "            {\"task\": task, \"topic\": state[\"topic\"], \"plan\": state[\"plan\"]},\n",
    "        )\n",
    "        for task in state[\"plan\"].tasks\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9d50c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(payload: dict) -> dict:\n",
    "\n",
    "    task = payload[\"task\"]\n",
    "    topic = payload[\"topic\"]\n",
    "    plan = payload[\"plan\"]\n",
    "\n",
    "    bullets_text = \"\\n- \" + \"\\n- \".join(task.bullets)\n",
    "\n",
    "    section_md = content_llm.invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "    content=(\n",
    "        \"You are a senior technical writer and developer advocate. Write ONE section of a technical blog post in Markdown.\\n\\n\"\n",
    "        \"Hard constraints:\\n\"\n",
    "        \"- Follow the provided Goal and cover ALL Bullets in order (do not skip or merge bullets).\\n\"\n",
    "        \"- Stay close to the Target words (±15%).\\n\"\n",
    "        \"- Output ONLY the section content in Markdown (no blog title H1, no extra commentary).\\n\\n\"\n",
    "        \"Technical quality bar:\\n\"\n",
    "        \"- Be precise and implementation-oriented (developers should be able to apply it).\\n\"\n",
    "        \"- Prefer concrete details over abstractions: APIs, data structures, protocols, and exact terms.\\n\"\n",
    "        \"- When relevant, include at least one of:\\n\"\n",
    "        \"  * a small code snippet (minimal, correct, and idiomatic)\\n\"\n",
    "        \"  * a tiny example input/output\\n\"\n",
    "        \"  * a checklist of steps\\n\"\n",
    "        \"  * a diagram described in text (e.g., 'Flow: A -> B -> C')\\n\"\n",
    "        \"- Explain trade-offs briefly (performance, cost, complexity, reliability).\\n\"\n",
    "        \"- Call out edge cases / failure modes and what to do about them.\\n\"\n",
    "        \"- If you mention a best practice, add the 'why' in one sentence.\\n\\n\"\n",
    "        \"Markdown style:\\n\"\n",
    "        \"- Start with a '## <Section Title>' heading.\\n\"\n",
    "        \"- Use short paragraphs, bullet lists where helpful, and code fences for code.\\n\"\n",
    "        \"- Avoid fluff. Avoid marketing language.\\n\"\n",
    "        \"- If you include code, keep it focused on the bullet being addressed.\\n\"\n",
    "    )\n",
    ")\n",
    ",\n",
    "            HumanMessage(\n",
    "                content=(\n",
    "                    f\"Blog: {plan.blog_title}\\n\"\n",
    "                    f\"Audience: {plan.audience}\\n\"\n",
    "                    f\"Tone: {plan.tone}\\n\"\n",
    "                    f\"Topic: {topic}\\n\\n\"\n",
    "                    f\"Section: {task.title}\\n\"\n",
    "                    f\"Section type: {task.section_type}\\n\"\n",
    "                    f\"Goal: {task.goal}\\n\"\n",
    "                    f\"Target words: {task.target_words}\\n\"\n",
    "                    f\"Bullets:{bullets_text}\\n\"\n",
    "                )\n",
    "            ),\n",
    "        ]\n",
    "    ).content.strip()\n",
    "\n",
    "    return {\"sections\": [section_md]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d59c2b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reducer(state: State) -> dict:\n",
    "\n",
    "    title = state[\"plan\"].blog_title\n",
    "    body = \"\\n\\n\".join(state[\"sections\"]).strip()\n",
    "    \n",
    "    final_md = f\"# {title}\\n\\n{body}\\n\"\n",
    "\n",
    "    # Save to file\n",
    "    filename = \"\".join(c if c.isalnum() or c in (\" \", \"_\", \"-\") else \"\" for c in title)\n",
    "    filename = filename.strip().lower().replace(\" \", \"_\") + \".md\"\n",
    "    Path(filename).write_text(final_md, encoding=\"utf-8\")\n",
    "\n",
    "    return {\"final\": final_md}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c515ad78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIMAAAGwCAIAAAAFZkGGAAAQAElEQVR4nOydB1wUx9vHZ/cavfemoggoKiAmRk3sPdbYazT2EkusiUajKWo01TeWaDRGjfUfNcaSWGPvUqxBELDQkX5w3O777O1xHMcdCjeHu8d+44fszs6Wm9/O88zOzj4jpmkaCXAAMRLgBoISXEFQgisISnAFQQmuICjBFWpIiavHM57HF8gLaGUJrSjS024mCYKq0J4WkUyiTjKBCBrRBIG000nISdHaC6WZEV0+kV3W3Z0gaIKmKa1TSwmSQBIpcnSXNn3bwc3HEpkYwqTPEwc3PE1LkhcV0CIJIbUkJFKSIEmqWN8Z2TIrD6lSQiedVuUlSKRdcFBsTEFCCZdPZ7MzWzXyEKWHoLV3V2XVVkICUtHywhJ5PpNTLCGs7UVt+jvVC7ZDpsFUSuz+JjHtSbGlDVkvxLrDIHfEc26dyYg+n5ubWSKzIntO8PD0s0K4wa9E9PmscwczbOzF737g7uRp8kpdwxxc/yTpgdzNTzJoVh2EFcxKHFz/9FlcYduBzo1aOCLzZfOnsZQSjf+iAcIHTiWuncyIPPVi3Bf1US3g0Oak1PjicZ9j+7HYlNj3fVJWWtH4z3HeJhznyNZnifcKJ63EIwaJcHBy97PM5NolA9DjfS/vBha/LIlHOMCjxL0rBRO+ql0ysPQa7w3t40MbnyKjwaDEpkWP6gSbWxvp1Rm7rF7i/UKlUomMw1glIv/NKiqk4dZAtRWCIJw8JNu/TETGYawS1/7O9GlggWo3A2d652a91jqhUCjk+XSfyT6odiORii0syYPrjPIWRilx4vd0mRWBapZHjx69++67qOosWLDg4MGDyDT4BlmmJMmRERilRGqCHLoqUc1y9+5dVC2qveOrENbOoaTYqCczo5SQ51OedWTINOTm5n799dd9+vR5++23J06ceODAAUhcv379Z599lpycHBERsWPHDkjZvXv3tGnT2rVr17Vr14ULFz558oTdfdeuXZBy5syZN954Y/Xq1ZD/2bNny5cvh5zIBLj5Mq3H+OhcVF2MUqJEQXvVN1X7FUo8KioKCnffvn0hISFfffUVrE6aNGnUqFEeHh7Xr18fPnz47du3Qa1mzZpBWUP+zMzMRYsWsbtLpdL8/HzYd9myZYMGDbpw4QIkLl68GLRBpgE60p/FVd9AGfemiEC2zhJkGm7evAmF3rJlS1iePn16p06dHBwcdPI0adJkz549fn5+YjHzQ6AFMWvWrOzsbHt7e2hcyuXy0aNHt2jRAjYVFRUhEyOCMxaUoOpilBLgrEkaz1N6RUJDQ7dv3/7ixYvw8PC33norODi4Yh6RSATmaM2aNTExMVAD2ESoGaAEu9y4cWNUU9DQiUdVv/1ibDlm55jqXlu6dOmwYcMuXbo0e/bszp07r1u3rqRE9447e/YsbG3UqNHPP/987dq1tWvX6mQAG4VqCqWSklpXvzyNqxMkSomX1wuyRSbAzs5u7NixY8aMiYyMPH369ObNm21tbUeMGKGd548//oCqM3XqVHYVnDx6fZQokLtv9dsvRikhkZHPYk1SJ8DWHzt2DBpOFhYWoSoePHhw//79itk8PT01q6dOnUKvibxsBZinwOb2qLoYZZ2cPaRpT02iBHjgjRs3zp8/HypERkbGX3/9BTKAHrAJ/HN6ejo0gRISEho2bHj58mVoR4HhYhu1wPPnzyseUCaTubm5aTIj3Fw5lo6Me8Y1Sok2fV31DpkxHmtra2iepqamfvDBB/BYsG3btpkzZ/bv3585aZs2IMmcOXOOHz8+ZcqUVq1agasAlw4PGdCQBZ/x4YcfQn2qeEywdeBLPvroo8LCQoSbR5H5Th7GmXoj39ltWPCobmOrriM9Ue1m7azY4R/7ObpWv4FgbNsp+E27uKgCVLuBN8cSC8IYGZDxYwDf6ecacz779N6U9gP1D2qCxqihx1qw1+wTmd69TNQtAVRy5Eouae/eva6urno3JScU9Z3qgYwDw4iC+Jjcv35JmfaN/renYJQNechKfralpaWhTcZTSWO3kksC10WSekzIr1/GSyTEsLl1kXHgGdux/8fEnEzlmCX1UC3j4uH0qH9fTFqF4R0+nr6K96b7kSTx+6rHqDbxPKHg1ik8MiC8I88OrH+anVY0erE/qgXcuZx5Zm/m1DXYRrRgHo352xePi+TUuOVmLsaebxPSnigwyoBMMUL5yJZncdEFPgEWfc3x/fa1ExlXj2ZJLTAPikUmGrUvzyveueZpYY7SyVPSsrtTvcYm6SKsSZRK5dGtKUkPCyglCmll17a/G8KNCb9kib2Te+F/6fkvlNAhY2EtsnEUWdmIxFKS0urEF4kIZQmFCHUK+3+4Is2CKlX13Unp5yfs50Cav5BGlfs6SP2FiiZP2WEpWvXBizqFXRCJEEWVy8wiFtEKhbIgh8rNKilSfQollqKAMJuOQ4x9bjCEab8pYok6nxkfU5idXgzv3EuUtLK4bBMpIqgSuqzvjFB9vEXTTAJRem1liaoSI8qumVlGqo+JEE0S7E7MX1r1nyanKpGmy6ewC6SYoJXMNh0lRBKCFDFffVnakj4BVm/3dUUmpiaUMDUnT56E3sBVq1YhPmMO355W8mDMIwQluIKgBFcwByUUCoVEYqrBPjWGUCe4gqAEVxCU4AqCn+AKphpLWZMISnAFwTpxBUEJriAowRUEJbiCoARXEJTgCoISXEHoAeQKQp3gCoISXEFQgisISnAFwWNzBaFOcAVnZ2eRSIR4jjko8eLFi+LiYsRzzEEJME2m+MS6hjETJYwPTfnaMQclwEkIdYITCNaJKwhKcAVBCa4gKMEVBCW4ArSdhFYsJxDqBFcQlOAKghJcQVCCKwhKcAXzaDuZw6h9eHUKL1ARz+FxjILu3bunpKRoVgmCoCjK29v78OHDiIfwuE4MGzYMagNZCigBZqpbt26In/BYiUGDBkEN0E7x9fUdMGAA4ic8VkImkw0cOBD+alJatmzp4WGqqD+mht8ee+jQoZpqARqAvUK8hfdtpxEjRrDVokWLFmCdEG95edsp8WH+fzdzi7Qn4CkNSqYKYIU0+xNsvDFaNx2po5VVCEJWmkNE0srys5loZ9AOcoZQueBk7JYrV6/I5fLm4eE2NrZIHaGL0MmpSddJrHgxumgFWtPZi9b6pRXOVbYqkSAnD3Hzji6oUl6ixOZPY4sKmHkmtGPqs8WK1L9BKwIZqbosuD4mBly5A0PThqJo9q96R8hClR5QhOjyT2aEqq6yGWAZFti/Oj9Sa5lWhTErl8jEmtOKTMeuslKUu0uYaGk0RWmlVNBGjxLs0ZifrFJY51zaSlgQiiIKUlr3cWnaWneqJQ2VPWNvWBjr4iXuMqouEjCa2FvZFw6mySwIQ7OFGKwTP38S6xNg0aZfbZ9JEy/bP4/tMdajTrBNxU36Pfalw6mUEgkyYMfZW3JqX4reTfqVSPxPbmFrDp2DXMM3yLYoT78R0l/cigIKUUgAO9aOUqWB/nv9SigpZMwkeQKGICmCNnCLCyaIKwhKcAX9HlsVEZr3sZX5hf46wXQWEIKfMAGEwXI1YJ2E+mAiaIO2xoB1Ipno9kigBjFgnaAzixasU42iXwm2V1UAO7ThcjVUJ4Smk0kgVLPG6EW/nxCJSCQYJ9NQNY+tZLo7kEBNwqH32F98uWj6jA9QbcWAEgTB9we7Pw7s+WrlElR1Plu24MjRg6jGMaAEzfuJpB48uIuqRbV3fBUqKVT9bSeSmfetypVi22+bjv99OD091c3NI7RZ81kzF7LTGPfp13HUiHH/nj8VFXXr4IFTdrZ2ly6d+/7HlWlpqQ3qN+zbd1D3br3ZI0jEktu3b3zx1aIXL7Jg0/Tp8xoFh7Cbjh3/89Cf++PjY+vVa9ChfZf3+g9lq21i4uMtW9ffjrwB907jxk2HDBrVpEnozNkTIiNvwta///5rw/rt0dG3d/6+Ba5nydJ5cLrpU+fABZw6fTwq+lZOTnZwUMjIkePCQiMgf/uOzN+vVy9ft/7bPw+egeULF87+um1jQmK8vb1DgwaBM6bPd3f30PlRp09ef8UiqqRM9dcJqBAUXbVXRVAcBw7umTxx5r69xz8YO+XM2X/27tvBbpJIJIeP/AE/4+tV/2dlaQWlsHjJnA/GTl3x1Q9t2rRf9fWyEyePsTlTUpMP/bnv44XLYVOxovjr1cvYugkZVq76rGFA0M7th8Z9MHXf/p1rf1oD6cXFxVDoIpFo5Yof13y9TiwSf7Jollwu/+6bjcHBIV269IQygr2kUmlBQf6hQ/sWLljWr88gyABiFxUVLZj/2ZdffOfnVxf2yszMgAMeO3IB/s6ds5iV4fqNK58unQvH2bPryJLFK1JSnn/3w4qKPwrhwMDzBF21Z+zcvNzfd/06edKsNm3awWq7tp3i4v7bvmNz/35D4Irh5rWzs4c7kc0Mmr3zdofOnbrDcouIlvn5eVBM7Ka0tJT1636zVQ1bgn1Xr/kc7lm4GY8cOdC0adjMGQsg3dHRaczoSatWLxsxbCwUX1ZWJtQPKG7YtOTTFZFRNyt+1QIXAKU/ZMjo8LAWbMqmjbssLS3hyLAMdeLgoX3RMbfbvtNRZ8dftqyDSx3wHjO0EDJPmTx7ztwp9x/cDQpspPOjjMeAdSIIZVWMU1JSgkKhCC61JEDDhsF5eXlPnybVrcvMCxzYsBGbTlHUo7j/OqlkYJk0cYZmuX79hqwMgL0dU0xQgra2VMydyFEjx2uyhYW1gOOAbWn5ZhsHB8cVq5Z27tQD7GFISDPWyOglKLCxZhm037R5Ldi0jIx0NgXsYcVd4H7Slof9Fffv3wElkNaPqgoGPYV+JSj1LKOvSmYm83ssZBaaFEtLK/hbWFjAroJ9YBegZKEQZVo5y12NVhA5TesNTBDIvPmXn+CfdmaoDTKZ7Ptvf/7ryAGwV7DVy8vn/VETOnfuoffgmmtISUmeMWtceNgbiz/5slGjJnCizl1bVswPdxJYMO1LtbJifpSmBmsO+OrQhkvVoMemqtJ2srZmBvAUygs1KezlOjnpDkGEsgM3DhYJvTIWFhZQBF0693ynvPXw8mQGAYGVnzxp5pj3J928efXosUNfrvi0Tl1/1lgZAnwYqAtOAgwUMlAb2PMi5tYp+1H5qh/l7PSScZWVUMnjm8G3p1VqxIJVAbd5505kcJDaAty7FwN2xtVVd/ZiyBYY2AiMsibl501roVymTpld+fHBFWksD1SR58+furm5Q8Ppzt0oaHpBqbVq9c6bb7bu1qP1w4f3KlcCfI+trR0rA3D235N6s0EFDWwYfOdOlCaFXfavH4CqSyWlql8kZvRqVaSAhilY6u07frl48d+c3BxoO/5xYPeAAcPZVqwOfXoNuHbt0u49v926fR1cJbj6evXqV3788R9Mu3DhDDxwgWWDJumy5Qtnz5kE+kGZQtNr3frvnjxNAl+1Y+cWcNchjZvBLt7evnA33Lx1m2SPggAAEABJREFUDYyYztH8/QPAPUCbGDJfuXoRKhN449TUZKSqsnD3XL9+Ga4NtvbrO/j8hTP79/8OPwpSflr3Dfj8gAaByAQY8thIWcXHialTPoJyX/7Fx/ADwF4PGzpm6JDRenN27fpuTm42NNLz8/OdnV0mjJ/eo3ufyg8Ojwgb1++Agt6w8QcwF40bNf18+TdQauCiZ8/6eOuvG/bs3Q7ZIpq/+c2a9WwboVfP/lA55s6bCg1cnaN17NA1ISFu228/f/vdV9B4mz9v6a7d23b+vjU3NweONnzYWGjdXb128fedh6H9mpaeunvvb9BohseIiOYtx4+bhkyD/nGxvy5/TFPEezPrIAGsPL6bf3bP82nfNqi4Sb91IglC6Is1BWRVNzGtWOH9hAmossdWNeWFSlGjGOrtYAbmIAHcVLkHkBQJQpgE2vDIMwN+Qsn79xMcxfDIM0OjbIS2U01j4P0EJdSJmsZAnSCQ4CdqGINtJ6FO1DCGv2QRpKhZDFknoRVb0xgaUSDUiJpGf52QWoroEt7HOOQgcIeLDDgE/XXC0hreGgpK4Cc1KZ8wMMuYfiXaD3IpzBPsE34S7xe4+8n0btKvhL2zpUc96Y6vYpEAPo5ue6yQK/tN0R8OrLL4TpePpd06le3pb+UdYGlpVcmIEv1DctgwULS+L/ZoTewtA4fT/qZJb7ayRKIsK1GhK58Nh1UupTQOl96LIfSdV71L6TZ12C9U9jZN57yqUXtlp6AIOvVxftKDfEgb86k/MsBLIm2BGPcu58kLlMpqRMatZMxUlYZTvUQKrTRat6dTqzRLH5Aq7FjxSDrH0Xt+7VhauuctH5dLJEEiEXL1lRmqDeqdzKC9evLkyePHj69atQrxGXOIFiGVSvkbnFSDOdQJ88AcIrzn5eVlZWUhnmMOShw9enTDhg2I55iDn7CysnJ1dUU8R/ATXMEcrFNOTk52djbiOeagxC4ViOeYg5+wtrZmvzrhNYKf4ArmYJ1evHiRm5uLeI45KLFx48YjR44gnmMOfsLGxsbR0RHxHMFPcAVzsE6ZmZn5+fmI55iDEqtXrz5//jziOebgJ+xVIJ4j+AmuYA7WKS0tTS6XI55jDkosWrQoJiYG8Rxz8BPOzs5slBleI/gJrmAO1ik5OdkMZio3ByW+//77+Ph4xHPMwU8UFxeLRCLEcwQ/wRXMwTqlpqYWFRUhnmMOSnz66adRUVGI55iDn/D09JRIJIjnCH6CK5iDdUpPTy8sLEQ8R3g/wRXMwU+4ubnJZDLEcwQ/wRXMwTplZWXl5VUhPDY3MQclNmzYcPToUcRzzMFPuLq6Cu8nBLBhDtYpOzs7JycH8RxzUOL333/fvXs34jnm4CecnJyUSt5H3uGxn+jcuXNGRoZmEh1ahbu7+7FjxxAP4bF16tKlC2Jno1RBkiT8bdWqFeInPFZi5MiRfn5+2ikeHh5Dhw5F/ITHSkC5s9VCQ2hoaEBA9ScRer3wu+00fPhwX191qB4XF5dhw4Yh3sJvJezt7Xv27MkuBwcHh4SEIN5i2lZs7O0cgmTHv5QPV6WJEkaop/XUDkXGBqoqt4MmGxNErWwmUMjTOuy9q4GJBYUFXVoPfxRV7nuWiuHNKoZAM0DF6GmIpigbJ8LDzwaZBpO0YqF1/+uyhII8SiRC6mBp6jBg6l+oikhHI+3oYJolQt9sMNqJWssVg5y9hAoHNxA4j6gYQpokmcxiCQpobtNhIP5wUvjrhLJYuW5BfJ1Ai3ZDfJDZEX0+49apLGePjGZvOyOs4K8TP82N7T3R297VEpkvO1fG1m1s2XW4N8IHZo+9e02CjaPYvGUAwjo5x0diHsOAWYnsdIVPQzOXAQhu7qik0KOoTIQPzH6ipATZOVR5rmI+QhLkizSEEcxKUCWoRGEOPe0vRamk8M4LYQ694uYBZiUIEpGi2vE6lnm+xFkrMCtBU4hS4q21XIV5OsV5zwnWiSsISlQTtrMM4QOzEmIJQdQO46Sa0YHD1qlEUVvGT9G4bzjBOnEFQYlqQmj+YEJQoprQiMY7XTLuJzv4r1Z0diD16y58YC42Gr3OuWv3/29Xx85vIH5iAutE15JmLGYEP1FN4HajsHrs12nU8/Pz23eMiIy8ya6eOHkMVv84sIddTUx8DKt37zHBzC5cODth4vCu3VsNGtLj40WzUlKS2TxLls5btnzhho0/QM5/z53SPrhSqZwzd8qIUf2yc5gJEe7ciZo3f1rvPu1Hju7/07pvNVFNtY9w9240emVUk+5x2U8QTHfsK2Jtbe3m5n7nrjrQQ0zMbXd3j7ulq9Ext22sbYICG12/ceXTpXO7dOm5Z9eRJYtXpKQ8/+6HFWweiUQSFx8L/75Y/k3TJmHaB1+1etnDh/dWrVxrb2f/5GnSnHlT5EXytT9uWf7Z6ri4/2bNnlACb7XKH8HPrx56ZQgCbyMWe9uJZrpjX52w0Bb37qlD+EVG3ezWtdeRowfZ1ejo2xERLUmS/GXLunfe7jDgPWZ8n729w5TJs+Fmv//gLogEXSvJyc/W//SbzqwH237bdPr039+sXu/lybz0P3HiqEQsAQ1gd1id89HiocN7nb9wpl3bToaO8FKYdgnWpgl+61SlywsPaxEVfQsx3wW9ePw4rnevARkZ6azxgToRHs40hOAWDgpqrNklsGEj+Hv//h12tY5fPU0hsoPGwcpt2br+44XLQ0Kasel37kTCEVgZEDOg1tPLy4c9r84RXiP4PXaVqmzz5m/m5GSDSwD7ENAg0MnJuVGjJlFRN994o9WzZ0/eaNEqLy+vqKhIJisrKfbjxoICtaGXan0TD31e4B5WrFwCyxZau+Tl5UIdAk+gfeqszIyKR3h1VD8T5338mttOzs4u9erVB1cR++hhk6aMoQdzD6ukSASGBdwGa83l8rIhLfkqDZydXAwd86PZn4ChW7Fq6ZbNexwdnSDFydmlSZPQMe9P0s5mb+eAjEBV9atiiF8GZutEMIMeqrQHCgtrAc2n6KhbzZqGw2qTkFCwG7duXQMnAatisTiwYTC0fDT52WX/+vpH54Nf6d6t94zp860srb74chGbWN8/IDU1GY4fFhrB/nN0cPLzq4uMhcOt2Gq4sfBQUOIGUydCQmE1JCQ0ISH+xo0rrJMA+vUdDN51//7fc3Jzbt2+/tO6b8C7gCmr5JiWlpZLl666HXljz97tsDpgwHCKotb+tEYulyclJUCbdey4wWAPkbFw++1pVTs7oMSTU57DHcpaEhsbm7p1/ePiYqGusBmg/ZqWnrp7729QlGCvIpq3HD9u2ksP2zAgaNTI8T9vWgv5/f0bbN60e9euXydOHgE+Cbz33DmLIQPiEpjHxa6dFRvRxbVxK95Hvn8pv34W26qnU3hHJ4QJobejumDuFDeBEiTx2vpiaxKCUL0CwAd+JfD2i3EWWvMHE6boFUcC1UDwE9WEUH3Hh/AhKFFNVMOduO0naskrO+ajSJLDSsDLCVHtUIL57JXisHVixorj7BarRbzmXnEBDSbwE6hWwPWx4rUH5oYTvmQxS7B/PwHvamqFyybhzQ6BM/ggZiVEYiLnRTGqBYCPcPbCOQ4B8zs7B3fJ0wcFyNy5fS4NnpzqBNkifGBWYuAMv4I85d2r6cisiT6XHfwW5kBPJonvtG5erIO7+M2ebq6evA/3rU1xcfGNvzP/u5nXY7xHvSA+KAFs+zw+N0sJxrRiSF2tmGVaVAh5pRsUSysDUZqgdUzojiMITeNS09JXLbPpmvOWBVxTDTPWuRj2UGXXWXpekmS2yKzI8I62zdu7ItyYNnJvZkpxJUqQWgOGiIqPhGCJtUZ2autHIpKiy95I3bxx/fLly1OmTiMQSSOqYnw0dTrN/IfKh1dDZZlL96MJUkRTlGZfdQbYxc3bhMFhTPs84eReE3FtiJg8OZXm6sXvGDrm8GSnUCjMYD47QQmuYA5KlJSUiMW8/yFmooRQJzgBWCehTnACwTpxBfNQwhwCCphH28kclBCsE1cQlOAKghJcQVCCKwhKcAVBCa4gKMEVBCW4gqAEVxCU4AqCElxBUIIrCEpwBUEJruDr6yuV8n6aKnNQIjExEV5RIJ5jDkqAaWJDo/EaQQmuICjBFcxBCZFIpFTi/NDqtSDUCa4gKMEVBCW4gqAEVxCU4ApC24krCHWCKwhKcAVBCa4gKMEVBCW4gqAEVyD4O4ty7969FSoKCgooiiJJEpZtbW1PnTqFeAiPv2Rp2LBhcnLyixcviouLoU7AX3iqiIiIQPyEx0pMmDDBy8tLO8XV1XXIkCGIn/C7TujUgMDAwPDwcMRP+P2d3bhx4zw8PNhle3v7wYMHI97CbyV8fX07dOjALvv7+7du3RrxFt5/ezps2DBvb29ra+uhQ4ciPoO5FbtjZXxuJtMxSqn6RrWjj2lCVpEkQZXGRi+LQ6YVSKtiqCw9azox0uiXhNEmdGOkvVLcNUOHEomRzIqI6OzQtI0zwgTOJ7t1c2PtXMiILs4uPhaIECGdQGU0QRHqoGKaee/LlKBIWh1ollDFfyvdi2K2qFeYoGVQEKq4ZWxC6alL45mpC1wT3qwMigmUhsryl12Y5nZRB0zT5GE2kXBlqDwiQpmXW/LgWva5A1m2jtJ6jfFEyMRWJ0CGiM52QW+6odrEji9jgyJs2g30QEaDx0/sXPXY3kVc22QAWvZ1unMlD+EAjxI56SWBEeY/m2BF6jdyAp9x9UQGMho8fgJctLPP659i+rUgFhFZyRgCeONRgpmHReWiayGKYqRUYJiIQpj1wFiYyR5xzAgiKGEsNMIzD42ghLHQNJ4HAUEJYyEJgsDRAhWU4Ar4lKjFE89yzGPX1nnswElgmUxRsE7GQpCESITBIAhKGAtN0Uql8GTHBQg8hllQwmhoPI0VjErUVpeNqU5gfI9dQ83YMR8M+u77FYg7CM/YHIF5xBaesbkA8zyBo068nlE2+/+3672BXc9fONOx8xs//t9qpIrRtGHjD2B5evZ6Z/7CDy9fPq/J/Phx3KTJI7v3bLPwk5n37sVo0u/dv9O+YwT81aSMGNn3p3XfssuJiY9nzBoPGYaP6LN+w/fFxeqXOXfuRM2bP613n/YjR/eHzPn5+RUv6dz50+iVIUWECEcpvh4lpFJpQUH+oUP7Fi5Y1q/PIEj54cdV+/bv7Nd38M4df7Z9p+OSz+ad/fckUs1oMH/hdFdX962/7Js4/sNdu7dlZLx8Ks/k5OfTpo9pEhK6ZvW6wYNHnTx1DI4P6U+eJs2ZN0VeJF/745bln62Oi/tv1uwJ7Ih/7UuCHdErQykpbvmJKrUfwLbK5fIhQ0aHh7WA1aKiouN/Hx429P3evd6D1R7d+8TERG777WeQ5N9zp1JTU77/dpO7OzN+4sPp8wYO7v7S44OoMguLMe9PEolEcAoo5QcP7kL6iRNHJWIJaGBv7wCrcz5aPHR4L6gH7dp20rmkKv0abh+KJ1cAAAwTSURBVFknuuptuaDAxuzCw4f3wHq0iHhLsym0WfO4uNjsnOynT5MsLCw8PDzZdGdnFzc395ceGW72gIAgkIFd7da114wP5yPGNEUGBTVmZQDgsF5ePlHRtypeUhXg3pNdlW8MTci4vLxc+Dt9xgc6GbIyM3Jysi0ty01jK5O9fOhCfn6eg4NjxXQ40f0Hd8F56Jyl4iVVAe492VUfZxdmQtePZn/i7e2rne7m5mFnZ19YWG7CbbDmho5TolR/42VtbZOvL5uTs0uTJqFgtbQT7e0ckDFwrU4YczU+3n4ymQwWwkLVd2tWViY8L1lZWXm4e4L5Bkvl798A0mNjH6anp7F5ZFJmF41OeXl5mk2BgY3+PLxfEzPz5KnjR48eXLnix/r+AX//81ezpuFk6SMANMx8fPyQMdDMmE3jwegnqg+U+PujJ4KLjo6+DQ4DWk3QwmEfpFu1agsWY/U3n4MeUNDLPl8ItYTdy9e3jq2N7ZGjB6EooNBXrFpia2vHburZoy8c55tvv7x+4wo0SX/e9CNUO3AbAwYMpyhq7U9r4GhJSQnQbh47bnBcfCwyCgKZ0/uJIYNH1a/fcOeurTdvXgXb0rhR048+WgTpNjY2X37x3caNP7zbuy247gnjPzxx8ii7i0QiWbz4q+9/WNmhUwsXF9eJE2ZkZmawPQ9wm6/46ofVq5cfPXYIalvXLu+OGzcN0u1s7TZv2r1r168TJ4+ABw7w3nPnLG4YEISMAF7YkTjME54Ryj/Oiu012c/ZnfdBW6vB9s8f1Wlk3WOMsYOUhd4ODHDLY9O1t1ecARkNxrZTbR3cQdNYujsE68QVBCW4guAnjIVkRtkg4xH8hLFQzCgbZDyCdeIKghJGY049gLyGQHh+PEaPXUuBZwlhlI1ZgUcJEqoEjp5hPkKKaJLAUCnwvJ8gxURBbiGqlYAIVg4YHAUeJSysRQ+u4wmawC/gfRSlQK17uSKjwaNE675OyXG1sU4c/inJyVMkwvGQjS2WzeO7eX9tTg7v4hDS0gXVAvIyi//anOjibdF3sg/CAc5IW1HnMi79lUVRSEQiRfkACtCBD+eBCkiVT1FBq/r4tYerqFPYDKoFmmDCPpUdkCTLfd2mdTRmGXog2LcGtPq7dfXFwIUpS/cSiVHpUJCy3dmHA5pWB4piT60VTYpgT02IaGUxcvGRDJ5dB2ECf+Tee9dfpCcV05SOEyNKQ1rpnq70h2pFJSuNl1UhRed4qt1pIj097Xny86ZNmlQ8qC5aEbY0Ab20A7Op9iRVaZqrQqXnVqfBIWwcxc07OCGs4H+eCI5wQDUbPfeff25de3xy2nsdEJ8R5gLmCoISXMEclFAoFBKJBPEcoU5wBUEJriAowRUEP8EVhDrBFXgf4R0J1ok7CEpwBUEJriB4bK4g1AmuICjBFQQluILgJ7iCUCe4gqAEVxCU4AqCElxBUIIrCEpwBRcXFzYUDq8xByVSUlLYWH68xhyUANMkKMEJBCW4gqAEVxCJREosH6e/VoQ6wRUEJbiCoARXEJTgCoISXEFoO3EFoU5wBUEJriAowRUEJbiCoARXMAclJBKJQqFAPAd/jIIao3fv3iAAQRDs/Fu2tra0iiNHjiAewuM64efnd/HiRc2cHqAHyBAeHo74CY+/KXr//ffhDbZ2io2NzaBBgxA/4bESERERoaHlJp6DWtK5c2fET/j9nd2IESM8PdUTrMlksqFDhyLewm8lmjZtGhYWxi57e3v36NED8Rbef3sK1cLNzU0qlQ4cOBDxmZprxV7/Jz3hfmFORkmxnFKWlMUYKw0tpg53pV6g1bHHUPmAZFrXWhYFjaYoGtEkKULMX0ITC0075lm5oGgIUaogapps2rMDisRMikhMWNiQHnVlHQa5k2RN3K8mVyLpv7wze9JzsqDskUgiklqJxTIRKRaJys8oowoyxpQHW0DsNWlyQFGTqqCsRLn8unHN2HInKh6ZSSw7Hl0qjd6DUcwMK5RCriwuUJQUK2klklqhwDDbtgNePsunMZhWia3LHudll1jYSNzqO9i52iB+En/zeX66HKrcGz0dI9o7I9NgKiXO7kuJvpBr5Sjzb+GFzIJnD9Iyk/JsHcWjF9VFJsAkSuxek5iZoqjf0ktqaW4z3P136UmJXDF5VQOEG/y+6NTe1Izk4uD2dc1PBiDgLR9LZ8sNCx4h3GCuE3u+TUhPVjRqVw+ZNU/vpuWk5E9eVR/hA2edOL03Je2J+csAeDdytbCVbloUh/CBU4k7F3MDWnuj2kG9CC95IfXX1icIE9iU+GVpvIWtxCx9gyH8W3jGR8oRJvAo8SwuvyBb2eAtPLHO+YKVvYVIinatTkA4wKPEiR2p8PCMuMrt6BNzFr+Zl5+FcOMW4Jz+DM+LWzxK5GQq3Rs4otqHs7cddJqcO5CCjAaDEpH/MveavQdfOzOMRGotjr1dgIwGg0l5eCuHNKVlunbz8KVrfzxPifV0bxDapNPbbw1h+/h+2/0xPA+FN+u2+3/LiooK6vg26dl1Wh3fEHavw8d+vB55RCa1Cmva1c3FD5kMa2eLF0kYZgbCUCdyMkskVqYK6nMz8vjuP5b7eAV+PPuP7p0n/3tx18Ej37KbSFKckBR94/bRGZO2fvnpWbFEuut/y9hNF6/uv3h1X/+ec2dM3OLs6PXP6c3IZIAxwPJwjEGJEjktk5mqUly9cdC/Tlj/XvNsbZwC/CO6dpxw4cre3LxMditUhcH9Fjk7eYtE4vCmXdPSEyAF0s9f2tO0ccemIR2srOxahL/bwN+EE2LYOFhCh3p2urHTNGFQAt4piEyjBEVR8YlRDQPe1KSAGDRNxT++za66udaVyazYZQsLW/hbUJgD/TfpmUnubmWP+j5eQciUgLHMzUBGgqEEVa/DTDLtaQm8qVEqjp1YD/+003PzM0tPredOkhflU5RSoxAglVoiU8K8ZzS6IDEoQYroYnkRMgFSqQW43OahPZo2LjfzDZijSvaykFnDm1SFouzpt6gYQ9umMmjk6mvsRGoYlLC0FkMPDDINXp4NC+W5Dfybs6slJYqMrKcO9pW9yISWlaOD5+PE6Lat1Sn3HlxAJiM7JRdqplRqbDcPBj/h5CEFK4JMQ4/Ok2Punb1y4xDjMxJub9/zyYYtU8FqVb5Xs5BO0XdPw6M1LJ86ty3hSQwyGTmpBWIpN2bbDG3nWKIwVZ2oVyd01uRt4KKXruy2Yev0QnnemOFfSyQviSHUqe2YN5v3OXBkDXRyQIXo3X0mQshE74nzs4ocXXG4WyzXt37BI3tPG8+GtWJ2Rx1i/onvPMItMNwOGQeefiePOrLs5/mo9vHkXrpYjIyXAeEatd93ss//zY7Nyyq0cdTfXoyKObXn4Bd6N1lZ2sFDgN5NYGF6dfsQYQLczObtH+ndBK1eaBDrDJRigc6Vrh3GIwPkPMsNjMDT4YbtPfaB9U+SHxcHta2jd2tRcWG+gU7poqJCmUy/flKplY21A8JHZtYzVEUsZDbwoK53k+ptdh6ucR44RxSAt7Bzt/EKqi3eAjxEr/EedYLx1Amc77HHLvXNepKLagf3zz72DbTAJQPCq4TUQtppmMudf+KRuXPvTLyds6jPRJxvi/GPASzMLt68NNE/wsPKybS9Pa+LB+cSAppZdxiMecAy/jGAlvbSbqNd428lP771HJkXYHvvnox3chdjlwGZdKz4pkVxxXLKwcfGK9AV8ZyCbHlSZGpJkbJlD6fmnTDPUc5i2lH7Z/9IuXc5V1mCLOykjj42Tl72iFcUFxQnP8zKyyykSmg3P9mgWb7IZNTEN0VXj6fdu5KXm62EBydSTDCdpapvVspfiNZHP8yHJyR7Yepk1Wc/zF6E+vMgWmcv9YcpdPk0ovxBCEQxp1Wns5dBlH7GRNOkajvzyQyk0LRSSdFKJJERPg0se44z+bcHNRqjIOm/3Niogpz0kpIiqkhedl7V90LwLq50nXkLiNhVEUlAgZR+BEYzH2WRqo+x2LImVQs0Ur0xImiKSSVFBKUsv0DCU7QqM5OBUH0BRjO7UAQiafWXZJT6XDIpKZIiCxvSy9+iaRuTGCK98DhahJlhDhFUzANBCa4gKMEVBCW4gqAEVxCU4Ar/DwAA//9RKcKeAAAABklEQVQDADNTu8sxgcCDAAAAAElFTkSuQmCC",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x000002B8F2812690>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 5) Graph\n",
    "# -----------------------------\n",
    "g = StateGraph(State)\n",
    "g.add_node(\"orchestrator\", orchestrator)\n",
    "g.add_node(\"worker\", worker)\n",
    "g.add_node(\"reducer\", reducer)\n",
    "\n",
    "g.add_edge(START, \"orchestrator\")\n",
    "g.add_conditional_edges(\"orchestrator\", fanout, [\"worker\"])\n",
    "g.add_edge(\"worker\", \"reducer\")\n",
    "g.add_edge(\"reducer\", END)\n",
    "\n",
    "app = g.compile()\n",
    "\n",
    "app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92eb63c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = app.invoke({\"topic\": \"Write a blog on Self Attention\", \"sections\": []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57603cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Mastering Self-Attention Mechanisms\n",
      "\n",
      "## Introduction to Self-Attention\n",
      "Self-attention is a fundamental mechanism for computing contextualized representations in sequence models. Understanding its limitations and benefits is crucial for designing effective machine learning architectures.\n",
      "\n",
      "### Traditional Sequence Models Limitations\n",
      "\n",
      "Traditional recurrent neural networks (RNNs) and long short-term memory (LSTM) architectures rely on sequential processing, where each element in the sequence is processed independently. However, this approach has several limitations:\n",
      "\n",
      "* **Vanishing gradients**: In deep RNNs, gradients from later layers can become extremely small due to the backpropagation algorithm's nature, making it challenging to train models with long-range dependencies.\n",
      "* **Sequential dependence**: RNNs and LSTMs process elements sequentially, which can lead to limited contextual understanding between adjacent elements.\n",
      "\n",
      "### Self-Attention Mechanism\n",
      "\n",
      "Self-attention addresses these limitations by considering all elements in a sequence simultaneously. It computes a weighted sum of the input representations based on their relevance to each other. This approach allows for:\n",
      "\n",
      "* **Long-range dependencies**: Self-attention captures relationships between distant elements, enabling models to understand complex sequential patterns.\n",
      "* **Contextual understanding**: By considering all elements together, self-attention promotes contextualized representations that capture nuances in sequential data.\n",
      "\n",
      "### Transformer Architecture\n",
      "\n",
      "The Transformer architecture is a primary application of self-attention mechanisms. Introduced in the paper \"Attention Is All You Need\" by Vaswani et al., it has revolutionized the field of natural language processing (NLP). The Transformer's key innovation is its use of self-attention to replace traditional recurrent layers.\n",
      "\n",
      "### Scenarios where Self-Attention is Beneficial\n",
      "\n",
      "Self-attention is particularly beneficial in NLP tasks, such as:\n",
      "\n",
      "* **Machine translation**: Capturing long-range dependencies and contextual understanding enables accurate translations.\n",
      "* **Text summarization**: Self-attention helps models comprehend complex sentence structures and relationships between words.\n",
      "* **Sentiment analysis**: Contextualized representations facilitated by self-attention improve sentiment detection accuracy.\n",
      "\n",
      "In summary, self-attention mechanisms offer a powerful approach to sequence modeling, enabling the capture of long-range dependencies and contextual understanding. By replacing traditional RNNs with self-attention-based architectures like the Transformer, developers can build more accurate and efficient machine learning models for NLP tasks.\n",
      "\n",
      "## Core Concepts of Self-Attention\n",
      "Self-attention is a fundamental mechanism in transformer architectures, enabling models to selectively focus on different parts of input data. To implement self-attention, developers need to grasp its core concepts.\n",
      "\n",
      "### Scaled Dot-Product Attention Mechanism\n",
      "The scaled dot-product attention mechanism is the heart of self-attention. It involves three key vectors:\n",
      "* **Query (Q)**: The vector representing the current token's context.\n",
      "* **Key (K)**: The vector representing the current token's content.\n",
      "* **Value (V)**: The vector representing the current token's relevance.\n",
      "\n",
      "The scaled dot-product attention mechanism computes attention scores using the formula:\n",
      "\\[ Attention_{q,k} = \\frac{e^{(Q \\cdot K^T / \\sqrt{d})}}{\\sum_{i=1}^{N} e^{(Q_i \\cdot K_i^T / \\sqrt{d})}} \\]\n",
      "where \\( d \\) is the dimensionality of the vectors, and $N$ is the sequence length.\n",
      "\n",
      "The softmax normalization ensures that the attention scores are probabilities, which sum to 1:\n",
      "\\[ Attention_{q,k} = Softmax(Q \\cdot K^T / \\sqrt{d}) \\]\n",
      "\n",
      "Here's a PyTorch code snippet demonstrating how to compute self-attention scores for a small sequence:\n",
      "\n",
      "```python\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "\n",
      "# Define the query, key, and value vectors\n",
      "Q = torch.randn(1, 5, 64)\n",
      "K = torch.randn(1, 5, 64)\n",
      "V = torch.randn(1, 5, 64)\n",
      "\n",
      "# Compute attention scores using scaled dot-product attention mechanism\n",
      "scaled_dot_product_attention = nn.Softmax(dim=2)\n",
      "\n",
      "attention_scores = scaled_dot_product_attention(torch.matmul(Q, K.T) / math.sqrt(K.shape[-1]))\n",
      "\n",
      "print(\"Attention Scores:\")\n",
      "print(attention_scores)\n",
      "```\n",
      "\n",
      "### Multi-Head Attention Concept\n",
      "The multi-head attention concept improves model robustness by capturing different patterns in input data. It involves multiple parallel attention mechanisms with different weight matrices.\n",
      "\n",
      "The idea is to:\n",
      "\n",
      "* Split the query, key, and value vectors into multiple heads.\n",
      "* Compute attention scores for each head separately.\n",
      "* Concatenate the results from all heads and apply a linear layer.\n",
      "\n",
      "This approach helps the model capture different patterns in input data, making it more robust.\n",
      "\n",
      "### Computational Complexity\n",
      "Self-attention has a computational complexity of O(n^2), where n is the sequence length. This can be challenging when dealing with long sequences, as the number of matrix multiplications increases quadratically.\n",
      "\n",
      "To mitigate this issue, developers can use techniques like:\n",
      "\n",
      "* Parallelization: Compute attention scores in parallel across multiple GPUs or CPU cores.\n",
      "* Approximations: Use approximate algorithms, such as the hierarchical attention mechanism.\n",
      "\n",
      "### Edge Cases and Failure Modes\n",
      "When dealing with self-attention, it's essential to consider edge cases and failure modes. Some common issues include:\n",
      "\n",
      "* **Vanishing gradients**: When the sequence length is very long, the gradients can vanish due to the scaling factor in the scaled dot-product attention mechanism.\n",
      "* **Out-of-vocabulary tokens**: When encountering out-of-vocabulary tokens during inference, self-attention mechanisms can produce unreliable results.\n",
      "\n",
      "To address these issues, developers can use techniques like:\n",
      "\n",
      "* **Gradient clipping**: Clip gradients to prevent vanishing or exploding values.\n",
      "* **Tokenization**: Use tokenization to handle out-of-vocabulary tokens and improve model robustness.\n",
      "\n",
      "## Common Mistakes and How to Avoid Them\n",
      "When implementing self-attention mechanisms, it's essential to recognize common pitfalls that can degrade model performance. Here are some frequent errors and tips on how to avoid them.\n",
      "\n",
      "* **Missing layer normalization**: Failing to apply layer normalization before the attention layer can lead to unstable gradients, causing the model to converge slowly or not at all. To avoid this, ensure you normalize the input sequence before passing it through the attention mechanism. This can be achieved using a normalization layer (e.g., `LayerNorm`) followed by the self-attention layer.\n",
      "\n",
      "```python\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "\n",
      "class SelfAttention(nn.Module):\n",
      "    def __init__(self, num_heads, seq_dim):\n",
      "        super(SelfAttention, self).__init__()\n",
      "        self.query_linear = nn.Linear(seq_dim, seq_dim)\n",
      "        self.key_linear = nn.Linear(seq_dim, seq_dim)\n",
      "        self.value_linear = nn.Linear(seq_dim, seq_dim)\n",
      "        self.norm1 = nn.LayerNorm(seq_dim)\n",
      "        self.norm2 = nn.LayerNorm(seq_dim)\n",
      "\n",
      "    def forward(self, x):\n",
      "        # Apply layer normalization\n",
      "        x = self.norm1(x)\n",
      "        \n",
      "        # Compute attention scores\n",
      "        query = self.query_linear(x).transpose(1, 2)  # (batch_size, seq_len, num_heads * seq_dim)\n",
      "        key = self.key_linear(x.transpose(1, 2))  # (batch_size, seq_len, num_heads * seq_dim)\n",
      "        value = self.value_linear(x)\n",
      "\n",
      "        # Compute attention weights\n",
      "        attention_weights = torch.matmul(query, key.transpose(-1, -2)) / math.sqrt(seq_dim)\n",
      "\n",
      "        # Apply attention mechanism\n",
      "        attention_output = attention_weights * value\n",
      "\n",
      "        # Apply layer normalization again\n",
      "        attention_output = self.norm2(attention_output)\n",
      "        \n",
      "        return attention_output\n",
      "```\n",
      "\n",
      "* **Inadequate handling of variable sequence lengths**: Failing to handle variable sequence lengths properly can result in padding issues, where the model is forced to process sequences of different lengths. To avoid this, ensure you use a mechanism like `additive_positional_encoding` or `relative_positional_encoding` to account for sequence length variations.\n",
      "\n",
      "```python\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "\n",
      "class SelfAttention(nn.Module):\n",
      "    def __init__(self, num_heads, seq_dim):\n",
      "        super(SelfAttention, self).__init__()\n",
      "        # ...\n",
      "        \n",
      "    def forward(self, x):\n",
      "        # Apply additive positional encoding\n",
      "        x = x + torch.sin(torch.arange(x.shape[1]).unsqueeze(0).expand(x.shape[1], x.shape[1]) * math.pi / 1000) * x\n",
      "        \n",
      "        # Compute attention scores and weights...\n",
      "```\n",
      "\n",
      "* **Ignoring quadratic complexity**: Failing to account for the quadratic complexity of self-attention can result in slow training times for long sequences. To mitigate this, use techniques like parallelization or approximation methods (e.g., `HierarchicalAttention`).\n",
      "\n",
      "```python\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "\n",
      "class HierarchicalAttention(nn.Module):\n",
      "    def __init__(self, num_heads, seq_dim):\n",
      "        super(HierarchicalAttention, self).__init__()\n",
      "        # ...\n",
      "        \n",
      "    def forward(self, x):\n",
      "        # Apply hierarchical attention mechanism\n",
      "        attention_output = []\n",
      "        for i in range(0, x.shape[1], 2 ** num_heads):\n",
      "            chunk = x[:, i:i + 2 ** num_heads]\n",
      "            # Compute attention scores and weights...\n",
      "            attention_weights = torch.matmul(query, key.transpose(-1, -2)) / math.sqrt(seq_dim)\n",
      "            # ...\n",
      "            attention_output.append(attention_weights * value)\n",
      "        \n",
      "        return torch.cat(attention_output, dim=1)\n",
      "```\n",
      "\n",
      "* **Ignoring input sanitization**: Failing to ensure input sanitization can make the model vulnerable to adversarial attacks on attention mechanisms. To avoid this, use techniques like `input validation` or ` whitening` to normalize and sanitize inputs.\n",
      "\n",
      "```python\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "\n",
      "class SanitizedSelfAttention(nn.Module):\n",
      "    def __init__(self, num_heads, seq_dim):\n",
      "        super(SanitizedSelfAttention, self).__init__()\n",
      "        # ...\n",
      "        \n",
      "    def forward(self, x):\n",
      "        # Apply input sanitization\n",
      "        x = (x - torch.mean(x)) / torch.std(x)\n",
      "        \n",
      "        # Compute attention scores and weights...\n",
      "```\n",
      "\n",
      "* **Failing to monitor attention maps**: Failing to monitor attention maps can lead to overfitting or lack of diversity in learned representations. To avoid this, use visualization tools like `matplotlib` or `seaborn` to inspect attention maps and ensure they are diverse and informative.\n",
      "\n",
      "```python\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Compute attention weights\n",
      "attention_weights = torch.matmul(query, key.transpose(-1, -2))\n",
      "\n",
      "# Visualize attention map\n",
      "plt.imshow(attention_weights.numpy(), cmap='hot', interpolation='nearest')\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "By recognizing and addressing these common mistakes, you can improve the performance and robustness of your self-attention mechanisms.\n",
      "\n",
      "## Conclusion and Practical Steps\n",
      "\n",
      "Self-attention has revolutionized the field of natural language processing (NLP) and computer vision, enabling models to accurately capture complex relationships between inputs. By providing a way to weigh the relevance of different input elements relative to each other, self-attention improves model accuracy and handles diverse data types more effectively.\n",
      "\n",
      "### Key Takeaways and Checklist\n",
      "\n",
      "To implement self-attention in your machine learning projects, consider the following best practices:\n",
      "\n",
      "* **Proper Normalization**: Scale attention weights using layer normalization or instance normalization to prevent exploding gradients. This helps maintain a stable learning process.\n",
      "  ```python\n",
      "import torch\n",
      "norm = torch.nn.LayerNorm(2 * embedding_dim)\n",
      "# Apply norm to attention output before further processing\n",
      "```\n",
      "* **Multi-Head Configuration**: Use multiple attention heads (e.g., 8, 32) to capture different aspects of the input data and reduce the risk of overfitting.\n",
      "* **Attention Visualization**: Utilize techniques like heatmaps or saliency maps to visualize the attention weights. This helps in understanding which parts of the model are giving more importance to specific inputs.\n",
      "\n",
      "### Next Steps\n",
      "\n",
      "To further improve performance, consider experimenting with:\n",
      "\n",
      "* **Sparse Attention**: Sparse attention mechanisms can provide significant efficiency gains by selectively focusing on relevant input elements. However, this may compromise accuracy slightly.\n",
      "* **Integration with Positional Encoding**: Combine self-attention with positional encoding to capture both local and global relationships in the input data.\n",
      "\n",
      "### Hyperparameter Tuning\n",
      "\n",
      "Attention mechanisms are sensitive to hyperparameters like attention dropout rate, number of attention heads, and embedding dimensions. Perform thorough hyperparameter tuning using techniques like grid search or random search to find the optimal configuration for your specific problem.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(out[\"final\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
