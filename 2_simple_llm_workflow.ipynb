{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5656b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph , START  , END\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from typing import TypedDict\n",
    "from dotenv  import load_dotenv\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1ce4885",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be24b225",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4105681",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMState(TypedDict):\n",
    "\n",
    "    question : str\n",
    "    answer : str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac2e019f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_qa(state = LLMState) -> LLMState:\n",
    "    # extract question from state\n",
    "    question = state['question']\n",
    "    prompt = f' answer the following {question}'\n",
    "    answer = model.invoke(prompt).content\n",
    "    state['answer'] = answer\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3340ab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create graph \n",
    "\n",
    "graph = StateGraph(LLMState)\n",
    "\n",
    "graph.add_node('llm_QA' , llm_qa)\n",
    "\n",
    "graph.add_edge(START , 'llm_QA')\n",
    "graph.add_edge('llm_QA' , END)\n",
    "\n",
    "workflow= graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2bbb33fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'what is langchain',\n",
       " 'answer': \"LangChain is a framework for developing applications powered by large language models (LLMs).  It's designed to make it easier to build applications that combine the power of LLMs with other sources of data and computation.  Instead of just interacting with an LLM directly, LangChain provides tools and abstractions to:\\n\\n* **Connect LLMs to other sources of data:**  This allows your application to access and process information from databases, APIs, documents, and more, enriching the LLM's responses.\\n\\n* **Manage the flow of information:** LangChain helps structure the interaction between the LLM and external data sources, ensuring a coherent and efficient process.  This includes features for chaining multiple LLMs together or combining LLMs with other components.\\n\\n* **Improve the memory and context of LLMs:** LLMs have limited context windows. LangChain offers mechanisms to extend this context, allowing the LLM to remember previous interactions or access a larger body of information.\\n\\n* **Build more complex applications:**  LangChain simplifies the development of sophisticated applications that go beyond simple prompt-response interactions, such as chatbots, question-answering systems, and agents that can interact with the real world.\\n\\nIn essence, LangChain acts as a glue that connects LLMs to the rest of your application and the world around it, enabling the creation of more powerful and useful AI applications.\"}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_state =  {'question' : 'what is langchain'}\n",
    "result = workflow.invoke(initial_state)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "acb4a0a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"LangChain is a framework for developing applications powered by large language models (LLMs).  It's designed to make it easier to build applications that combine the power of LLMs with other sources of data and computation.  Instead of just interacting with an LLM directly, LangChain provides tools and abstractions to:\\n\\n* **Connect LLMs to other sources of data:**  This allows your application to access and process information from databases, APIs, documents, and more, enriching the LLM's responses.\\n\\n* **Manage the flow of information:** LangChain helps structure the interaction between the LLM and external data sources, ensuring a coherent and efficient process.  This includes features for chaining multiple LLMs together or combining LLMs with other components.\\n\\n* **Improve the memory and context of LLMs:** LLMs have limited context windows. LangChain offers mechanisms to extend this context, allowing the LLM to remember previous interactions or access a larger body of information.\\n\\n* **Build more complex applications:**  LangChain simplifies the development of sophisticated applications that go beyond simple prompt-response interactions, such as chatbots, question-answering systems, and agents that can interact with the real world.\\n\\nIn essence, LangChain acts as a glue that connects LLMs to the rest of your application and the world around it, enabling the creation of more powerful and useful AI applications.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['answer']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
